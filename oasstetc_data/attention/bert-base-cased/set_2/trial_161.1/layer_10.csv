text;attention
The;1.7111732088820087e-18
easiest;3.17618009391431e-18
way;1.4859425896305035e-18
to;1.6646926395769289e-18
import;2.2366157716964938e-18
the;2.2137503461247552e-18
BERT;3.217406132993388e-18
language;1.0342220453084629e-17
model;3.634629204107731e-18
into;1.5165111097039067e-18
python;6.5781806344635196e-18
for;1.7619670510635518e-18
use;1.7450819519587464e-18
with;2.2268226608029744e-18
PyTorch;2.0359385652077505e-17
is;1.5247072918004145e-18
using;1.3996687108544994e-18
the;2.051155515135117e-18
Hugging;4.1570408461463125e-18
Face;4.884430981333844e-18
Transformer's;6.191005714163247e-18
library,;3.8537664145183425e-18
which;1.6139657357342088e-18
has;1.9080083528359565e-18
built;1.7683099720865715e-18
in;1.4660015590679338e-18
methods;1.6758892369028736e-18
for;1.510678569591332e-18
pre-training,;8.011365105431017e-18
inference,;1.0136548207276256e-17
and;1.7346815089246493e-18
deploying;3.498413191584474e-18
BERT.;0.9999999999999942
â€˜**;6.496959737803196e-18
from;1.6929090652560966e-18
transformers;8.178329851034169e-18
import;2.2981666058177263e-18
AutoTokenizer,;7.116751202121822e-18
BertModel;4.634230884573425e-18
import;2.666268395420606e-18
torch;6.7478644726293875e-18
tokenizer;4.0162482138454626e-18
=;1.6656976294884207e-18
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.7681301403939085e-15
model;1.7078035936177558e-18
=;1.5603530292880972e-18
"BertModel.from_pretrained(""bert-base-uncased"")";2.616916656428494e-15
inputs;2.0100344220997377e-18
=;1.9057593797829167e-18
"tokenizer(""Hello,";3.619437484855846e-17
my;1.7523184153113897e-18
dog;3.519509186219919e-18
is;1.6280416856522623e-18
"cute"",";5.656339886988769e-18
"return_tensors=""pt"")";9.845235135658748e-17
outputs;2.1158524840527783e-18
=;1.4669895536581334e-18
model(**inputs);2.1014759931366465e-17
last_hidden_states;9.612592059911906e-18
=;1.7630032869447764e-18
outputs.last_hidden_state;2.446134285554191e-17
***;4.084310685576168e-18
