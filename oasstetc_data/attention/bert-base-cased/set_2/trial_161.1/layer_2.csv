text;attention
The;4.6876246397851026e-07
easiest;7.60350683772476e-07
way;2.9331931449846347e-07
to;5.079221515956085e-07
import;3.147586230450038e-07
the;5.803529670848912e-07
BERT;7.803315237353829e-07
language;3.8805917740127183e-07
model;4.5271999508242083e-07
into;3.7137708891923087e-07
python;8.431646228603237e-07
for;3.7174964408139924e-07
use;2.9627717958112097e-07
with;3.7889034944374195e-07
PyTorch;2.244262374523665e-06
is;6.698648755129265e-07
using;3.0964477674269524e-07
the;5.929933613076963e-07
Hugging;4.846506855252634e-07
Face;3.0223826509330776e-07
Transformer's;1.6502324361893854e-06
library,;2.6891142985111084e-06
which;2.789152247980366e-07
has;2.9290087328232357e-07
built;2.773630940342792e-07
in;3.3923047881404317e-07
methods;3.36924669993015e-07
for;3.1865965063332354e-07
pre-training,;8.187285402966937e-06
inference,;4.93837881367397e-06
and;5.884179301823268e-07
deploying;9.380825627788804e-07
BERT.;1.538415361493278e-05
â€˜**;1.140203424070607e-05
from;3.3677767972883614e-07
transformers;4.546385383906483e-07
import;3.6756657095240755e-07
AutoTokenizer,;1.188867245125994e-05
BertModel;9.957550767861242e-07
import;3.196588334737816e-07
torch;3.4962314925384955e-07
tokenizer;4.822475094078393e-07
=;3.495574703799946e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.548436143697926
model;3.8120519008693055e-07
=;3.669658882709653e-07
"BertModel.from_pretrained(""bert-base-uncased"")";0.45107879437007065
inputs;3.219002561888271e-07
=;3.568858999024685e-07
"tokenizer(""Hello,";8.619966691833309e-06
my;3.7990324406403164e-07
dog;3.0494064174403334e-07
is;3.789285854495157e-07
"cute"",";2.8146183111515555e-06
"return_tensors=""pt"")";0.00026346055886841344
outputs;5.635754512256924e-07
=;3.6110739376169396e-07
model(**inputs);6.685396729230824e-06
last_hidden_states;7.182552331210281e-06
=;3.0297949090954586e-07
outputs.last_hidden_state;0.00011742377325729972
***;5.787930775284727e-07
