text;attention
The;0.00018037987052897593
easiest;0.00019035772404599337
way;0.00017195497666418473
to;0.00017372438427239444
import;0.00033579231307515446
the;0.0002249243024493134
BERT;0.0002535629772101704
language;0.00024608647592478294
model;0.00018087644321614673
into;0.00023210105938725696
python;0.0002903600267508549
for;0.00016373985319529733
use;0.00018149383785878172
with;0.00023844158875675745
PyTorch;0.0006474396519675158
is;0.00023340104344453394
using;0.00021147295656471368
the;0.00022567407631115277
Hugging;0.0002192441043968373
Face;0.00016855503632715117
Transformer's;0.00040856016744499556
library,;0.0003298270496204204
which;0.00016094127475550129
has;0.00019264386299214472
built;0.00026206001225562197
in;0.00017086571058533138
methods;0.00018120486306645556
for;0.0002049927636625865
pre-training,;0.0006975455055073186
inference,;0.0003191606634353361
and;0.00019530983820497118
deploying;0.0003932240443925584
BERT.;0.006965444709109929
â€˜**;0.0008330218433728275
from;0.00019113161771175087
transformers;0.00020842748825037038
import;0.00021340061425407418
AutoTokenizer,;0.0005858565306391009
BertModel;0.0003747997774739355
import;0.0002518702393858132
torch;0.0001862924272622313
tokenizer;0.0002357988697786694
=;0.00020191860354357342
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2894317162533291
model;0.00018096877434800864
=;0.00021196074306592384
"BertModel.from_pretrained(""bert-base-uncased"")";0.6722012348727952
inputs;0.00016652900902131004
=;0.00023310161753174443
"tokenizer(""Hello,";0.0022268256767954252
my;0.00015853539785123713
dog;0.0001599009879614429
is;0.00014456907821682743
"cute"",";0.0005102756228732925
"return_tensors=""pt"")";0.012230156900404578
outputs;0.00016756441414725329
=;0.00021090225295109508
model(**inputs);0.0009789460308303906
last_hidden_states;0.0005676775767589703
=;0.00018875203456503078
outputs.last_hidden_state;0.0012163516854929322
***;0.0002801498920068852
