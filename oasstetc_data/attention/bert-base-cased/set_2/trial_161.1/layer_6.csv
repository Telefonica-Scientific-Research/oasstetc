text;attention
The;6.245444959596823e-06
easiest;1.2892638185191346e-05
way;7.2153927746180145e-06
to;5.666961428232027e-06
import;7.770179625112627e-06
the;6.298715324312792e-06
BERT;1.0308698657491901e-05
language;5.8007759128149485e-06
model;7.443265697164443e-06
into;6.250130833070372e-06
python;1.2697195795733534e-05
for;6.0937200463462475e-06
use;5.992112562909932e-06
with;6.714235367023059e-06
PyTorch;3.0522968253836406e-05
is;8.43849161823717e-06
using;7.83838075348829e-06
the;6.014707065982186e-06
Hugging;8.457397400469972e-06
Face;5.908916672328622e-06
Transformer's;1.3937128346008862e-05
library,;1.2343448110465133e-05
which;6.118140508924621e-06
has;6.680821678355473e-06
built;7.807268482857857e-06
in;6.307075746964823e-06
methods;6.627751366294553e-06
for;6.815820880308757e-06
pre-training,;3.3249698858734554e-05
inference,;1.250832225929672e-05
and;5.874329522971097e-06
deploying;1.762514239460107e-05
BERT.;4.9345466446374035e-05
â€˜**;3.619327981402812e-05
from;6.790512012563251e-06
transformers;9.392444981056306e-06
import;7.1779883085937846e-06
AutoTokenizer,;3.867060679522633e-05
BertModel;1.7252463484718392e-05
import;6.325144765155973e-06
torch;5.07593699690461e-06
tokenizer;7.55536997225754e-06
=;8.43515505023369e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5462966207955872
model;7.206782305543379e-06
=;8.64014231122417e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.45162690597795674
inputs;7.378547732331742e-06
=;9.232924375033363e-06
"tokenizer(""Hello,";7.311584068607381e-05
my;5.075157689424138e-06
dog;5.6496393802775416e-06
is;6.3114786833421805e-06
"cute"",";1.4228152609830092e-05
"return_tensors=""pt"")";0.0010566703180149124
outputs;9.33672471309421e-06
=;6.749795694382202e-06
model(**inputs);0.00011929091867753241
last_hidden_states;5.1957576408713455e-05
=;6.6381294081368e-06
outputs.last_hidden_state;0.00016779426483804304
***;1.8517187211326573e-05
