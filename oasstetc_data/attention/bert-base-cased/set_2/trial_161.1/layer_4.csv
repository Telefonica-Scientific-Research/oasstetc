text;attention
The;4.12344049630189e-06
easiest;6.547473196500368e-06
way;3.4829035994156054e-06
to;2.8542011185997177e-06
import;4.205400694614474e-06
the;3.3919834713194675e-06
BERT;5.151700024490596e-06
language;5.096900378733979e-06
model;3.7775840889666564e-06
into;2.9713394436131344e-06
python;5.796766709891424e-06
for;2.8586638848599385e-06
use;2.8828642946471317e-06
with;4.361649206152321e-06
PyTorch;9.379243000469223e-06
is;5.516096043703034e-06
using;3.45823066143111e-06
the;5.5123833389729845e-06
Hugging;3.0711549424923122e-06
Face;2.63901341901123e-06
Transformer's;8.252697632107825e-06
library,;1.2856425402124077e-05
which;2.8462763829727105e-06
has;5.39733277566255e-06
built;3.432279887414657e-06
in;2.9355392430344423e-06
methods;3.100130836726766e-06
for;3.5404198163369585e-06
pre-training,;1.4415463131843492e-05
inference,;7.92936993508238e-06
and;3.5170603091534003e-06
deploying;7.252349520620528e-06
BERT.;2.2338387107164237e-05
â€˜**;2.0343940250457925e-05
from;2.6421365189940823e-06
transformers;4.871847876028732e-06
import;6.954991497818553e-06
AutoTokenizer,;2.45516075529491e-05
BertModel;9.010833706317268e-06
import;3.685222582144502e-06
torch;2.446693284887591e-06
tokenizer;3.2897950386031872e-06
=;5.543774641928861e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5495479225152526
model;2.9650602546370347e-06
=;5.9443882136787424e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.44896680734743893
inputs;2.713461520084798e-06
=;1.1651350364581173e-05
"tokenizer(""Hello,";6.522100585226201e-05
my;2.597352376829412e-06
dog;2.939619793685229e-06
is;3.342935116306821e-06
"cute"",";1.5655180771282785e-05
"return_tensors=""pt"")";0.0009745994479470418
outputs;3.5637036774901136e-06
=;5.6500301617816595e-06
model(**inputs);3.810708458261605e-05
last_hidden_states;1.4956505910483215e-05
=;4.805424874982403e-06
outputs.last_hidden_state;5.820566005702681e-05
***;4.11835888905021e-06
