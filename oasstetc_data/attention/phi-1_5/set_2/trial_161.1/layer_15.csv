text;attention
The;0.999999299754552
easiest;2.440588875210009e-25
way;2.109140265468397e-25
to;2.202480597638935e-25
import;3.7363364364947314e-25
the;2.5272479099872826e-25
BERT;1.0223975007614748e-24
language;2.691476542095352e-25
model;5.067979108047988e-25
into;2.4364044454228837e-25
python;2.805952462345535e-25
for;2.328203699098565e-25
use;1.9776462817055059e-25
with;2.269929580847093e-25
PyTorch;3.5481783014325172e-25
is;2.7994897678783883e-25
using;2.376724979947857e-25
the;2.119945578499885e-25
Hugging;3.1765054005783816e-25
Face;2.8476731523337293e-25
Transformer's;5.207565801663544e-25
library,;2.515371614291787e-25
which;1.988439112919016e-25
has;1.9946763413067647e-25
built;1.8128456844743985e-25
in;1.9590341594853101e-25
methods;2.0960863699139354e-25
for;2.2893507911311593e-25
pre-training,;3.983972031478186e-25
inference,;2.478517561056566e-25
and;1.8447499803043415e-25
deploying;2.097322481010769e-25
BERT.;7.002454481013666e-07
â€˜**;2.4328007774849232e-23
from;3.2074361213722254e-25
transformers;2.54738971537155e-25
import;2.4084043062462977e-25
AutoTokenizer,;6.202807271616929e-25
BertModel;3.927597617774505e-25
import;2.5941509592521076e-25
torch;2.217394694852025e-25
tokenizer;3.195100812193999e-25
=;2.774851272723136e-25
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";9.813315620458961e-24
model;2.212448120386513e-25
=;2.2094133191685106e-25
"BertModel.from_pretrained(""bert-base-uncased"")";1.6559125020624654e-24
inputs;2.291659697532077e-25
=;2.5194815666050917e-25
"tokenizer(""Hello,";5.516924696249875e-25
my;1.8603979148244402e-25
dog;1.9815019579303441e-25
is;1.79214972656531e-25
"cute"",";2.1597254165327004e-25
"return_tensors=""pt"")";5.26103184325207e-25
outputs;2.0981088172874805e-25
=;1.98822602500529e-25
model(**inputs);4.352070197758299e-25
last_hidden_states;3.0931360550671477e-25
=;1.9251682623241322e-25
outputs.last_hidden_state;2.6289962624158385e-25
***;1.7017721150289644e-25
