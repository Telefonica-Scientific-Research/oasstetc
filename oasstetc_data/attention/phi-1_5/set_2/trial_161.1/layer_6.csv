text;attention
The;0.9999999737654566
easiest;8.644745438016295e-26
way;8.249648141032623e-26
to;8.207927533061393e-26
import;1.2677459011737477e-25
the;8.703982901876475e-26
BERT;1.7699975696707394e-25
language;1.414477524424476e-25
model;1.1780009652912593e-25
into;9.417974625655704e-26
python;9.766860790862929e-26
for;7.922192762430388e-26
use;8.416441889407848e-26
with;8.326028157419695e-26
PyTorch;1.2448591266990787e-25
is;8.188042420117131e-26
using;8.927224827456255e-26
the;7.401399034324558e-26
Hugging;1.1328674433226346e-25
Face;9.81476105351445e-26
Transformer's;1.5942050975833912e-25
library,;9.719576653673465e-26
which;7.072416590077947e-26
has;7.261698026365632e-26
built;7.0417076457264e-26
in;8.32597654550479e-26
methods;8.139796155912837e-26
for;8.124894000456865e-26
pre-training,;1.3428225025983914e-25
inference,;8.890000163093976e-26
and;6.809258199267738e-26
deploying;7.331448523627834e-26
BERT.;2.623454333843147e-08
â€˜**;6.396530445057794e-25
from;1.325143865346892e-25
transformers;1.2135703877343251e-25
import;1.0911157206897092e-25
AutoTokenizer,;2.852157724142637e-25
BertModel;1.5890269380798695e-25
import;1.1925453959095795e-25
torch;9.114581022046117e-26
tokenizer;1.293234014765574e-25
=;1.952141941993795e-25
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.4963796885934224e-24
model;1.1356663448142258e-25
=;1.5533126141294452e-25
"BertModel.from_pretrained(""bert-base-uncased"")";1.236817018773487e-24
inputs;1.0891314061103128e-25
=;1.1466677655692016e-25
"tokenizer(""Hello,";3.982220040953016e-25
my;7.101003054917994e-26
dog;7.798576575460161e-26
is;7.277424629963207e-26
"cute"",";1.0189737778086891e-25
"return_tensors=""pt"")";2.3139980175089297e-25
outputs;9.249936744261707e-26
=;8.438300922268619e-26
model(**inputs);1.822930618211166e-25
last_hidden_states;1.7153196146552286e-25
=;7.862848655271258e-26
outputs.last_hidden_state;1.1539182604863592e-25
***;6.39354405264081e-26
