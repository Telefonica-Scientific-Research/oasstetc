text;attention
The;0.9998886529958261
easiest;4.560744356198425e-27
way;4.185231246397844e-27
to;4.5324204739330914e-27
import;5.912731888143037e-27
the;4.8924688242638844e-27
BERT;1.404044304244646e-26
language;6.528919124066332e-27
model;5.731680694183489e-27
into;4.72172697392468e-27
python;4.7422193909156526e-27
for;4.784914191325773e-27
use;4.1675012371565075e-27
with;4.860256355410913e-27
PyTorch;7.151996970261841e-27
is;4.486589451419144e-27
using;5.32231552229105e-27
the;4.827749570739192e-27
Hugging;7.712692295604517e-27
Face;5.406063255964863e-27
Transformer's;1.3683595333933834e-26
library,;5.20749230946785e-27
which;4.353154082938715e-27
has;4.27462923759855e-27
built;4.0858269164054204e-27
in;4.2874817022198805e-27
methods;4.471024698328673e-27
for;4.4763594875005316e-27
pre-training,;7.198984820611014e-27
inference,;5.632040639171531e-27
and;4.081962060673957e-27
deploying;5.111520711193439e-27
BERT.;0.00011134700417384825
â€˜**;1.2412590863728877e-25
from;5.1797033323166256e-27
transformers;7.146116343176393e-27
import;5.2622394811274115e-27
AutoTokenizer,;8.9424205140605e-27
BertModel;6.564136755981374e-27
import;5.2166352030223006e-27
torch;5.5771711321620505e-27
tokenizer;6.586824632499653e-27
=;4.885431706807788e-27
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.156060627212124e-26
model;4.565604480546458e-27
=;4.655386291734997e-27
"BertModel.from_pretrained(""bert-base-uncased"")";1.3314634219440962e-26
inputs;4.8724754778253835e-27
=;4.274411622544804e-27
"tokenizer(""Hello,";8.704548257805866e-27
my;4.21800035655412e-27
dog;4.471325913136147e-27
is;4.1182607959923935e-27
"cute"",";4.6776994258980594e-27
"return_tensors=""pt"")";7.914414022475013e-27
outputs;4.7651819295984455e-27
=;4.3157274554736995e-27
model(**inputs);5.704043648132016e-27
last_hidden_states;6.411376647811351e-27
=;4.0306099071931966e-27
outputs.last_hidden_state;5.2193055460602954e-27
***;3.9786318398106896e-27
