text;attention
The;0.999995259618258
easiest;4.96800836302753e-27
way;4.419324966969694e-27
to;4.7751068990400245e-27
import;6.743187914146204e-27
the;4.791266645183581e-27
BERT;2.0352582181049874e-26
language;5.1705918564997684e-27
model;6.1681017373537506e-27
into;4.60608296292677e-27
python;4.9452176422731655e-27
for;4.715115284607564e-27
use;4.223292269561855e-27
with;4.460903919184809e-27
PyTorch;7.020466472378931e-27
is;4.6853452144546815e-27
using;4.64300642921199e-27
the;4.6320151887489944e-27
Hugging;9.690537584522251e-27
Face;4.962278264888784e-27
Transformer's;1.0003725251742689e-26
library,;4.9401847078323026e-27
which;4.434135191797836e-27
has;4.510417365771893e-27
built;4.021148704942644e-27
in;4.3143331109342325e-27
methods;4.497757526391033e-27
for;4.8857328595043064e-27
pre-training,;8.206984472008041e-27
inference,;5.3169038055266426e-27
and;4.124652579329842e-27
deploying;4.679608368817561e-27
BERT.;4.740381742168884e-06
â€˜**;2.415085313681964e-25
from;5.513239906416308e-27
transformers;5.331004263658218e-27
import;4.891969509451824e-27
AutoTokenizer,;1.156268596465219e-26
BertModel;7.105224297068295e-27
import;5.737370767842099e-27
torch;4.901049690753767e-27
tokenizer;6.882399705887415e-27
=;4.818636499730104e-27
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";9.432942709043777e-26
model;5.22189022540039e-27
=;4.889844760262699e-27
"BertModel.from_pretrained(""bert-base-uncased"")";1.4816622876234756e-26
inputs;5.384875850999082e-27
=;4.5176463948694255e-27
"tokenizer(""Hello,";9.956526546013946e-27
my;4.1692754977654934e-27
dog;4.2687659885735066e-27
is;4.114604841763873e-27
"cute"",";4.546414237667292e-27
"return_tensors=""pt"")";9.443942540847213e-27
outputs;4.552361657249141e-27
=;4.442265017212417e-27
model(**inputs);6.43347847824276e-27
last_hidden_states;6.205357804547559e-27
=;4.0942455257875744e-27
outputs.last_hidden_state;6.182837954324699e-27
***;3.860243166459146e-27
