text;attention
The;0.9999999766982469
easiest;3.861807154311765e-26
way;3.6465368991830507e-26
to;3.8052507037984174e-26
import;5.632861254338874e-26
the;4.262951375298396e-26
BERT;1.0074316539399078e-25
language;6.351288508492587e-26
model;6.229396153886734e-26
into;5.291730423158439e-26
python;4.656889197779176e-26
for;4.431861100242391e-26
use;4.105051963861957e-26
with;4.179937945502667e-26
PyTorch;6.04084450238464e-26
is;4.904825893613733e-26
using;4.4156402927833177e-26
the;3.9034651197867956e-26
Hugging;5.724669930740027e-26
Face;4.8866849767952907e-26
Transformer's;1.1381592794875422e-25
library,;6.074511720969235e-26
which;3.9107016406881196e-26
has;4.095107137176147e-26
built;3.4280718796900024e-26
in;3.739942639591906e-26
methods;3.9714150837329626e-26
for;4.003547983402467e-26
pre-training,;6.546278712254815e-26
inference,;4.95566742451702e-26
and;3.3912172234515586e-26
deploying;3.9512953990058764e-26
BERT.;2.3301753146791547e-08
â€˜**;3.1802048023782468e-25
from;6.536405808593426e-26
transformers;5.763235161470424e-26
import;4.611719274945334e-26
AutoTokenizer,;1.3416855915797105e-25
BertModel;6.795054339709338e-26
import;6.852089704784688e-26
torch;4.248541217350604e-26
tokenizer;5.95184301964323e-26
=;8.969516576184103e-26
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.6280917599525799e-24
model;4.8376760142298305e-26
=;8.455467626574832e-26
"BertModel.from_pretrained(""bert-base-uncased"")";5.216698965821344e-25
inputs;4.738661872035684e-26
=;5.132725405764704e-26
"tokenizer(""Hello,";1.4221261348797566e-25
my;3.393958714340867e-26
dog;3.646130584993391e-26
is;3.3961349503234304e-26
"cute"",";4.4543431920461544e-26
"return_tensors=""pt"")";9.431699565228865e-26
outputs;4.2932104344436854e-26
=;3.90025281993249e-26
model(**inputs);8.416373939567362e-26
last_hidden_states;6.238459257800605e-26
=;4.0434257572606363e-26
outputs.last_hidden_state;4.5829522935443886e-26
***;3.226480617883821e-26
