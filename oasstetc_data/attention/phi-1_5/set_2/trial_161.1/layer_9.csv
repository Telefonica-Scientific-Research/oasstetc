text;attention
The;0.9999976792626429
easiest;1.4831930790493778e-25
way;1.364944439696816e-25
to;1.401421461363426e-25
import;2.271700218687103e-25
the;1.5543441647210859e-25
BERT;4.201247525354299e-25
language;2.179289596005977e-25
model;3.6780831996375034e-25
into;2.1623554914139556e-25
python;1.7827923278789278e-25
for;2.018575652571653e-25
use;1.4277943168297465e-25
with;1.8087811022364202e-25
PyTorch;2.2647434824704416e-25
is;1.7740253386301502e-25
using;1.6197726127807474e-25
the;1.3996387450545486e-25
Hugging;2.3505271654136032e-25
Face;2.019399868399805e-25
Transformer's;4.3191670821847005e-25
library,;1.9887656703225343e-25
which;1.3392191824800578e-25
has;1.3906616195088071e-25
built;1.2684640529759492e-25
in;1.457541879171874e-25
methods;1.5832591574686557e-25
for;1.4718459626973718e-25
pre-training,;2.909937374799555e-25
inference,;3.144869490184879e-25
and;1.3225412320750172e-25
deploying;1.551000831417617e-25
BERT.;2.3207373569817667e-06
â€˜**;8.641843962554767e-25
from;2.224663319971723e-25
transformers;1.838128644329437e-25
import;1.5806673911223835e-25
AutoTokenizer,;5.811872708725867e-25
BertModel;3.069424821882132e-25
import;1.7653607541415596e-25
torch;1.5261301588368142e-25
tokenizer;2.3855514479592952e-25
=;2.399984199357409e-25
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.5330206925261415e-24
model;2.0022341185359299e-25
=;1.7027396156533653e-25
"BertModel.from_pretrained(""bert-base-uncased"")";1.0894503048399359e-24
inputs;1.8251082670808168e-25
=;1.7260743362004807e-25
"tokenizer(""Hello,";6.371615429608423e-25
my;1.341919102760826e-25
dog;1.4382650450889622e-25
is;1.2614436561796133e-25
"cute"",";1.6315312370682155e-25
"return_tensors=""pt"")";4.0533043868505397e-25
outputs;1.6560376148017116e-25
=;1.3877286324818267e-25
model(**inputs);3.774958429617958e-25
last_hidden_states;2.401226303743052e-25
=;1.3288217699139695e-25
outputs.last_hidden_state;1.660551794842797e-25
***;1.200554747369843e-25
