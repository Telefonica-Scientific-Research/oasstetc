text;attention
The;0.9999993139145613
easiest;2.567264049401531e-25
way;2.3340888700965024e-25
to;2.4423139620349137e-25
import;3.941653145201635e-25
the;2.7104546586383583e-25
BERT;6.512302811159739e-25
language;3.633807954108951e-25
model;4.109774293319458e-25
into;3.4989819252497544e-25
python;3.73352970463614e-25
for;3.001526370313813e-25
use;2.3839712352564235e-25
with;2.7263015153700926e-25
PyTorch;3.8290716093915518e-25
is;2.57066726826098e-25
using;2.8534894774568737e-25
the;2.486915090111234e-25
Hugging;4.17941603241455e-25
Face;3.2884139729464415e-25
Transformer's;8.09719187085321e-25
library,;3.8405403276495552e-25
which;2.4777715552943143e-25
has;2.554278903012653e-25
built;2.254172627914889e-25
in;2.495480589214735e-25
methods;2.8143090811436315e-25
for;2.683447622201589e-25
pre-training,;6.766073894295617e-25
inference,;4.06014645165769e-25
and;2.322639780516259e-25
deploying;2.520565294527328e-25
BERT.;6.860854386093784e-07
â€˜**;1.2806885674894508e-24
from;3.103658744930828e-25
transformers;3.16776649632956e-25
import;2.849293939709815e-25
AutoTokenizer,;9.52089926787309e-25
BertModel;4.595168816162315e-25
import;3.4697159782282173e-25
torch;2.77300116992442e-25
tokenizer;4.491819321069949e-25
=;4.795094709971271e-25
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.0377338224128472e-23
model;3.88428745937891e-25
=;3.9733346929362044e-25
"BertModel.from_pretrained(""bert-base-uncased"")";2.816644195024334e-24
inputs;4.074766373920876e-25
=;3.6845470681199217e-25
"tokenizer(""Hello,";1.0214499470971297e-24
my;2.2106895935942605e-25
dog;2.377736106426841e-25
is;2.2547808194998107e-25
"cute"",";3.0210472058876834e-25
"return_tensors=""pt"")";8.236193704788545e-25
outputs;2.803887333952121e-25
=;2.4688345012360197e-25
model(**inputs);5.9506368482069295e-25
last_hidden_states;5.665566942792281e-25
=;2.412721262934098e-25
outputs.last_hidden_state;3.128397443014137e-25
***;2.08076884027261e-25
