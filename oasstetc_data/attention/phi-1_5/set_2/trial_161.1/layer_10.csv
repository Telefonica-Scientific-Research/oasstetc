text;attention
The;0.9999996555816337
easiest;4.487167183730313e-25
way;4.0473019572530443e-25
to;3.961776446101333e-25
import;6.375921441042979e-25
the;4.291400011436566e-25
BERT;1.757696026570077e-24
language;7.145886015188383e-25
model;1.0902107489810078e-24
into;6.223209325474918e-25
python;5.1907672022991e-25
for;4.581252927748586e-25
use;3.7655319108353874e-25
with;4.292379788906575e-25
PyTorch;7.6973126860713815e-25
is;4.817860964702862e-25
using;4.679075112893006e-25
the;4.112941944446846e-25
Hugging;7.540656357592727e-25
Face;5.903439546004589e-25
Transformer's;1.2058469448998557e-24
library,;5.4662640458964545e-25
which;3.7896606695844528e-25
has;3.930328786808226e-25
built;3.506590207237097e-25
in;3.919759419751989e-25
methods;4.513432244410658e-25
for;4.240741888895384e-25
pre-training,;8.995955588320902e-25
inference,;6.595783786893759e-25
and;3.814350071299263e-25
deploying;4.549899102159996e-25
BERT.;3.4441836616777077e-07
â€˜**;5.502955445276455e-24
from;5.518919098141644e-25
transformers;5.875630184308219e-25
import;4.876051738792748e-25
AutoTokenizer,;1.7988729140953935e-24
BertModel;9.499132002076405e-25
import;5.815497563121186e-25
torch;4.397242645223072e-25
tokenizer;7.424242709411407e-25
=;5.425165318381933e-25
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.585796969273253e-23
model;5.088064195232191e-25
=;5.163029085323112e-25
"BertModel.from_pretrained(""bert-base-uncased"")";7.71296388817314e-24
inputs;4.684100422575714e-25
=;4.686823915304483e-25
"tokenizer(""Hello,";1.4856838135673382e-24
my;3.5808796804070813e-25
dog;3.8867334401778745e-25
is;3.5264695479072724e-25
"cute"",";5.988083078626081e-25
"return_tensors=""pt"")";1.8881053993520237e-24
outputs;4.0818630350683012e-25
=;4.031363167537835e-25
model(**inputs);1.2066857104989077e-24
last_hidden_states;8.902311065170969e-25
=;4.298131598242486e-25
outputs.last_hidden_state;5.141991835981692e-25
***;3.4078298680500983e-25
