text;attention
The;0.9999941999775099
easiest;8.668418031732678e-27
way;8.488583044145506e-27
to;8.542650393922456e-27
import;1.3448047536199084e-26
the;9.202680517199431e-27
BERT;2.4453855100500038e-26
language;8.720461021609246e-27
model;1.7618159737066143e-26
into;8.867160151714816e-27
python;9.63517398972598e-27
for;8.189334668616872e-27
use;7.71420680009834e-27
with;7.957206934689087e-27
PyTorch;1.2549526729612863e-26
is;8.351861830482788e-27
using;8.671772437342369e-27
the;8.106246912911039e-27
Hugging;1.3168208815995172e-26
Face;9.675092852052097e-27
Transformer's;1.8411825856609824e-26
library,;9.297110586138161e-27
which;7.613225812567122e-27
has;8.034492754504118e-27
built;7.112987864810414e-27
in;8.009164000438912e-27
methods;8.005022100168475e-27
for;7.988859217810883e-27
pre-training,;1.3248312289519794e-26
inference,;9.88458467282558e-27
and;7.193750133878498e-27
deploying;8.29654996829465e-27
BERT.;5.800022490059892e-06
â€˜**;5.492303231233222e-25
from;1.0829395141939865e-26
transformers;9.262909937767363e-27
import;8.602283650066632e-27
AutoTokenizer,;1.971430948783926e-26
BertModel;1.2841982221868513e-26
import;1.2133602451230135e-26
torch;8.17984472668903e-27
tokenizer;1.4054649399833033e-26
=;1.0001956859076921e-26
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";9.93125345372381e-26
model;9.47356235055299e-27
=;8.597463231248723e-27
"BertModel.from_pretrained(""bert-base-uncased"")";2.3286317990207307e-26
inputs;9.372963671850921e-27
=;8.351681874194182e-27
"tokenizer(""Hello,";1.4456441187704074e-26
my;7.383721357546543e-27
dog;7.707657740049015e-27
is;7.154925350349755e-27
"cute"",";7.96585710507233e-27
"return_tensors=""pt"")";2.3001947342081925e-26
outputs;8.79677600915662e-27
=;8.222149957322448e-27
model(**inputs);1.3388781365655708e-26
last_hidden_states;1.2516230206307193e-26
=;7.65295509001388e-27
outputs.last_hidden_state;8.122732653149307e-27
***;7.020896119856401e-27
