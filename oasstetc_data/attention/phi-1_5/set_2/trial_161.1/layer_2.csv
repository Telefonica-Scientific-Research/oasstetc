text;attention
The;0.8055583392441743
easiest;2.674617796078843e-19
way;1.6440559597856346e-19
to;1.108634247098059e-19
import;2.727368929907995e-19
the;1.2921546008070661e-19
BERT;1.7401653564420898e-19
language;1.4659919598293617e-19
model;1.4945966365209932e-19
into;1.244209186586251e-19
python;8.834062876661376e-20
for;8.18591498440751e-20
use;8.553937614817949e-20
with;1.0911643579304006e-19
PyTorch;1.990007178083937e-19
is;1.161446890253797e-19
using;1.12372885254529e-19
the;1.265564057854247e-19
Hugging;1.7751909074788692e-19
Face;9.916606033947608e-20
Transformer's;1.8016485587180842e-19
library,;2.095184853186156e-19
which;9.550759426431188e-20
has;8.914433619196604e-20
built;9.246499418986e-20
in;8.439456475630164e-20
methods;9.439612645861695e-20
for;1.006129615548845e-19
pre-training,;2.306215503088051e-19
inference,;1.0792707858870392e-19
and;8.131395821583617e-20
deploying;8.862241616151059e-20
BERT.;0.19444166075582572
â€˜**;7.897706706551331e-19
from;9.529553800066225e-20
transformers;1.4661020626755855e-19
import;1.0921752634922801e-19
AutoTokenizer,;3.9891508517968373e-19
BertModel;1.369915053957038e-19
import;1.0265708302586594e-19
torch;8.249296245797691e-20
tokenizer;1.1699508836434683e-19
=;1.1234047187197048e-19
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.4097052200408668e-17
model;8.230821559305688e-20
=;1.088723986072934e-19
"BertModel.from_pretrained(""bert-base-uncased"")";5.4062923091842076e-18
inputs;9.560667576697593e-20
=;1.0685365694363078e-19
"tokenizer(""Hello,";3.13029945788241e-19
my;7.712150719509149e-20
dog;8.12135241831998e-20
is;7.187691865513369e-20
"cute"",";1.1237843966344136e-19
"return_tensors=""pt"")";3.1009215385852013e-19
outputs;8.051339890343145e-20
=;9.554810923526633e-20
model(**inputs);3.3307887280841565e-19
last_hidden_states;2.1273575153180608e-19
=;7.671699038917863e-20
outputs.last_hidden_state;1.7105541116764365e-19
***;6.393661104663377e-20
