text;attention
The;0.9536401309143254
easiest;2.784287696244346e-21
way;2.3251384335766495e-21
to;5.768637786456465e-21
import;4.035934156634831e-21
the;3.9374529200576916e-21
BERT;5.2041562676768045e-21
language;2.1183442605245517e-21
model;2.9568398889021424e-21
into;2.5786819468523958e-21
python;2.3973586044748743e-21
for;2.885570728199221e-21
use;2.608160178835637e-21
with;2.851707696419142e-21
PyTorch;2.9161887515410798e-21
is;3.740443087152217e-21
using;3.018796566671199e-21
the;3.2621626881902004e-21
Hugging;3.325046674275648e-21
Face;3.014824613309206e-21
Transformer's;4.123317134456052e-21
library,;2.8102163625558884e-21
which;2.4953987702007814e-21
has;2.4579498806747545e-21
built;2.2055159204621405e-21
in;2.3948229400567703e-21
methods;4.116609174764167e-21
for;4.2810833380453575e-21
pre-training,;7.135397549221505e-21
inference,;3.0438759498806395e-21
and;1.9492738909224674e-21
deploying;2.2961179903380015e-21
BERT.;0.046359869085674736
â€˜**;7.312503484379598e-18
from;2.339829773175938e-21
transformers;2.6039267145953073e-21
import;2.2961222329801335e-21
AutoTokenizer,;4.1494794851862984e-21
BertModel;2.3627270027422847e-21
import;2.4677816061935485e-21
torch;2.8880567593972917e-21
tokenizer;3.954800737991114e-21
=;2.514462002629589e-21
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.933541969257839e-20
model;2.5395666833759797e-21
=;2.1067175238476688e-21
"BertModel.from_pretrained(""bert-base-uncased"")";2.277431886791556e-20
inputs;2.604287477424386e-21
=;2.888471133945948e-21
"tokenizer(""Hello,";7.031188083636092e-21
my;1.885231972546485e-21
dog;2.1071298737836166e-21
is;1.7813963403480245e-21
"cute"",";2.304083934765628e-21
"return_tensors=""pt"")";9.910472333119712e-21
outputs;2.0418310855368724e-21
=;2.044716470304142e-21
model(**inputs);4.080845917378862e-21
last_hidden_states;8.025143938095288e-21
=;1.9703414074366146e-21
outputs.last_hidden_state;5.219679249308345e-21
***;1.9960214219609963e-21
