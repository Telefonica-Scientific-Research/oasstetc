text;attention
The;0.012263906060674859
easiest;0.013503430558240827
way;0.011099960871734132
to;0.01132072590152447
import;0.01698650300733428
the;0.012033282386010818
BERT;0.0244763354195773
language;0.012968860415162466
model;0.015519154211448552
into;0.01951320546707101
python;0.016114291165179782
for;0.014227578556619732
use;0.010554633044603178
with;0.010473797483977384
PyTorch;0.015836654447583573
is;0.019182306241519214
using;0.011565323178843418
the;0.010692019133251006
Hugging;0.012114461397076125
Face;0.014143010108427188
Transformer's;0.018578142141089528
library,;0.01706288164518806
which;0.010736840660926606
has;0.010780987362007643
built;0.009245916690048997
in;0.011373853527256624
methods;0.010845574373798308
for;0.011074477524860831
pre-training,;0.017091926440155347
inference,;0.012299281138326778
and;0.009278167769687937
deploying;0.01026108221199133
BERT.;0.017505394267300644
â€˜**;0.01629752217334711
from;0.012469002595988467
transformers;0.012560645218823281
import;0.014144912862795069
AutoTokenizer,;0.02321528794394114
BertModel;0.013966213904576308
import;0.01568355109427385
torch;0.010681783676897538
tokenizer;0.012636225996946162
=;0.015527087305621443
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.12598445413690687
model;0.013121291740666806
=;0.011627656209600629
"BertModel.from_pretrained(""bert-base-uncased"")";0.05542346917091489
inputs;0.011916548836372855
=;0.010403018210151556
"tokenizer(""Hello,";0.03120685005475483
my;0.00974244003037187
dog;0.009854505096243598
is;0.009472201108907327
"cute"",";0.011940886063129298
"return_tensors=""pt"")";0.020958238214129232
outputs;0.011793845240469168
=;0.009496864972943522
model(**inputs);0.016015393714150984
last_hidden_states;0.013193561482127771
=;0.009573145900840855
outputs.last_hidden_state;0.011559145178522698
***;0.008810287057086842
