text;attention
The;0.011396306984161896
easiest;0.007668769252019179
way;0.007389197733524374
to;0.006743397360774673
import;0.009649922624429581
the;0.0077003482287990605
BERT;0.032231406814427294
language;0.008244599981791846
model;0.013591929721360033
into;0.013964415504348234
python;0.02287726011420794
for;0.010581404849380418
use;0.008275600894203044
with;0.00751817624882055
PyTorch;0.035941116460745506
is;0.013225392588713797
using;0.008805152149206661
the;0.008016280591813653
Hugging;0.011262388356351414
Face;0.01282573978342133
Transformer's;0.01814625075759933
library,;0.012997202037035357
which;0.007205400693346588
has;0.006376320404312338
built;0.007134704058110899
in;0.007722459438349942
methods;0.008028170324330251
for;0.0071218990316876
pre-training,;0.015867085293838793
inference,;0.01019118853423542
and;0.006591016641736709
deploying;0.007430570233824199
BERT.;0.017593483761436594
â€˜**;0.018520161211915694
from;0.009736148345146751
transformers;0.012943093399918422
import;0.01074612368851831
AutoTokenizer,;0.037001798097790695
BertModel;0.021147861103312716
import;0.010378710213097563
torch;0.011950270375679757
tokenizer;0.008318132017558637
=;0.011788575735673426
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.23816287418859378
model;0.008451510719135838
=;0.008510200870799293
"BertModel.from_pretrained(""bert-base-uncased"")";0.054252200883081046
inputs;0.008847262906293054
=;0.0076646133347167975
"tokenizer(""Hello,";0.026323251174084725
my;0.006390380558173017
dog;0.006791649780903293
is;0.006127789319449677
"cute"",";0.007896956899207027
"return_tensors=""pt"")";0.018624531208717144
outputs;0.008140290303261459
=;0.006629395284908015
model(**inputs);0.01381596056208166
last_hidden_states;0.010084050406604554
=;0.006624639472869982
outputs.last_hidden_state;0.007998264944908776
***;0.005818745541254507
