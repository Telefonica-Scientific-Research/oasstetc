text;attention
The;0.010859843746824121
easiest;0.009338493549724396
way;0.009127629422013086
to;0.008176851763011103
import;0.013861277807484194
the;0.009263234408051692
BERT;0.046747734302219464
language;0.009704306912496012
model;0.014118598042677805
into;0.010284966074243349
python;0.01710475058286652
for;0.009595127965703226
use;0.00925431184114349
with;0.009024143669478448
PyTorch;0.02195832762524072
is;0.010766793101588975
using;0.011401397776542583
the;0.011339140461473059
Hugging;0.012028447401784879
Face;0.014250500627027577
Transformer's;0.017363397838829175
library,;0.011765977975776241
which;0.00918921465011355
has;0.009299175403744907
built;0.008259967019973006
in;0.009703320175131988
methods;0.009575175208752748
for;0.009526096087658125
pre-training,;0.015474718588117714
inference,;0.011484585668066212
and;0.00804085637590035
deploying;0.009062107191801764
BERT.;0.013460969518034877
â€˜**;0.013222215393631753
from;0.012874401014294101
transformers;0.013087562602540907
import;0.012430461821346479
AutoTokenizer,;0.03514716603906072
BertModel;0.025408483043559936
import;0.012892388539174751
torch;0.0118011598114796
tokenizer;0.012230561173483166
=;0.010039238178180733
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.20381308598086975
model;0.009950365287400599
=;0.009325214589702582
"BertModel.from_pretrained(""bert-base-uncased"")";0.04380877699956912
inputs;0.011480285199966722
=;0.008473323735516784
"tokenizer(""Hello,";0.020015793958371088
my;0.00808174071296205
dog;0.008740629774332197
is;0.007822788638177517
"cute"",";0.009198869378768784
"return_tensors=""pt"")";0.01989923386318046
outputs;0.009840110878295458
=;0.00795774025379427
model(**inputs);0.013603861098400047
last_hidden_states;0.012318934619588093
=;0.008016096184262602
outputs.last_hidden_state;0.010272020063394309
***;0.00783605238320005
