text;attention
The;0.006703011777355158
easiest;0.004111400849427598
way;0.0038637405648652015
to;0.0034004627061484213
import;0.005127572434844247
the;0.003942243884903922
BERT;0.019155074838848565
language;0.0038556832899428665
model;0.006682191477073276
into;0.005943332853534805
python;0.009782371504050863
for;0.004850547847009341
use;0.003850850749148771
with;0.0034572820169783075
PyTorch;0.011620389274415688
is;0.0059929360662906595
using;0.004442434829859416
the;0.004157112198369143
Hugging;0.005459907189416763
Face;0.005806001404563119
Transformer's;0.009442224161028119
library,;0.005691114019490249
which;0.003913902751288374
has;0.0035479879078210946
built;0.0037276699867947742
in;0.004056088563455807
methods;0.0036770570477207554
for;0.0037214597220465214
pre-training,;0.010309196097556586
inference,;0.0056025297448096065
and;0.003549295814132783
deploying;0.003403053682206835
BERT.;0.00744406278720924
â€˜**;0.011613874153585621
from;0.005727104270790148
transformers;0.006226102897451314
import;0.0058843662146864616
AutoTokenizer,;0.027356250722386302
BertModel;0.010138428283065425
import;0.005417568652499735
torch;0.0049202379637728905
tokenizer;0.005508192502412849
=;0.0074486835889425755
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.515280180028906
model;0.005644668035748526
=;0.004922213233849731
"BertModel.from_pretrained(""bert-base-uncased"")";0.08826524904168592
inputs;0.005401084891593458
=;0.004837077339238243
"tokenizer(""Hello,";0.029617778144907028
my;0.0034098667664717915
dog;0.0037623522203659946
is;0.003248071721116711
"cute"",";0.004347198400469631
"return_tensors=""pt"")";0.013441488198802198
outputs;0.0045156705198382925
=;0.0036337271079063143
model(**inputs);0.011337236994178712
last_hidden_states;0.006557711317946938
=;0.0037215521096759037
outputs.last_hidden_state;0.004614186747454785
***;0.0029116878876438122
