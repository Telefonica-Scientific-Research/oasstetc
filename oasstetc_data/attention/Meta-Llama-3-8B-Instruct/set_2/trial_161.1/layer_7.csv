text;attention
The;0.00869573810410111
easiest;0.0076174704402772785
way;0.007371421957212
to;0.007069891793167311
import;0.012322778052520513
the;0.00814739957075565
BERT;0.03297397440480857
language;0.008612902169333697
model;0.016762461498681252
into;0.01780151833834364
python;0.020464755112197384
for;0.010877193226197027
use;0.007337739635760867
with;0.00707565054462061
PyTorch;0.021660430882711607
is;0.016613677285333056
using;0.00869627456775246
the;0.007722829005476518
Hugging;0.008946383061699367
Face;0.01162822634872485
Transformer's;0.018263805521914672
library,;0.015723854524857514
which;0.008090756446610139
has;0.00713422921571288
built;0.006573417431734825
in;0.007493220324655814
methods;0.007799929280960011
for;0.007251534634244372
pre-training,;0.013219312783629839
inference,;0.010200980114713178
and;0.006355667681581858
deploying;0.007054253847358978
BERT.;0.021802419196311172
â€˜**;0.019919823051460825
from;0.010417448122636137
transformers;0.011311879521307761
import;0.010567439239202439
AutoTokenizer,;0.02739803577175683
BertModel;0.01480229703565517
import;0.011762834312908971
torch;0.008962560756830539
tokenizer;0.009619917568862922
=;0.012251280484945299
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.23558412650150937
model;0.009262358118791237
=;0.008857789489384682
"BertModel.from_pretrained(""bert-base-uncased"")";0.06213947949090849
inputs;0.0092882735879066
=;0.00834605801812705
"tokenizer(""Hello,";0.03746706748475267
my;0.0066358955544111
dog;0.007097482611715755
is;0.006302897654666273
"cute"",";0.008097807130010772
"return_tensors=""pt"")";0.01769449038141574
outputs;0.009067791865320609
=;0.007127266976411957
model(**inputs);0.013730589506846037
last_hidden_states;0.010419647682833395
=;0.006674389617950472
outputs.last_hidden_state;0.008019401461036666
***;0.005811573996474268
