text;attention
The;0.005269913407716963
easiest;0.004744144939351564
way;0.004584982425849676
to;0.004286231338494815
import;0.005756118157944849
the;0.004900441446009891
BERT;0.007701464602238301
language;0.004603579043777499
model;0.005220307837472187
into;0.004894672434164475
python;0.005222228792552629
for;0.004966410549395361
use;0.0047641309044729
with;0.004734920529680257
PyTorch;0.009235029727672399
is;0.004651184299595035
using;0.005565018921314259
the;0.005410718499748962
Hugging;0.005686992640973214
Face;0.005230245917164178
Transformer's;0.0073998087686352665
library,;0.0062371823914941695
which;0.004546770833001075
has;0.004656829397551495
built;0.004562123317943186
in;0.0046057229769972795
methods;0.004807421687370746
for;0.004866996377814212
pre-training,;0.008950702068920208
inference,;0.006058129787274034
and;0.004072825085243752
deploying;0.004643610943988134
BERT.;0.010558695252276838
â€˜**;0.010565357578215768
from;0.005391990768938924
transformers;0.0051044251149347224
import;0.0057109335814350395
AutoTokenizer,;0.011251104672141089
BertModel;0.008944428667712528
import;0.005090624581240343
torch;0.005044471450000058
tokenizer;0.00552309226547725
=;0.004792213937645456
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.43029420471065793
model;0.004960768189872224
=;0.004640853227527781
"BertModel.from_pretrained(""bert-base-uncased"")";0.21059876854047568
inputs;0.005158123082455403
=;0.004491999346291946
"tokenizer(""Hello,";0.012471194983161293
my;0.004388032526339394
dog;0.0044994225180474915
is;0.0041752831856687106
"cute"",";0.006086199428964844
"return_tensors=""pt"")";0.014554181609977023
outputs;0.00447966607984221
=;0.004034986092634597
model(**inputs);0.010495038744356915
last_hidden_states;0.007702746586676177
=;0.0040065492384280385
outputs.last_hidden_state;0.007937039674441896
***;0.004210744312339195
