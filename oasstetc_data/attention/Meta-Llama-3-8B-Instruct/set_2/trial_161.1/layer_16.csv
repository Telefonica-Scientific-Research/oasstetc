text;attention
The;0.008343699832644323
easiest;0.006851514765195148
way;0.006796964753263561
to;0.005869785665248784
import;0.00894195217283596
the;0.006433465037997842
BERT;0.05429626792251145
language;0.007082645702220862
model;0.010328419791999685
into;0.007498299305025999
python;0.011470531186394095
for;0.007217697083800811
use;0.006250315192885451
with;0.006080128002449534
PyTorch;0.02262089767394926
is;0.00806517540708502
using;0.007403788514700644
the;0.007369798183187243
Hugging;0.00982912598448177
Face;0.011753659826807112
Transformer's;0.01482199517134792
library,;0.009205709499326677
which;0.00662144436622962
has;0.00625503417385839
built;0.006092179167500363
in;0.0065517441059839135
methods;0.00662128571125698
for;0.006385930015920997
pre-training,;0.011455188699713115
inference,;0.007555555116063478
and;0.005702693863428045
deploying;0.005913071039150544
BERT.;0.010922387803176952
â€˜**;0.012174653427668903
from;0.009120930413767728
transformers;0.009526071665188207
import;0.0073628984405789384
AutoTokenizer,;0.023776527502362907
BertModel;0.020308076942970075
import;0.009140253684105919
torch;0.007715348535670676
tokenizer;0.009101272591858556
=;0.0076057781791263514
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3580473855137556
model;0.00792790383505444
=;0.006809230808046818
"BertModel.from_pretrained(""bert-base-uncased"")";0.055014938794324134
inputs;0.00868955738038046
=;0.006217725957199352
"tokenizer(""Hello,";0.016056461301949165
my;0.005661222670152452
dog;0.006420310337828782
is;0.005443583960212745
"cute"",";0.006850992617524946
"return_tensors=""pt"")";0.01814328758935153
outputs;0.008171995499586054
=;0.0060542272370578045
model(**inputs);0.012063991136488617
last_hidden_states;0.012469153492094627
=;0.005873653663573675
outputs.last_hidden_state;0.008063485726394192
***;0.005580730358084589
