text;attention
The;0.016396621603605843
easiest;0.016851591880565432
way;0.015954222807389968
to;0.014851141263769059
import;0.019583346897387
the;0.014627998257568652
BERT;0.01971298964242274
language;0.016438029855563672
model;0.016294826609768984
into;0.01558110277202547
python;0.015233506220947348
for;0.014776400147751991
use;0.014773808351133613
with;0.014470253260832741
PyTorch;0.017491979806509857
is;0.015886256025749928
using;0.015553708228277573
the;0.014022001023424815
Hugging;0.015576161562288793
Face;0.015056285812904571
Transformer's;0.015555762642390896
library,;0.015709314155407425
which;0.01439880128722624
has;0.014297706166573443
built;0.014626671400050854
in;0.014639182752821903
methods;0.014454394864925757
for;0.014299360294746623
pre-training,;0.01875214000661435
inference,;0.014762806453082162
and;0.013623325057356929
deploying;0.01424104208836397
BERT.;0.016188512710275115
â€˜**;0.01783975872550324
from;0.015639952899511755
transformers;0.015951735089458147
import;0.015536417037493689
AutoTokenizer,;0.01879286695639916
BertModel;0.015376282700384322
import;0.015055774289590549
torch;0.014327304389053745
tokenizer;0.01526113702717315
=;0.016241647457211822
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.03955042023735897
model;0.015625511421834857
=;0.015537794123893684
"BertModel.from_pretrained(""bert-base-uncased"")";0.02505445051974206
inputs;0.015263175199699589
=;0.016257530239045918
"tokenizer(""Hello,";0.0226984175253989
my;0.013998344448803917
dog;0.013742552560596361
is;0.013562334784594848
"cute"",";0.014521608979669039
"return_tensors=""pt"")";0.017467213731698867
outputs;0.014119438085087456
=;0.01477039426726109
model(**inputs);0.01610573168397285
last_hidden_states;0.015302362046770861
=;0.013917708925626292
outputs.last_hidden_state;0.01444130961383843
***;0.013359573123602676
