text;attention
The;0.009792691476719592
easiest;0.004000797296272823
way;0.003023812341656476
to;0.01011293902357833
import;0.006524959667288745
the;0.009764230171705404
BERT;0.007385077937393122
language;0.003117723701272742
model;0.003122346693596546
into;0.004340048039637273
python;0.010137652937051529
for;0.010993170740426815
use;0.002994718206762123
with;0.009971508067717665
PyTorch;0.008745921725331349
is;0.004968549462485816
using;0.003387312845612992
the;0.008236431834295363
Hugging;0.00721481282674019
Face;0.0028101890949465396
Transformer's;0.007393538843302467
library,;0.05070432672951548
which;0.002969483694990055
has;0.0038670601991179095
built;0.003036813853822871
in;0.007770979253260653
methods;0.0029003313260926263
for;0.008331917039741416
pre-training,;0.04278804169423035
inference,;0.03191062531283043
and;0.008620263557672805
deploying;0.0030924508434827217
BERT.;0.025723564883979616
â€˜**;0.004172621783896109
from;0.00513206647930224
transformers;0.0033145825766305106
import;0.004070559190325889
AutoTokenizer,;0.03359394882292733
BertModel;0.0034733554624098244
import;0.004033888458549945
torch;0.003779850574861659
tokenizer;0.0031479251342319244
=;0.006360922766007431
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.31250222944261646
model;0.0026142038491986557
=;0.005973649385765423
"BertModel.from_pretrained(""bert-base-uncased"")";0.18608327243182393
inputs;0.0026265085419273707
=;0.005166420697824651
"tokenizer(""Hello,";0.03348268438835928
my;0.0034173761756346815
dog;0.0025073302023679305
is;0.0034668608073278946
"cute"",";0.004472955433078294
"return_tensors=""pt"")";0.011364288583440537
outputs;0.0024701756042998257
=;0.004294748652023066
model(**inputs);0.0063671514909237675
last_hidden_states;0.0033685416995772686
=;0.0034504460004640336
outputs.last_hidden_state;0.003365256145887887
***;0.002171887895785447
