text;attention
The;0.006339142782681654
easiest;0.0055550051789754725
way;0.005098962308187946
to;0.004859034693785215
import;0.006594241838925928
the;0.005694630291295727
BERT;0.014458919138151852
language;0.005332698146394069
model;0.00560610848648039
into;0.0058078054263662636
python;0.00700076757187725
for;0.0057353919266125535
use;0.0049028533050484695
with;0.005474023661960279
PyTorch;0.011354873398594496
is;0.0059022000825561345
using;0.006077908715611897
the;0.006415756751762382
Hugging;0.006833638570075184
Face;0.007336934713392229
Transformer's;0.008946968185906262
library,;0.00637937229021809
which;0.004997757495346436
has;0.0052003355529142165
built;0.004782742760317874
in;0.004903904433080382
methods;0.005244546635297571
for;0.005112019828821967
pre-training,;0.008605495389048277
inference,;0.00620008199795799
and;0.004397225386349907
deploying;0.004707728380534276
BERT.;0.011208499522502073
â€˜**;0.01943286495910839
from;0.006324775009161752
transformers;0.005404900994568692
import;0.006826088650135282
AutoTokenizer,;0.016801726896107982
BertModel;0.011069714756275249
import;0.00672482907825796
torch;0.0059697033516893094
tokenizer;0.0063021928683727035
=;0.005285438643220692
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4037153389628355
model;0.005435304725600523
=;0.004876306240395867
"BertModel.from_pretrained(""bert-base-uncased"")";0.1903829143242515
inputs;0.005910254019606015
=;0.005155006113881087
"tokenizer(""Hello,";0.011595740298606268
my;0.004739801976235392
dog;0.004895301255824387
is;0.004449581661323841
"cute"",";0.005873628662349671
"return_tensors=""pt"")";0.010379591937981754
outputs;0.005160847250904589
=;0.004425242788650042
model(**inputs);0.007854626632531857
last_hidden_states;0.007053861143493709
=;0.004422549776213578
outputs.last_hidden_state;0.00600886908586512
***;0.004455423089520449
