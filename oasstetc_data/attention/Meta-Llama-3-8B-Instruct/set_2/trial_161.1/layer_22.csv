text;attention
The;0.015678560360943276
easiest;0.014081766866785592
way;0.013200319957602374
to;0.012676675726517891
import;0.016504338294420853
the;0.013315840330014246
BERT;0.03443506772944866
language;0.013579334531456431
model;0.01796662774770066
into;0.013554814733803511
python;0.015157335778013214
for;0.013493947642052605
use;0.012886408996162878
with;0.012955735570544923
PyTorch;0.01846114257476407
is;0.01370110610188971
using;0.013575193102565915
the;0.013832059308018112
Hugging;0.01539838355962312
Face;0.014685127888740457
Transformer's;0.016537992449091786
library,;0.015130681900720524
which;0.013036984361922537
has;0.012988499644654233
built;0.012087658411791644
in;0.012818826431255365
methods;0.0131357371815993
for;0.01294949944853801
pre-training,;0.01592979131920021
inference,;0.014749315455785325
and;0.011983876869479696
deploying;0.012326657643729437
BERT.;0.019738633589902414
â€˜**;0.024546559826260892
from;0.014017112713442777
transformers;0.014027971747813636
import;0.01388231103506774
AutoTokenizer,;0.023016226991812397
BertModel;0.020609638953213428
import;0.013760825979228432
torch;0.013735309992662892
tokenizer;0.014958960845313037
=;0.013416455330458385
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.05987706953680487
model;0.01374396891664979
=;0.012827172986518283
"BertModel.from_pretrained(""bert-base-uncased"")";0.03219715527739251
inputs;0.016141931615927994
=;0.013069149584388602
"tokenizer(""Hello,";0.021551740975062704
my;0.01184781085957373
dog;0.012269271314949984
is;0.011664091381886815
"cute"",";0.013726815557137187
"return_tensors=""pt"")";0.020930623415323316
outputs;0.014592864962718492
=;0.012487261227219215
model(**inputs);0.01680379945011894
last_hidden_states;0.018507793691727135
=;0.012231634836682664
outputs.last_hidden_state;0.014905910750443726
***;0.012098618735461334
