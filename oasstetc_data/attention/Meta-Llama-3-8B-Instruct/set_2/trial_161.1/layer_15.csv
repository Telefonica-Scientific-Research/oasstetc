text;attention
The;0.00776944225757954
easiest;0.006325631243048831
way;0.005976974056264076
to;0.005331924660749833
import;0.008435751771695811
the;0.006077728165115631
BERT;0.02998572588756886
language;0.006983860546701357
model;0.009287716996274687
into;0.007822889031587059
python;0.012524331203002878
for;0.006672370410494709
use;0.005739830458258297
with;0.005659603953762828
PyTorch;0.01656510980361266
is;0.0066822116944101236
using;0.006618589827272223
the;0.007605647707179085
Hugging;0.008765943726984424
Face;0.010233416616351949
Transformer's;0.013900427523111597
library,;0.008430678436675297
which;0.005840279109057702
has;0.00596663628066438
built;0.006068505307459898
in;0.005809303197777754
methods;0.0057835964868522514
for;0.0055804480627430235
pre-training,;0.011400438889411894
inference,;0.007196266601084948
and;0.004837166864001206
deploying;0.005256307005746583
BERT.;0.009254550926011983
â€˜**;0.007985101720546445
from;0.006571976471578725
transformers;0.008939108205245773
import;0.006637823098681577
AutoTokenizer,;0.03330146170244052
BertModel;0.020165704863522156
import;0.006442745503336094
torch;0.00673442042663143
tokenizer;0.008689534692557153
=;0.006757756776398875
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4363540075528743
model;0.0065696968588929
=;0.005646511152242813
"BertModel.from_pretrained(""bert-base-uncased"")";0.041117728000671344
inputs;0.007704965531906559
=;0.005656708473421412
"tokenizer(""Hello,";0.020154167753717974
my;0.005043407206437087
dog;0.005894408532884084
is;0.004873995820904144
"cute"",";0.006087117059454499
"return_tensors=""pt"")";0.016024827001885594
outputs;0.0064468068084807305
=;0.0050423711085013714
model(**inputs);0.010272741634684853
last_hidden_states;0.011725724250050685
=;0.005144328410207367
outputs.last_hidden_state;0.007048577465976811
***;0.004576971207353688
