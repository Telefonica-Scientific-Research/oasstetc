text;attention
The;0.006791901553681016
easiest;0.003915647008049679
way;0.003906475124490468
to;0.0036082503580020546
import;0.0059996435934450984
the;0.004190333256980525
BERT;0.04961489813173752
language;0.004745312313992581
model;0.00851922777984147
into;0.005634870493837444
python;0.010903663397751534
for;0.004831238135289224
use;0.004249125809906519
with;0.0039967781376543604
PyTorch;0.024522465852035557
is;0.005025507839742346
using;0.0048080877496143246
the;0.006208804258093362
Hugging;0.006285877838999454
Face;0.007791493629435187
Transformer's;0.010479420028385578
library,;0.005549526060365421
which;0.003997513014972231
has;0.003973894109344736
built;0.004271888460703932
in;0.003840951809588594
methods;0.0037313366348499686
for;0.0037294077146710768
pre-training,;0.0085719338936534
inference,;0.005375753387180389
and;0.003561210889764328
deploying;0.0038382662120494794
BERT.;0.007465069303877723
â€˜**;0.006705814556058996
from;0.005515879161454544
transformers;0.0056269952785630035
import;0.005756058563698844
AutoTokenizer,;0.036056546168963394
BertModel;0.02345347819309638
import;0.006102787801844932
torch;0.005555154694398998
tokenizer;0.006416162140995642
=;0.005686973421752853
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.503013539348124
model;0.0052750523545728395
=;0.004237979962229834
"BertModel.from_pretrained(""bert-base-uncased"")";0.04195533471770753
inputs;0.006920702800275539
=;0.004165778817224998
"tokenizer(""Hello,";0.016291190240421416
my;0.0035493884153327635
dog;0.00388383143328713
is;0.0034250223462633455
"cute"",";0.004405534415568167
"return_tensors=""pt"")";0.011202089344671108
outputs;0.006065078321934104
=;0.003801374582826469
model(**inputs);0.00886572535896373
last_hidden_states;0.009962879676937429
=;0.0039019514899382023
outputs.last_hidden_state;0.0049768761545386985
***;0.003289046456368578
