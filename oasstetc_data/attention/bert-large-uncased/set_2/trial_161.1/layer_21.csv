text;attention
The;5.311657643574837e-10
easiest;3.726918547002128e-10
way;3.49539064188111e-10
to;4.663864889391964e-10
import;4.0163408817130085e-10
the;5.319213276375034e-10
BERT;3.3152734800297344e-10
language;6.831424341780669e-10
model;4.2922345052230707e-10
into;3.4739606558839767e-10
python;7.325227939187048e-10
for;3.397354789145815e-10
use;3.519438407528263e-10
with;3.609394095621738e-10
PyTorch;2.587140588432047e-05
is;3.789670073917771e-10
using;3.5568106933996133e-10
the;5.049308070125802e-10
Hugging;3.545714974617217e-10
Face;4.4509724666466254e-10
Transformer's;2.463286439095317e-09
library,;1.694070193764532e-08
which;3.4219669465872087e-10
has;3.5651048743225943e-10
built;3.866442807887232e-10
in;3.342313851333988e-10
methods;4.152481886246884e-10
for;3.6184125775572137e-10
pre-training,;3.0665114096649174e-05
inference,;4.288616073380216e-09
and;3.4267995359963524e-10
deploying;5.296624178684636e-10
BERT.;7.574640614342459e-07
â€˜**;1.0093387091435397e-09
from;3.7508837170563125e-10
transformers;4.511925338946196e-10
import;4.2487723702017334e-10
AutoTokenizer,;0.08079139648659091
BertModel;4.529103873298174e-06
import;3.867390912890203e-10
torch;3.9314536894758373e-10
tokenizer;6.835430991395864e-10
=;3.852928689198258e-10
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6014972371814279
model;4.066200049334886e-10
=;3.6793317757418815e-10
"BertModel.from_pretrained(""bert-base-uncased"")";0.3176494406288128
inputs;4.4298215405572617e-10
=;3.4857862958123537e-10
"tokenizer(""Hello,";2.9160454453706118e-08
my;3.5196096928825453e-10
dog;5.057849973608623e-10
is;3.694850433121724e-10
"cute"",";4.44763030571441e-09
"return_tensors=""pt"")";1.9518833410056396e-08
outputs;3.779058793749572e-10
=;3.459328823273088e-10
model(**inputs);2.0177367791708257e-09
last_hidden_states;1.505599503728645e-09
=;3.622709307729127e-10
outputs.last_hidden_state;2.067168645391653e-09
***;8.785214686590815e-10
