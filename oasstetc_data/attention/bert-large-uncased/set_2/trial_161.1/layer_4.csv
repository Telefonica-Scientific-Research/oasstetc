text;attention
The;1.1224203522879728e-05
easiest;4.105442987979593e-07
way;3.971665305522002e-07
to;1.2592320144264625e-06
import;4.7591997111123924e-07
the;1.8853178538321335e-05
BERT;4.991716219020475e-07
language;5.498680896095368e-07
model;4.6807087102331873e-07
into;5.909613515830046e-07
python;5.755223777430636e-07
for;6.461430083406793e-07
use;5.217642069271211e-07
with;8.87846268813376e-07
PyTorch;1.208180318750283e-06
is;1.3229359570035151e-06
using;5.253883910572382e-07
the;1.5137949797712018e-05
Hugging;4.53572304979186e-07
Face;5.426994539954449e-07
Transformer's;7.278321983878869e-06
library,;5.342458581602913e-06
which;4.950513356597268e-07
has;7.761723811130488e-07
built;4.533835182086168e-07
in;7.14354331737884e-07
methods;5.588761488786202e-07
for;7.982953037034917e-07
pre-training,;1.6802752687763763e-05
inference,;3.0949707586212136e-06
and;1.6305725200288704e-06
deploying;7.018909354800954e-07
BERT.;2.3271288021263738e-05
â€˜**;1.1388756406960644e-05
from;8.297477222414556e-07
transformers;6.289766279374543e-07
import;4.946542669175991e-07
AutoTokenizer,;3.3427217374421174e-05
BertModel;7.877377506328799e-07
import;5.050410086476985e-07
torch;4.5880979820466966e-07
tokenizer;7.929746394783091e-07
=;6.779446552484539e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.8775769101032993
model;4.7343613284521493e-07
=;7.107941047600757e-07
"BertModel.from_pretrained(""bert-base-uncased"")";0.12209312757658335
inputs;4.5766494391809347e-07
=;6.310457940340462e-07
"tokenizer(""Hello,";3.319370165372856e-05
my;4.776473463695192e-07
dog;4.148217979321571e-07
is;1.03805276301174e-06
"cute"",";8.94061547505033e-06
"return_tensors=""pt"")";6.180100089148758e-05
outputs;4.434513269583587e-07
=;5.644774986838441e-07
model(**inputs);7.583618570594009e-06
last_hidden_states;1.4889128958872223e-06
=;5.429717257116136e-07
outputs.last_hidden_state;4.206972843414417e-05
***;6.698111080932692e-07
