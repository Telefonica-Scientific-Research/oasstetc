text;attention
The;1.1079643933290642e-06
easiest;4.4960342170710244e-07
way;4.5543324606214397e-07
to;6.314405049382555e-07
import;5.032495748934007e-07
the;1.4659471246290902e-06
BERT;4.162999610776847e-07
language;4.744986155677233e-07
model;4.551269800426719e-07
into;5.004770598783897e-07
python;5.232513469038177e-07
for;5.200628748978303e-07
use;4.3366167832585403e-07
with;5.363320137547755e-07
PyTorch;1.8338168929723307e-06
is;7.757097292343154e-07
using;5.126725773503563e-07
the;2.0091433161931146e-06
Hugging;4.4829996220858947e-07
Face;4.438800232214511e-07
Transformer's;6.9882880727725716e-06
library,;1.4484483326525025e-06
which;5.28759058535556e-07
has;7.423653751435724e-07
built;4.3412176258438927e-07
in;5.411253759451144e-07
methods;4.683807145829657e-07
for;6.045301981576605e-07
pre-training,;3.4581298035219533e-06
inference,;8.058769191185615e-07
and;7.95600654920155e-07
deploying;1.0912370008829452e-06
BERT.;2.926305346981978e-06
â€˜**;1.4285712441159842e-05
from;8.421636779188769e-07
transformers;5.111731095967261e-07
import;5.757808686155013e-07
AutoTokenizer,;6.634776066868765e-06
BertModel;1.3453144562524438e-06
import;6.045444673179166e-07
torch;5.826415061283867e-07
tokenizer;8.77769079680858e-07
=;8.632580423767049e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7247906837691805
model;5.497062604939337e-07
=;8.90451123171014e-07
"BertModel.from_pretrained(""bert-base-uncased"")";0.2749100029284395
inputs;4.6571440648941357e-07
=;8.842053946713263e-07
"tokenizer(""Hello,";2.9564211978357305e-05
my;4.264116716758389e-07
dog;4.244068235100325e-07
is;6.202782499473244e-07
"cute"",";1.96720133941153e-06
"return_tensors=""pt"")";0.0001348705426067749
outputs;5.818171229258386e-07
=;7.636345786526886e-07
model(**inputs);1.3793102203520827e-05
last_hidden_states;6.5868827598119855e-06
=;5.977737477803378e-07
outputs.last_hidden_state;4.251443179237149e-05
***;1.3593566916175526e-06
