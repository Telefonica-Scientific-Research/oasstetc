text;attention
The;5.2358826519675945e-05
easiest;3.003949763820033e-06
way;3.166972868993759e-06
to;1.1323858761084552e-05
import;3.186715918665908e-06
the;3.944464633717412e-05
BERT;3.2269168201726003e-06
language;4.513200961568364e-06
model;3.7242642612641823e-06
into;4.337954567321839e-06
python;3.2881204672663624e-06
for;5.611165156489512e-06
use;3.2614892801830177e-06
with;6.071462292695494e-06
PyTorch;8.218692869185693e-06
is;6.926320130938872e-06
using;3.779729674410744e-06
the;3.686419887285028e-05
Hugging;2.979391263531989e-06
Face;3.897545890753433e-06
Transformer's;6.676639753866025e-05
library,;3.8723109506037235e-05
which;4.3659633771223105e-06
has;4.8642458452007825e-06
built;4.284913995892944e-06
in;5.886010784248449e-06
methods;3.244142614734807e-06
for;6.346868925694189e-06
pre-training,;0.0001327158972925398
inference,;2.77157266459735e-05
and;8.724932463745588e-06
deploying;5.763887674746735e-06
BERT.;0.00032783454279514536
â€˜**;2.00747544051323e-05
from;6.44675531811054e-06
transformers;4.2302023917961776e-06
import;3.5170242289519915e-06
AutoTokenizer,;0.00021102979806838574
BertModel;5.45769484037973e-06
import;3.6968503620912562e-06
torch;4.02829381898576e-06
tokenizer;4.425672313590384e-06
=;5.697197139966081e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.9005412671015293
model;3.596632027190545e-06
=;4.6229988361388194e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.09714483693133026
inputs;2.9663580804546137e-06
=;4.163863388803877e-06
"tokenizer(""Hello,";9.482617121686472e-05
my;3.843314048169643e-06
dog;3.5611699332421824e-06
is;5.622160976027806e-06
"cute"",";3.8957321401517115e-05
"return_tensors=""pt"")";0.00010885429596655079
outputs;2.8411313184357443e-06
=;4.222937180687697e-06
model(**inputs);3.395306715336003e-05
last_hidden_states;1.0103082297642704e-05
=;4.5418545095037234e-06
outputs.last_hidden_state;0.0008673296940037753
***;4.8636097767611745e-06
