text;attention
The;0.0006207484780354714
easiest;0.0005016153624027376
way;0.0006704944340347209
to;0.0005532070464596279
import;0.0007175948625266538
the;0.0006645427843561695
BERT;0.0005130531877241493
language;0.0006160828890684911
model;0.0005085447290559022
into;0.0006512399412885973
python;0.0007395500165713312
for;0.0005998262022765874
use;0.0004814164215316985
with;0.0007150552039209392
PyTorch;0.0007092301904266125
is;0.000782987662511067
using;0.0008022166048036151
the;0.0006967108350974686
Hugging;0.0005463531106149103
Face;0.0004894806631572692
Transformer's;0.004509794904452888
library,;0.0008531252838067474
which;0.0005661851792810043
has;0.0006728675644452726
built;0.0005151851567500753
in;0.000450827291038848
methods;0.0005883271806249339
for;0.0008456756179701677
pre-training,;0.001909895079449085
inference,;0.0005944738555835601
and;0.00048263518892221135
deploying;0.0006879535185731246
BERT.;0.001956271450067146
â€˜**;0.001450401912973529
from;0.0010230182590828321
transformers;0.0006077784180276946
import;0.0007831471024425372
AutoTokenizer,;0.0015128474384412465
BertModel;0.0006205376075030474
import;0.0007179042134242496
torch;0.0006504382058020875
tokenizer;0.0006936572615310838
=;0.0011806146809481854
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6781752405943057
model;0.0005204424729651764
=;0.000997372195672326
"BertModel.from_pretrained(""bert-base-uncased"")";0.23672989518959428
inputs;0.0005762974622329837
=;0.0007074898191570454
"tokenizer(""Hello,";0.009481563703282606
my;0.0005497475085708136
dog;0.00047428413862100595
is;0.000577476080961609
"cute"",";0.0021009011201352235
"return_tensors=""pt"")";0.018559189818254516
outputs;0.0005849629319846881
=;0.0007431178404096025
model(**inputs);0.005041182852475841
last_hidden_states;0.0012854945000789998
=;0.0010431542000413675
outputs.last_hidden_state;0.004277223040591881
***;0.0011214515336587765
