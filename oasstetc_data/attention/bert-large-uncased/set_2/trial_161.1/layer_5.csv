text;attention
The;0.0008214640010361147
easiest;1.0119207849047553e-06
way;1.0322214341952532e-06
to;4.305656056807347e-06
import;1.054409876861252e-06
the;0.004253172799300043
BERT;1.082285622915163e-06
language;1.5012872264071859e-06
model;1.1414479634579499e-06
into;1.2328475680235912e-06
python;1.7970374270475046e-06
for;1.2286313699111736e-06
use;1.3193480999225312e-06
with;1.4687388646903447e-06
PyTorch;2.5903723307846596e-06
is;2.5529942367102746e-06
using;1.1855376782015219e-06
the;0.07016895813920745
Hugging;1.1845663111929182e-06
Face;1.32073059444661e-06
Transformer's;9.02587839966697e-05
library,;4.814000156578904e-05
which;9.669960843533116e-07
has;1.3863984267521748e-06
built;1.1356267977480695e-06
in;1.3082303962825674e-06
methods;1.0535324793473321e-06
for;1.4850995684074973e-06
pre-training,;3.382075633765602e-05
inference,;1.515838487088568e-05
and;2.4068104107429007e-06
deploying;1.4733993198851565e-06
BERT.;0.0015135392698755348
â€˜**;0.007962748677328121
from;1.7911115129721767e-06
transformers;2.4647990260857995e-06
import;1.0704569414847162e-06
AutoTokenizer,;4.6683562793660836e-05
BertModel;1.5517114718906133e-06
import;1.0520076236046802e-06
torch;1.1159149229239102e-06
tokenizer;1.8118972527457424e-06
=;1.186852253918177e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.538364272358364
model;1.1749489356431575e-06
=;1.2006736572999685e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.37508267958861863
inputs;1.1525108531167398e-06
=;1.1247518545383064e-06
"tokenizer(""Hello,";9.307991323311814e-05
my;1.0808986603734222e-06
dog;1.0257415989281722e-06
is;2.0082271696552646e-06
"cute"",";0.000255800190389511
"return_tensors=""pt"")";2.1472949340829297e-05
outputs;1.0046691430039337e-06
=;9.602746447492083e-07
model(**inputs);3.6695085153823874e-06
last_hidden_states;2.5454694744637692e-06
=;9.538570564264654e-07
outputs.last_hidden_state;0.0011593767219342788
***;1.2014923082398498e-06
