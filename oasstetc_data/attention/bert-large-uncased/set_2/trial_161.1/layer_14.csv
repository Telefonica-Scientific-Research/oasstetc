text;attention
The;0.0007839227125905984
easiest;0.0009073151125399627
way;0.0009395820807082068
to;0.0007598881184243279
import;0.0011204317639444178
the;0.000816359259661303
BERT;0.0007971886235164614
language;0.0008978839161178367
model;0.0008433763722643594
into;0.000820494333530092
python;0.0011396005146626304
for;0.0007445994208241457
use;0.0007215812093856866
with;0.0009677936891973131
PyTorch;0.0013804103317018894
is;0.0011768345187827324
using;0.001065913755664065
the;0.00112419554670324
Hugging;0.0009619537570470938
Face;0.0008447974705890726
Transformer's;0.002237859780690612
library,;0.0014980582266070563
which;0.0008073807227838697
has;0.0009576010409710625
built;0.000760427362657081
in;0.0006876975205982702
methods;0.0010572676801150966
for;0.0011151565488158907
pre-training,;0.00234655087081863
inference,;0.0010135657907786371
and;0.0008044437021269427
deploying;0.0012846432902817336
BERT.;0.002572605463945035
â€˜**;0.003175621837299411
from;0.0012990020009520159
transformers;0.0016776361295385421
import;0.0010639136966764501
AutoTokenizer,;0.0020798977262277566
BertModel;0.0011303729889844862
import;0.0010547411586008452
torch;0.0014360386203625846
tokenizer;0.001317562652561921
=;0.0012725651953239576
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7128473615778206
model;0.0008825297136413664
=;0.0013509520658022275
"BertModel.from_pretrained(""bert-base-uncased"")";0.1837315381146051
inputs;0.0010516694091271376
=;0.001152564804210591
"tokenizer(""Hello,";0.0056993293989065375
my;0.000758448694901062
dog;0.0009299828965632603
is;0.0008105737193488152
"cute"",";0.0018358180164988812
"return_tensors=""pt"")";0.019400283052576864
outputs;0.0009083038041550284
=;0.0010036470034828584
model(**inputs);0.0060620061985588985
last_hidden_states;0.002559531170045639
=;0.0013117934176467617
outputs.last_hidden_state;0.004559594969610944
***;0.0016793394569243989
