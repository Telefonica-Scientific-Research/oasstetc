text;attention
The;1.7329266238201932e-05
easiest;1.327121773808448e-05
way;1.4093082011486849e-05
to;1.3140987219720836e-05
import;1.4149606144895437e-05
the;2.1938730555503295e-05
BERT;1.349932053155111e-05
language;1.402115609152839e-05
model;1.5624462651084857e-05
into;1.548382820598194e-05
python;1.3402578302934421e-05
for;1.6172331758757958e-05
use;1.2995847566052346e-05
with;1.3858180092514413e-05
PyTorch;3.3015299546011566e-05
is;2.083523921348546e-05
using;1.4786776632434463e-05
the;3.2825072042733356e-05
Hugging;1.5017379541561239e-05
Face;1.2738226542427628e-05
Transformer's;4.936187336598693e-05
library,;2.8631879755304865e-05
which;1.3110823935124993e-05
has;1.4424308308970333e-05
built;1.4193323210854671e-05
in;1.1303063311975632e-05
methods;1.5640163711813612e-05
for;2.1158668348301375e-05
pre-training,;6.762656952838839e-05
inference,;2.2763956557905532e-05
and;1.574267768807104e-05
deploying;2.0060809921584702e-05
BERT.;7.11461414594781e-05
â€˜**;0.00011402276671957901
from;2.002578031189415e-05
transformers;1.3804391191542339e-05
import;2.0238607812537207e-05
AutoTokenizer,;9.287357482394458e-05
BertModel;2.6355185335279724e-05
import;2.2367734583223156e-05
torch;1.519634694636497e-05
tokenizer;1.942836568686594e-05
=;1.7539938339522476e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5449670279313642
model;1.7592958407777664e-05
=;1.9825564642667153e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.4521455662992907
inputs;1.7431106082922187e-05
=;1.9392539486371076e-05
"tokenizer(""Hello,";0.00027910661161048203
my;1.4849566373837926e-05
dog;1.3241962680212918e-05
is;1.7268264115824414e-05
"cute"",";3.6769168533263815e-05
"return_tensors=""pt"")";0.0006530290199810517
outputs;1.5760833705013034e-05
=;1.8637799246785984e-05
model(**inputs);0.00016555616679404567
last_hidden_states;9.628482793108888e-05
=;1.7401016357370887e-05
outputs.last_hidden_state;0.0003840957910948088
***;3.594703281991884e-05
