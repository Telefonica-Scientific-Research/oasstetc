text;attention
The;4.739468102127702e-15
easiest;4.613331923119546e-15
way;4.327406645105442e-15
to;5.825161175632214e-15
import;5.173185748065065e-15
the;6.056846384547737e-15
BERT;6.007058622364045e-15
language;5.330620873382041e-15
model;5.098117904721754e-15
into;4.822866746688437e-15
python;4.849829323144615e-15
for;4.3594968080140385e-15
use;4.993810725822138e-15
with;5.518990666667725e-15
PyTorch;8.239883629152951e-15
is;5.473826728669354e-15
using;5.735226782659499e-15
the;7.187046767851402e-15
Hugging;5.8532055121580145e-15
Face;4.4003743831344364e-15
Transformer's;1.0411452249182277e-14
library,;1.2326488481353822e-14
which;4.957671460196917e-15
has;5.6664629313811965e-15
built;4.521620355265492e-15
in;5.234908347124919e-15
methods;5.382321229946708e-15
for;5.43592675413336e-15
pre-training,;3.449095617524041e-14
inference,;6.8082458528084845e-15
and;5.6951336532444524e-15
deploying;4.691039780636507e-15
BERT.;0.999718224181749
â€˜**;1.0543163008022534e-14
from;5.383246529392739e-15
transformers;7.618645311714697e-15
import;6.4958937402351e-15
AutoTokenizer,;1.3566808007456431e-14
BertModel;6.426785298917942e-15
import;6.056496208978904e-15
torch;6.4914387641752245e-15
tokenizer;5.830021838869133e-15
=;6.648129458012585e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.2478022192235022e-12
model;4.695655578064843e-15
=;6.731693569418937e-15
"BertModel.from_pretrained(""bert-base-uncased"")";1.2932845499870476e-12
inputs;4.9403880023052024e-15
=;6.8929229427686086e-15
"tokenizer(""Hello,";2.66485834629912e-14
my;4.844371674968436e-15
dog;4.84062403358373e-15
is;5.031997400701594e-15
"cute"",";7.437344670729533e-15
"return_tensors=""pt"")";5.5329209103599216e-14
outputs;4.802857666791017e-15
=;7.831618148607099e-15
model(**inputs);2.329869391871749e-14
last_hidden_states;1.3391417961877024e-14
=;5.497909330775956e-15
outputs.last_hidden_state;0.00028177581523346434
***;5.0918642693874865e-15
