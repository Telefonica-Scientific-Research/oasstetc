text;attention
The;5.444364170686442e-11
easiest;5.193403524052822e-11
way;5.005824253056886e-11
to;5.947854545025898e-11
import;6.324023951337747e-11
the;7.247110264163866e-11
BERT;7.907820003916685e-11
language;7.846097066382074e-11
model;8.679040389221656e-11
into;5.403351032381257e-11
python;6.209410117188264e-11
for;4.558297692158104e-11
use;4.1174842723490604e-11
with;5.046065680446031e-11
PyTorch;5.834726343396524e-11
is;5.6222353712375494e-11
using;5.664571421733556e-11
the;5.842219611190053e-11
Hugging;4.8748605736272696e-11
Face;4.4221614673715175e-11
Transformer's;9.498849341631521e-11
library,;4.396197346472462e-10
which;5.262462416458768e-11
has;4.927551735977275e-11
built;4.226465190634465e-11
in;4.3743791743475557e-11
methods;5.428323658508542e-11
for;5.514023736898828e-11
pre-training,;5.755187045276196e-10
inference,;3.082760896464017e-10
and;4.524600953999883e-11
deploying;5.4631186438169225e-11
BERT.;0.05417886114234294
â€˜**;9.914039691038912e-11
from;8.209653700749938e-11
transformers;9.057668129150889e-11
import;7.139583044235041e-11
AutoTokenizer,;1.5401619045520908e-10
BertModel;7.323008567552346e-11
import;8.721187894156082e-11
torch;8.26509462159959e-11
tokenizer;1.0130911655372117e-10
=;6.703656356659395e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07984951955804356
model;7.089904509792073e-11
=;6.104523176265044e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.7684713070815367
inputs;8.098056425734397e-11
=;6.615264251414056e-11
"tokenizer(""Hello,";2.9862018127228115e-10
my;4.5989939577571384e-11
dog;4.7188154939235805e-11
is;4.2874085195054936e-11
"cute"",";6.479189598133273e-11
"return_tensors=""pt"")";4.767937834733273e-10
outputs;6.562337631211479e-11
=;6.033525239085767e-11
model(**inputs);6.747626076434709e-10
last_hidden_states;1.8551989310548913e-10
=;6.598088777214073e-11
outputs.last_hidden_state;0.09750030594511605
***;6.921732925784185e-11
