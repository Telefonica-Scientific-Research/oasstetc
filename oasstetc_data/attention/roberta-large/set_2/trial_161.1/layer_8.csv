text;attention
The;3.1773572141281194e-09
easiest;2.3750825495158942e-09
way;1.7932024851800025e-09
to;2.2854942687109734e-09
import;3.224654003504852e-09
the;2.4634256307669017e-09
BERT;4.444862467544397e-09
language;2.4677555313069957e-09
model;3.291602731083635e-09
into;2.391644066224453e-09
python;3.131657262207401e-09
for;1.8531598648630573e-09
use;1.7894502202633696e-09
with;2.573238635604656e-09
PyTorch;5.3386893987595295e-09
is;2.611624339677001e-09
using;2.779392095220324e-09
the;3.192136447406004e-09
Hugging;4.442821351476472e-09
Face;2.3889135959952573e-09
Transformer's;6.8729126610930835e-09
library,;8.899077285308709e-09
which;2.3167681678291104e-09
has;2.8597578753443543e-09
built;1.753463325653304e-09
in;1.89058723900119e-09
methods;2.2032275829605423e-09
for;2.688594637729906e-09
pre-training,;1.142675484476959e-07
inference,;4.875074332830987e-09
and;2.025789432113609e-09
deploying;2.2707400455540287e-09
BERT.;0.08667956146029868
â€˜**;8.384744135897986e-09
from;3.818178493185924e-09
transformers;3.86040929883305e-09
import;6.4041496870786956e-09
AutoTokenizer,;1.3343101535716567e-08
BertModel;4.735233880057205e-09
import;6.42636359592695e-09
torch;3.132800398047936e-09
tokenizer;4.831325808116676e-09
=;3.1355545603357845e-09
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";5.33654831671004e-05
model;3.2719977233401194e-09
=;2.96474097019207e-09
"BertModel.from_pretrained(""bert-base-uncased"")";0.1724381071774942
inputs;3.161057680271695e-09
=;3.2112001653582387e-09
"tokenizer(""Hello,";1.9647146157566396e-08
my;2.0270158023496526e-09
dog;2.0019328996414148e-09
is;1.8608274222458218e-09
"cute"",";4.3469425445250006e-09
"return_tensors=""pt"")";1.1180639617347933e-07
outputs;2.9059392625818317e-09
=;2.9082046597071144e-09
model(**inputs);2.4176858728282567e-08
last_hidden_states;1.5680275511140385e-08
=;3.0126837795082133e-09
outputs.last_hidden_state;0.7408284916386451
***;2.2456088977713013e-09
