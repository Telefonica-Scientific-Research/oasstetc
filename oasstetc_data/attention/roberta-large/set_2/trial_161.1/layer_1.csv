text;attention
The;9.205072361792184e-05
easiest;4.949717009577863e-05
way;4.774853218943367e-05
to;0.00012598453700081937
import;4.440100621105042e-05
the;0.00014455348376545026
BERT;9.017187263939794e-05
language;6.41704280789825e-05
model;5.5744039938936896e-05
into;7.847729659115036e-05
python;6.677762711542562e-05
for;0.00011803074475613947
use;5.777895441352749e-05
with;9.787173635099182e-05
PyTorch;0.00013008302383824414
is;0.00010602512460681522
using;5.836866594423462e-05
the;0.0001431316525398051
Hugging;8.452876532894634e-05
Face;4.998319183434004e-05
Transformer's;0.0003461264220344221
library,;0.00027941105135297913
which;7.084906844614662e-05
has;8.388871593647843e-05
built;5.7528291752689535e-05
in;0.00010483082604131103
methods;6.291961313240699e-05
for;0.00011144680770309842
pre-training,;0.0011343352773252966
inference,;0.00018875148519341195
and;0.00012671447129265225
deploying;5.141096468194369e-05
BERT.;0.0006109873185534584
â€˜**;0.0001693314183242742
from;9.058461835032584e-05
transformers;9.703147406492471e-05
import;4.1066847949786365e-05
AutoTokenizer,;0.0005349449136944193
BertModel;7.126725971371125e-05
import;4.1397337658347975e-05
torch;4.8820638488858833e-05
tokenizer;7.805308169471177e-05
=;6.318452799765643e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6877746817010113
model;5.6080889102136e-05
=;6.642844385525379e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.2994009935150189
inputs;4.83347934558716e-05
=;5.773230504715305e-05
"tokenizer(""Hello,";0.0008445313407299525
my;8.470327063331127e-05
dog;6.0085423045876364e-05
is;9.272970859341033e-05
"cute"",";0.00011294898857615882
"return_tensors=""pt"")";0.0017918433137785995
outputs;4.333975980665316e-05
=;6.13259936158708e-05
model(**inputs);0.0011281769475937052
last_hidden_states;0.00031695861662777103
=;6.259679685638561e-05
outputs.last_hidden_state;0.001779933136662156
***;4.631404774887157e-05
