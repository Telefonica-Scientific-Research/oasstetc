text;attention
The;4.4605196925288123e-10
easiest;3.1759842106314615e-10
way;3.1129451804229096e-10
to;4.2656846151352553e-10
import;4.1693167521413384e-10
the;4.694103392639438e-10
BERT;4.816233235248068e-10
language;5.183828677098668e-10
model;4.743837263326319e-10
into;4.6200630842764855e-10
python;5.316225025318143e-10
for;3.4418805007697053e-10
use;2.708705742951218e-10
with;3.8734716283718326e-10
PyTorch;3.726979230658348e-10
is;3.644341006569375e-10
using;4.0699111711122317e-10
the;4.020889202476047e-10
Hugging;3.5284017190399914e-10
Face;2.732763107503911e-10
Transformer's;7.196493744380114e-10
library,;3.639235153940627e-09
which;3.9063398669214817e-10
has;2.9414280314569166e-10
built;2.746750157293534e-10
in;2.8409996853228536e-10
methods;3.132085332370658e-10
for;3.650552041497057e-10
pre-training,;6.7998512254877725e-09
inference,;3.4193171830643617e-09
and;3.446223435484426e-10
deploying;3.4173641398331366e-10
BERT.;0.009910560829950184
â€˜**;1.0152930488242466e-09
from;3.9616200787868965e-10
transformers;6.876320070725307e-10
import;4.946273174708853e-10
AutoTokenizer,;1.8385504752908164e-09
BertModel;6.968984147366661e-10
import;6.613204961201953e-10
torch;9.249051063219086e-10
tokenizer;4.920813921789472e-10
=;3.586338099287236e-10
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6566344430692115
model;4.1502848940308714e-10
=;3.564706926828785e-10
"BertModel.from_pretrained(""bert-base-uncased"")";0.3191661002001364
inputs;4.4078423278554883e-10
=;4.1756653118800903e-10
"tokenizer(""Hello,";3.18003713711395e-09
my;3.031927203325032e-10
dog;3.5903680731365283e-10
is;2.567440264998473e-10
"cute"",";6.154119478876925e-10
"return_tensors=""pt"")";5.971726789519145e-09
outputs;4.051555722981386e-10
=;3.6607368076207536e-10
model(**inputs);4.141140832753558e-09
last_hidden_states;9.239953939306882e-10
=;4.344918979150042e-10
outputs.last_hidden_state;0.01428884394277835
***;5.881273122483456e-10
