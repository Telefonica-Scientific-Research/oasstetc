text;attention
The;3.000822274193749e-11
easiest;2.2938615073691387e-11
way;2.0952507348633327e-11
to;3.6355988407865055e-11
import;2.4967332684796873e-11
the;2.836505136357555e-11
BERT;3.059694311942632e-11
language;4.41565854960316e-11
model;3.806204815293454e-11
into;2.5168409893217075e-11
python;3.1893705205016436e-11
for;2.355664983658734e-11
use;2.0964167826604216e-11
with;2.6269502238208345e-11
PyTorch;3.036271207463704e-11
is;2.134075465542176e-11
using;2.247672193187565e-11
the;2.6977297927869713e-11
Hugging;2.66736855059978e-11
Face;2.435012128408905e-11
Transformer's;4.695933483275005e-11
library,;4.310451976819914e-10
which;2.335275010132718e-11
has;2.0929667418009697e-11
built;2.0715208534688504e-11
in;1.924408852835531e-11
methods;2.4240209600317146e-11
for;2.6067899830934742e-11
pre-training,;6.151197745197367e-10
inference,;4.3075510086245946e-10
and;2.221583276888531e-11
deploying;2.921724906642549e-11
BERT.;0.02055972827264954
â€˜**;4.9058879324268276e-11
from;2.3743098741200537e-11
transformers;4.2093052658208885e-11
import;2.7488245912819437e-11
AutoTokenizer,;7.309651853072016e-11
BertModel;3.480199625743411e-11
import;2.8949371413712302e-11
torch;1.985306749754125e-10
tokenizer;2.8219771001576064e-11
=;2.2880522610065583e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6199682959974858
model;2.454873188204244e-11
=;2.297169554254945e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.32386592036656325
inputs;3.0824330027234775e-11
=;2.449145089425933e-11
"tokenizer(""Hello,";1.6438568568012762e-10
my;2.2298635230656988e-11
dog;2.7323639714549758e-11
is;2.3323168019834534e-11
"cute"",";3.335125974163126e-11
"return_tensors=""pt"")";3.243413319283775e-10
outputs;2.8321569776143207e-11
=;2.479954764653903e-11
model(**inputs);1.3142137046022977e-10
last_hidden_states;7.402467117179951e-11
=;2.282420088019726e-11
outputs.last_hidden_state;0.03560605155875587
***;3.0132465239008666e-11
