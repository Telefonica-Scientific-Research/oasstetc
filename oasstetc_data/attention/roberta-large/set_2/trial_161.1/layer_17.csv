text;attention
The;2.2745609481936186e-10
easiest;2.0829790082758983e-10
way;1.8874735118179225e-10
to;2.629188403093986e-10
import;2.3759827634700495e-10
the;2.4751743574854095e-10
BERT;2.812907856926509e-10
language;2.6862442401463135e-10
model;2.952356361358393e-10
into;1.9764337771109275e-10
python;3.024148592155187e-10
for;1.8908670372547168e-10
use;1.6593598938413012e-10
with;2.1840163730810924e-10
PyTorch;2.9701612715112364e-10
is;2.2683106240436353e-10
using;2.497008180422598e-10
the;2.5412821323353387e-10
Hugging;2.5717154807254673e-10
Face;1.864941910694477e-10
Transformer's;4.4811474293174656e-10
library,;5.088157746197085e-09
which;2.4536317733251e-10
has;1.8467860281256724e-10
built;1.831946821548776e-10
in;1.7211418336815287e-10
methods;2.337724175740601e-10
for;2.0715172375865686e-10
pre-training,;7.395275395220394e-09
inference,;5.036175787369396e-09
and;1.8859109972724552e-10
deploying;2.3653184529560954e-10
BERT.;0.01618976630625439
â€˜**;3.8860413799252656e-10
from;2.425017622310154e-10
transformers;3.820516072797405e-10
import;2.7290275455369985e-10
AutoTokenizer,;1.2632923293207291e-09
BertModel;4.728749516223013e-10
import;2.6440462347828387e-10
torch;3.8134599592497086e-10
tokenizer;2.8781614920740526e-10
=;2.354121446670166e-10
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5773456214537606
model;2.3118505866164958e-10
=;2.428139576026324e-10
"BertModel.from_pretrained(""bert-base-uncased"")";0.3752894570598772
inputs;2.435183283369104e-10
=;2.824106463425269e-10
"tokenizer(""Hello,";2.132801770859132e-09
my;1.927897939636482e-10
dog;2.3443001862968264e-10
is;1.8194012772090943e-10
"cute"",";3.1293173604539996e-10
"return_tensors=""pt"")";3.804293426691513e-09
outputs;2.6648846075945824e-10
=;2.9424358060223984e-10
model(**inputs);1.923609677539113e-09
last_hidden_states;7.490634905603998e-10
=;2.4681329581060913e-10
outputs.last_hidden_state;0.031175115043124046
***;2.248111826137031e-10
