text;attention
The;1.5988084054741095e-13
easiest;1.2812657988171965e-13
way;1.2302689473664556e-13
to;1.8479353406045865e-13
import;1.4016304158080406e-13
the;1.4130570409213292e-13
BERT;1.603920863954896e-13
language;1.4407000721606377e-13
model;1.7605017334606074e-13
into;1.2926312106057153e-13
python;1.5782022290846004e-13
for;1.338335074035329e-13
use;1.302625126542533e-13
with;1.3073440204413372e-13
PyTorch;2.11278994488059e-13
is;1.2764451086379336e-13
using;1.2647569217181686e-13
the;1.462446770681168e-13
Hugging;1.7864998660708783e-13
Face;1.4721469978867426e-13
Transformer's;2.0523932822583728e-13
library,;1.685224630257401e-12
which;1.7062941244581926e-13
has;1.31100162910581e-13
built;1.3051293371478357e-13
in;1.3672424158628684e-13
methods;1.3033717955287104e-13
for;1.3126439372313698e-13
pre-training,;1.7234740420637712e-12
inference,;1.6135233657464029e-12
and;1.5175984646515174e-13
deploying;1.332475170454138e-13
BERT.;0.03071094388279092
â€˜**;2.83627330195543e-13
from;1.4629293285907342e-13
transformers;1.7847370470135542e-13
import;1.582232496931282e-13
AutoTokenizer,;2.188326875578064e-13
BertModel;1.648104730659832e-13
import;1.6923291245098868e-13
torch;1.7817481592224626e-13
tokenizer;1.67095520563072e-13
=;1.316762201724959e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.35804387426932904
model;1.4528232542811173e-13
=;1.3447233158349219e-13
"BertModel.from_pretrained(""bert-base-uncased"")";0.5253091209625704
inputs;1.402427064049211e-13
=;1.334083039496425e-13
"tokenizer(""Hello,";2.855564823316314e-13
my;1.2777942805197088e-13
dog;1.8399132263796298e-13
is;1.3495919294619497e-13
"cute"",";1.6788589344373093e-13
"return_tensors=""pt"")";5.172694955966823e-13
outputs;1.4320609504300497e-13
=;1.3377144401665707e-13
model(**inputs);3.971171291145336e-13
last_hidden_states;2.607765283302748e-13
=;1.3107707599930005e-13
outputs.last_hidden_state;0.08593606087099478
***;1.6125423303812186e-13
