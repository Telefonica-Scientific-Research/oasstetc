text;attention
The;6.552993857447608e-11
easiest;5.7141015646232825e-11
way;5.098010932337751e-11
to;6.848901603825081e-11
import;7.932485470991824e-11
the;6.930115908158789e-11
BERT;7.230823890196985e-11
language;5.752130457028021e-11
model;9.134946115206654e-11
into;5.350478056103061e-11
python;6.573006490397779e-11
for;5.1966343108261455e-11
use;4.4742957788022205e-11
with;5.0814315686789464e-11
PyTorch;7.298189288362586e-11
is;5.0803178608570555e-11
using;5.803083438942728e-11
the;7.107837313571373e-11
Hugging;9.206566045329464e-11
Face;5.2720275238865634e-11
Transformer's;1.4832771080358446e-10
library,;3.4762183744522627e-10
which;4.7829183339377553e-11
has;4.739852582500832e-11
built;4.4453953787152897e-11
in;4.686899629262449e-11
methods;5.3638468344677336e-11
for;6.087220743182609e-11
pre-training,;6.571811679051786e-10
inference,;2.62811806469975e-10
and;4.6288495514347464e-11
deploying;4.6360275698226625e-11
BERT.;0.011778917180972348
â€˜**;8.658446786889609e-11
from;6.805266678495719e-11
transformers;9.473938218611581e-11
import;7.531571780527922e-11
AutoTokenizer,;1.8487418253140445e-10
BertModel;1.0178338308259958e-10
import;7.496927105672476e-11
torch;7.297397837899345e-11
tokenizer;1.3188800799580153e-10
=;6.421987639919491e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.003923169022837682
model;9.493271470359561e-11
=;6.458186609828837e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.9485787675416067
inputs;7.836244794109607e-11
=;5.942532666497512e-11
"tokenizer(""Hello,";5.249588287723832e-10
my;4.776406032913048e-11
dog;6.697771197745282e-11
is;4.783853643925222e-11
"cute"",";7.309172310108306e-11
"return_tensors=""pt"")";1.4064265692552075e-09
outputs;6.800760049519695e-11
=;6.356228514315643e-11
model(**inputs);6.100707791154193e-10
last_hidden_states;2.6320562070063795e-10
=;6.627752782247989e-11
outputs.last_hidden_state;0.03571913863327424
***;4.4387998355623785e-11
