text;attention
The;0.000594330415038005
easiest;0.0007933759542049085
way;0.0006157918521711495
to;0.0006193594118825229
import;0.000785192127262488
the;0.0007099842874763577
BERT;0.0008695285025095181
language;0.0007758419264552657
model;0.0006400628097271285
into;0.0006375051591117408
python;0.0007567438914464359
for;0.0006948326909001177
use;0.000713259260619691
with;0.0006870757097823574
PyTorch;0.0017457150474919697
is;0.0007293905278539085
using;0.0006598764150884834
the;0.0007018977796745407
Hugging;0.001105601202005855
Face;0.0006689975988689483
Transformer's;0.002147914857005994
library,;0.0012195504702882498
which;0.0006057535029581564
has;0.0007261344486255033
built;0.0006480246284475457
in;0.000689188188997475
methods;0.0007293367946549735
for;0.0007146356456690652
pre-training,;0.0033782986605075203
inference,;0.0010616080839391587
and;0.0007429258577872134
deploying;0.0006422708243341933
BERT.;0.0021927735560924202
â€˜**;0.0023601311567660105
from;0.000753686387278511
transformers;0.0011352987184736084
import;0.0007046036450116422
AutoTokenizer,;0.0026825691416661043
BertModel;0.0008893886354540482
import;0.0007697613764224548
torch;0.0005937370440383872
tokenizer;0.0008505130007595185
=;0.0006416786402523718
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.49500910371247664
model;0.000633794922405206
=;0.0005945253722618667
"BertModel.from_pretrained(""bert-base-uncased"")";0.408801016284829
inputs;0.0006258578791637431
=;0.0006237391536534004
"tokenizer(""Hello,";0.005283306388154847
my;0.0006950868613857859
dog;0.0008986355200255443
is;0.0007306597550001989
"cute"",";0.0014075999520597067
"return_tensors=""pt"")";0.022056693528744353
outputs;0.0006008413777904717
=;0.0006455231134527492
model(**inputs);0.006545960778127792
last_hidden_states;0.0035820424895578592
=;0.0006766863109310188
outputs.last_hidden_state;0.007569566177382053
***;0.0006352145875961926
