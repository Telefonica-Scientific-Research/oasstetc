text;attention
The;1.3020680476477056e-09
easiest;1.2191930800990062e-09
way;1.156208001167129e-09
to;1.4716480264017156e-09
import;1.3528013950019683e-09
the;2.268815291410133e-09
BERT;1.8992688629851162e-09
language;1.894833912892702e-09
model;1.863627947441592e-09
into;1.2252949201909958e-09
python;1.4388244060404655e-09
for;1.125508299054833e-09
use;9.35758754976554e-10
with;1.2795344760063595e-09
PyTorch;1.6098923408015665e-09
is;1.1371522109096572e-09
using;1.3800694728123474e-09
the;1.579091804436519e-09
Hugging;1.277657531939541e-09
Face;1.0604948831272243e-09
Transformer's;2.6171102071999864e-09
library,;7.03059124450351e-08
which;1.2022405809379786e-09
has;1.1411444367619039e-09
built;9.205273901869414e-10
in;9.800356444405244e-10
methods;1.2703683128094161e-09
for;1.5752902204030193e-09
pre-training,;8.518539016417428e-08
inference,;5.016398160228471e-08
and;1.0347485699650676e-09
deploying;1.2741609398839876e-09
BERT.;0.035230573803804746
â€˜**;2.6617024569592045e-09
from;1.4754529273304583e-09
transformers;2.0738164855985815e-09
import;1.5139622976444344e-09
AutoTokenizer,;6.677396681443069e-09
BertModel;1.8208952593454385e-09
import;1.4396020361032612e-09
torch;2.563463676211279e-09
tokenizer;2.384865390604638e-09
=;1.42762398195512e-09
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3162844316844491
model;1.5335710175214202e-09
=;1.4211939859885234e-09
"BertModel.from_pretrained(""bert-base-uncased"")";0.6037454126894837
inputs;1.4410250752747772e-09
=;1.3448540732628713e-09
"tokenizer(""Hello,";7.991102696797e-09
my;1.0948427155188518e-09
dog;1.438786157313407e-09
is;9.973271821823404e-10
"cute"",";1.6765275785906238e-09
"return_tensors=""pt"")";1.0910265710677132e-08
outputs;1.2114415650186907e-09
=;1.3302722800312631e-09
model(**inputs);7.615080795661459e-09
last_hidden_states;2.351222998425698e-09
=;1.302717954877863e-09
outputs.last_hidden_state;0.04473926680887525
***;1.1657200215322802e-09
