text;attention
The;0.2874068847939472
easiest;0.003333606589378787
way;0.0029735383859306893
to;0.0025431829618712343
import;0.004799140627579257
the;0.002822096872429373
BERT;0.016422556938766365
language;0.003653707035123755
model;0.005145663866746928
into;0.00572872264893557
python;0.006424347241939392
for;0.0038039407966831834
use;0.0026916876276747593
with;0.0026030636145773435
PyTorch;0.007859824711018853
is;0.0046722971070622815
using;0.0035092056289468933
the;0.0028179160829424803
Hugging;0.005601772602344413
Face;0.004346504071141418
Transformer's;0.010027504839988974
library,;0.0065591734420654335
which;0.002600136871677318
has;0.0025098210086099025
built;0.002179689085485631
in;0.002886352688046311
methods;0.003006267603413667
for;0.0025435398192129714
pre-training,;0.00873315385643121
inference,;0.0047647539240819655
and;0.0023359471394314613
deploying;0.00364056832727794
BERT.;0.007147412181252821
â€˜**;0.005863764128356434
from;0.006262903934969803
transformers;0.004270906248491137
import;0.0036290713657694306
AutoTokenizer,;0.02146580577975797
BertModel;0.0056390033302030915
import;0.003636290295240936
torch;0.0036818123265513164
tokenizer;0.0038595571334028808
=;0.004355608569017528
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.34165015496882856
model;0.0033149323587306507
=;0.0035765733825480423
"BertModel.from_pretrained(""bert-base-uncased"")";0.07469541906456977
inputs;0.0037807366300298254
=;0.003303602461424698
"tokenizer(""Hello,";0.02088325946321679
my;0.0024165352598853993
dog;0.0027221532006482326
is;0.002239639110561163
"cute"",";0.003308435162536815
"return_tensors=""pt"")";0.010974349752008845
outputs;0.0025114195124806976
=;0.002345983725409838
model(**inputs);0.00675635137952881
last_hidden_states;0.0055760343580705625
=;0.0024751647443122297
outputs.last_hidden_state;0.00468078856038935
***;0.0020297628010432687
