text;attention
The;0.9985664684141994
easiest;5.813542217938254e-06
way;4.787278692211163e-06
to;3.5157164351379987e-06
import;5.609970217475898e-06
the;3.921033595726694e-06
BERT;3.60201320360784e-05
language;4.229041142152864e-06
model;9.266731288718973e-06
into;6.7886458244679034e-06
python;1.684305123980748e-05
for;6.054805356697164e-06
use;4.180148526046431e-06
with;3.493152086241854e-06
PyTorch;2.5495332621618775e-05
is;7.9598346310142e-06
using;4.877809395656184e-06
the;4.061344976819531e-06
Hugging;1.2149828332179034e-05
Face;6.485675244205446e-06
Transformer's;2.852574513285119e-05
library,;9.560811413734878e-06
which;3.667141407829067e-06
has;3.5325547148894677e-06
built;3.033644769815814e-06
in;4.2038271300678595e-06
methods;4.356649948674978e-06
for;3.6870245729546805e-06
pre-training,;1.6596413931120356e-05
inference,;6.5047895973412605e-06
and;3.173755943059755e-06
deploying;4.031503269116585e-06
BERT.;1.3924893804477928e-05
â€˜**;6.997250756503736e-06
from;8.211397765176436e-06
transformers;5.318305929595516e-06
import;5.597698861199037e-06
AutoTokenizer,;2.5761640332485016e-05
BertModel;6.034471232330699e-06
import;6.1970979776484165e-06
torch;6.645704984422663e-06
tokenizer;1.1667741188230969e-05
=;1.2023950069597614e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0007834048833970333
model;4.971039852681679e-06
=;4.984047690508948e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.0001279246786998919
inputs;5.872734375081827e-06
=;4.2572578593331935e-06
"tokenizer(""Hello,";5.210485591989968e-05
my;3.106674759693072e-06
dog;3.360198746882266e-06
is;2.868446198511328e-06
"cute"",";5.1332027453503035e-06
"return_tensors=""pt"")";2.0078347408814996e-05
outputs;4.016772748291746e-06
=;3.4303041488159505e-06
model(**inputs);1.6613227120852797e-05
last_hidden_states;7.986676274228853e-06
=;3.6562354997755276e-06
outputs.last_hidden_state;6.272946615230547e-06
***;2.6819671459005413e-06
