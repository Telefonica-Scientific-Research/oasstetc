text;attention
The;0.7742085068213083
easiest;0.00020140385539689368
way;0.00014482029600612452
to;0.00014924323303379866
import;0.000267739066668473
the;0.00047711587451327783
BERT;0.0014372821443344678
language;0.0001412484031199215
model;0.00015697008939581628
into;0.00017175184235662756
python;0.00019863172726397978
for;0.000174159409224152
use;0.00010666897331115321
with;0.00012469251300044058
PyTorch;0.0006269237311387818
is;0.00012786814550510498
using;0.0002029046652090953
the;0.00017820072572758813
Hugging;0.0005182737749220498
Face;0.00014642999091155618
Transformer's;0.0024311308258883205
library,;0.0002807302588686767
which;9.706457773445543e-05
has;0.00011341172016025486
built;0.00010460659421677769
in;0.00011259300431650178
methods;0.00011466951292549087
for;0.00012413667846283429
pre-training,;0.0006470981003810956
inference,;0.0001880926102890307
and;9.14028996787795e-05
deploying;0.0001442668347187641
BERT.;0.0011225637837978337
â€˜**;0.00050810857469346
from;0.00017390085177130266
transformers;0.00017880408466738665
import;0.00020281204585101073
AutoTokenizer,;0.0015774325690542372
BertModel;0.0004800810434119984
import;0.00019635450736011064
torch;0.00016007109409729337
tokenizer;0.00026525333316158113
=;0.00016196036522758993
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1991275815193393
model;0.00011279350459089058
=;0.00010219636192822862
"BertModel.from_pretrained(""bert-base-uncased"")";0.008006744671271784
inputs;0.00014142860167949258
=;0.00010856231515300548
"tokenizer(""Hello,";0.0006659786774683993
my;9.457504817299486e-05
dog;0.00010333172007589091
is;8.6894125401506e-05
"cute"",";0.00012534655301761196
"return_tensors=""pt"")";0.0006390216841143401
outputs;9.241476930581152e-05
=;8.999557520629788e-05
model(**inputs);0.0004726229911557944
last_hidden_states;0.00029026101372978043
=;8.749233773612861e-05
outputs.last_hidden_state;0.000335709241576351
***;7.966813599427393e-05
