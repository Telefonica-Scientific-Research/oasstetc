text;attention
The;0.9999614991859267
easiest;5.544099341362607e-07
way;4.075659211730364e-07
to;3.447477066228993e-07
import;4.195806964250803e-07
the;3.3822518964392614e-07
BERT;1.0441693015143057e-06
language;4.167829228713598e-07
model;3.7938813497135813e-07
into;3.815637216728235e-07
python;4.97240787817569e-07
for;3.744185252337675e-07
use;3.5247230554876165e-07
with;3.70254807723136e-07
PyTorch;1.1255291835775767e-06
is;3.571531165732445e-07
using;4.240129599223837e-07
the;4.475255550215563e-07
Hugging;6.191333203931122e-07
Face;4.6806149999415755e-07
Transformer's;9.184397366984511e-07
library,;4.4401879257235285e-07
which;3.5472939069634124e-07
has;3.683592019474132e-07
built;3.272815047961092e-07
in;3.9057912689899323e-07
methods;3.5598438544217747e-07
for;4.2158845233605116e-07
pre-training,;6.202525121973259e-07
inference,;4.197722775097337e-07
and;3.3845113034941574e-07
deploying;3.4721643679028994e-07
BERT.;4.6841255112328907e-07
â€˜**;4.597366848178183e-07
from;5.95569076873084e-07
transformers;3.7109441531331574e-07
import;4.1462016529697447e-07
AutoTokenizer,;9.034358377111934e-07
BertModel;4.812929205497572e-07
import;5.210687124955967e-07
torch;5.073466387482957e-07
tokenizer;4.8228182868114e-07
=;4.441516895295934e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";7.012325485394393e-06
model;3.553357335800668e-07
=;3.676139934231326e-07
"BertModel.from_pretrained(""bert-base-uncased"")";2.3762282156186036e-06
inputs;4.159746841238614e-07
=;4.271071137344965e-07
"tokenizer(""Hello,";1.4896508347943188e-06
my;3.0909943950009906e-07
dog;3.5066540571052365e-07
is;3.0428782167613697e-07
"cute"",";4.751050422446533e-07
"return_tensors=""pt"")";7.669723449099525e-07
outputs;3.683421974803504e-07
=;3.539461281577163e-07
model(**inputs);8.106630118648474e-07
last_hidden_states;6.834944841232305e-07
=;3.451299418674138e-07
outputs.last_hidden_state;7.129400929153666e-07
***;2.980130417724109e-07
