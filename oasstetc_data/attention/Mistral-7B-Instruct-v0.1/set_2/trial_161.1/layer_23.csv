text;attention
The;0.9999591960154858
easiest;6.22380204524146e-07
way;6.488384085138609e-07
to;4.489429453651281e-07
import;7.202048159340311e-07
the;4.193914052580976e-07
BERT;7.675267801772264e-07
language;3.613799060217981e-07
model;5.317580786800763e-07
into;4.3775242479344055e-07
python;4.5855731541052664e-07
for;4.4412932945087913e-07
use;4.3408361876894707e-07
with;3.736026516515773e-07
PyTorch;7.561492235803108e-07
is;4.6903590635289377e-07
using;5.224937826809334e-07
the;4.2985562117151396e-07
Hugging;5.762272708574181e-07
Face;4.4236137969475297e-07
Transformer's;8.428041480624852e-07
library,;5.964144866966718e-07
which;4.4677736792444976e-07
has;4.3807573364849054e-07
built;4.0722263474534933e-07
in;4.460768694209273e-07
methods;4.7091870218311295e-07
for;4.100690532176653e-07
pre-training,;7.634475445166657e-07
inference,;4.654040372146874e-07
and;3.6072609384024026e-07
deploying;5.119492935338442e-07
BERT.;6.186248730875971e-07
â€˜**;5.614945592362396e-07
from;7.978281824813534e-07
transformers;5.028502110483721e-07
import;5.128892866685431e-07
AutoTokenizer,;8.140469376850612e-07
BertModel;4.2974363184881227e-07
import;6.613192361814847e-07
torch;4.4142441428394886e-07
tokenizer;4.810613904114399e-07
=;5.256274135859632e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";6.497229063926075e-06
model;3.807130235362389e-07
=;3.934372734988576e-07
"BertModel.from_pretrained(""bert-base-uncased"")";2.411669210216892e-06
inputs;4.2818250373293056e-07
=;4.371144821737507e-07
"tokenizer(""Hello,";1.8626033044468792e-06
my;3.767694950755014e-07
dog;4.263024171268428e-07
is;3.645314994575234e-07
"cute"",";4.250348763987468e-07
"return_tensors=""pt"")";8.796858202763509e-07
outputs;3.8869952547596486e-07
=;3.904723571276714e-07
model(**inputs);8.309337381828267e-07
last_hidden_states;5.621187330707161e-07
=;4.3508248335684964e-07
outputs.last_hidden_state;6.188584056772394e-07
***;3.2307913098601705e-07
