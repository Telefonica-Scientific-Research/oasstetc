text;attention
The;0.9998591943129493
easiest;7.40943285478425e-07
way;7.732626972046952e-07
to;6.307820697295218e-07
import;8.024172917488729e-07
the;8.429139739156432e-07
BERT;2.342537448116735e-06
language;6.327718367281644e-07
model;8.998568037772668e-07
into;7.258593404911931e-07
python;1.019791508683889e-06
for;6.532077261453312e-07
use;6.020434748528842e-07
with;6.553453953374997e-07
PyTorch;2.006665382840188e-06
is;8.459127395828842e-07
using;1.040727873984362e-06
the;8.867590139577327e-07
Hugging;1.1933282315911423e-06
Face;9.02795407714096e-07
Transformer's;7.529139695758325e-06
library,;1.2251302605296292e-06
which;5.270228323623335e-07
has;5.913149680357765e-07
built;5.689199443102569e-07
in;5.477200660906117e-07
methods;6.471026995561441e-07
for;6.351450949543719e-07
pre-training,;2.279558353193669e-06
inference,;7.429776346965601e-07
and;4.590264750426399e-07
deploying;7.725222005529886e-07
BERT.;2.8560392762985165e-06
â€˜**;1.2773377819948736e-06
from;6.893838122201776e-07
transformers;8.893871229140908e-07
import;7.86312429872854e-07
AutoTokenizer,;2.101805497813043e-06
BertModel;8.451009838822222e-07
import;8.008631040606577e-07
torch;6.093682452887879e-07
tokenizer;9.186427926479731e-07
=;6.919661602131364e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";6.836607071380143e-05
model;5.33579391831724e-07
=;5.274177160937127e-07
"BertModel.from_pretrained(""bert-base-uncased"")";1.076221455838281e-05
inputs;6.616173049740855e-07
=;5.744768245475591e-07
"tokenizer(""Hello,";1.9878082051823216e-06
my;4.6344577806343506e-07
dog;5.198878240737422e-07
is;4.4371031651544435e-07
"cute"",";6.563480337479352e-07
"return_tensors=""pt"")";2.113119195826332e-06
outputs;4.906569295868249e-07
=;4.889284324354425e-07
model(**inputs);1.4215792358648001e-06
last_hidden_states;1.178583265932617e-06
=;4.687826546567456e-07
outputs.last_hidden_state;1.5409564322738382e-06
***;4.167953026715765e-07
