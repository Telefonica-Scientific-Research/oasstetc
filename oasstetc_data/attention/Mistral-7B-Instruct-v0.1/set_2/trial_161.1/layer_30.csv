text;attention
The;0.990513643839147
easiest;3.79727941729303e-06
way;3.3085212593677947e-06
to;3.1084428837027286e-06
import;3.4201814905735343e-06
the;3.872424468537875e-06
BERT;8.933294108209565e-06
language;2.566933599127615e-06
model;2.543014153508914e-06
into;3.205074524643521e-06
python;3.1192587111187813e-06
for;3.88734112048729e-06
use;2.2083713057008053e-06
with;2.6509503865980255e-06
PyTorch;6.0378976665866415e-06
is;2.708229714585042e-06
using;4.651599808295843e-06
the;4.718242273404486e-06
Hugging;4.8971948443554055e-06
Face;3.4954402241022435e-06
Transformer's;2.971132110217154e-05
library,;5.521859445701251e-06
which;2.3618130455703054e-06
has;2.7059573473485443e-06
built;2.407654662941372e-06
in;2.6464707506372665e-06
methods;2.305701756622246e-06
for;3.195128919265545e-06
pre-training,;1.863685203941816e-05
inference,;4.316901160061624e-06
and;2.262947592810496e-06
deploying;2.5968775268964663e-06
BERT.;4.494814474289726e-06
â€˜**;1.3194206903199966e-05
from;3.9392930137810665e-06
transformers;4.051419840471678e-06
import;4.336668244998191e-06
AutoTokenizer,;3.396628712533971e-05
BertModel;7.344641179377266e-06
import;3.9688709230772815e-06
torch;3.0386168182442308e-06
tokenizer;6.204705347110619e-06
=;3.929553587962861e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.009093573699006617
model;2.4702176358048996e-06
=;2.239520818293553e-06
"BertModel.from_pretrained(""bert-base-uncased"")";7.04819406565196e-05
inputs;3.494981687995703e-06
=;2.5340464286596062e-06
"tokenizer(""Hello,";1.199642430732215e-05
my;2.3723021174405944e-06
dog;2.3970389270215455e-06
is;2.0428313829422474e-06
"cute"",";2.9787385497558134e-06
"return_tensors=""pt"")";1.5398224493151804e-05
outputs;2.2563461921851213e-06
=;2.1004695750889333e-06
model(**inputs);8.263613197399054e-06
last_hidden_states;9.786389054662622e-06
=;1.8492509957272367e-06
outputs.last_hidden_state;7.887828497886282e-06
***;1.964042561042297e-06
