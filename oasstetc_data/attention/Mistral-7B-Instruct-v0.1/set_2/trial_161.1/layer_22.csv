text;attention
The;0.9999735597239418
easiest;4.53079156011239e-07
way;3.52084264804803e-07
to;2.7471482735391423e-07
import;4.673202445772335e-07
the;2.614767652493912e-07
BERT;8.448917845541953e-07
language;2.4506958728641847e-07
model;3.203274926770337e-07
into;3.2036533725111083e-07
python;2.927933111199866e-07
for;2.757485845604691e-07
use;2.621739788639655e-07
with;2.492376867380349e-07
PyTorch;5.282513922471833e-07
is;2.5921506799004394e-07
using;2.8980292990076183e-07
the;2.4941251819086673e-07
Hugging;8.362505525874796e-07
Face;3.0542547320232265e-07
Transformer's;6.418195524720861e-07
library,;2.933067882573227e-07
which;2.574684875901668e-07
has;2.443593713883529e-07
built;2.426746516594857e-07
in;2.9660577109628967e-07
methods;2.8473036309932486e-07
for;2.416879325035375e-07
pre-training,;4.246203631538979e-07
inference,;4.3999663940850976e-07
and;2.423857897333108e-07
deploying;2.620487698420465e-07
BERT.;3.19031089859541e-07
â€˜**;3.0486392667520717e-07
from;3.331014992405563e-07
transformers;3.764192074048969e-07
import;2.861007099689311e-07
AutoTokenizer,;4.850488170005464e-07
BertModel;3.4408600774491897e-07
import;3.625667291631639e-07
torch;3.0639354126311666e-07
tokenizer;3.179381187866638e-07
=;2.55230001386562e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.836442552617849e-06
model;2.670297653875533e-07
=;2.673751754705532e-07
"BertModel.from_pretrained(""bert-base-uncased"")";1.1440804400440959e-06
inputs;3.3402236586214335e-07
=;2.475779924840733e-07
"tokenizer(""Hello,";6.973589846227501e-07
my;2.4796277587839817e-07
dog;2.6139617086459684e-07
is;2.3168941328552707e-07
"cute"",";2.807891313011156e-07
"return_tensors=""pt"")";7.146285689322539e-07
outputs;2.6692790115478996e-07
=;2.429177088035007e-07
model(**inputs);4.1463974112512934e-07
last_hidden_states;4.981036743109866e-07
=;2.398567668325168e-07
outputs.last_hidden_state;3.8191847224350475e-07
***;2.1543337334085188e-07
