text;attention
The;0.0074189070879008695
easiest;0.005377652316027061
way;0.0036423131702949673
to;0.0035179435875472317
import;0.010632344859614803
the;0.04704315507320485
BERT;0.0060937993536793154
language;0.003717523189530133
model;0.0038204334927131214
into;0.0031624676355392596
python;0.028146206124820616
for;0.0032473156635024142
use;0.0031116828856799037
with;0.0032779181466688714
PyTorch;0.011608035388180131
is;0.0029518918891676795
using;0.0032981306376916697
the;0.026212484492286955
Hugging;0.005210159258471736
Face;0.00312098569577544
Transformer's;0.013759182675511083
library,;0.005977399095516262
which;0.002890094404359184
has;0.002894552993128975
built;0.0031321938895858222
in;0.0027243224973949555
methods;0.003133310327775541
for;0.0028458646554069753
pre-training,;0.014663384315708307
inference,;0.007262780612449278
and;0.0028000649623669497
deploying;0.005073702056485487
BERT.;0.0066208023610964265
â€˜**;0.004627003525221374
from;0.002657037814274589
transformers;0.003961054345540609
import;0.004367569440337908
AutoTokenizer,;0.009001429605645676
BertModel;0.004516294408198748
import;0.004126675176356343
torch;0.0036777153372693203
tokenizer;0.0036842677320563615
=;0.002738778149186629
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4755511861154158
model;0.002498363803064153
=;0.0025806305953602034
"BertModel.from_pretrained(""bert-base-uncased"")";0.16324865008432146
inputs;0.002501571235898979
=;0.002487213205316625
"tokenizer(""Hello,";0.007903099396916418
my;0.00233097617094104
dog;0.002310494490969919
is;0.002224710254059191
"cute"",";0.0034892040377253308
"return_tensors=""pt"")";0.008672395436556095
outputs;0.0022479524892696835
=;0.0023439092661696097
model(**inputs);0.005612194285066813
last_hidden_states;0.004010916389109362
=;0.002212351395646837
outputs.last_hidden_state;0.00412117069767438
***;0.001906180321348362
