text;attention
The;0.9999573449339476
easiest;3.346419882344661e-07
way;4.253884477982599e-07
to;2.639417533720013e-07
import;3.621416796510841e-07
the;2.815566872295343e-07
BERT;5.731517916879978e-07
language;2.4015143862227955e-07
model;2.9152475204045094e-07
into;2.9782473042264166e-07
python;3.2277889611644586e-07
for;3.4553184772347586e-07
use;2.7054925028899227e-07
with;2.7914439338903524e-07
PyTorch;5.736135557990344e-07
is;3.1225983922156944e-07
using;3.038195784005921e-07
the;3.214575141812755e-07
Hugging;4.0109671008816955e-07
Face;2.9880064774111616e-07
Transformer's;8.25733233481996e-07
library,;3.761159108219695e-07
which;2.8486412715510297e-07
has;2.511223500145372e-07
built;2.195127183877119e-07
in;2.3852257760558824e-07
methods;3.078052030132749e-07
for;2.613591770066785e-07
pre-training,;5.57911450568296e-07
inference,;3.290696116322843e-07
and;2.2811866922277096e-07
deploying;2.699500963430196e-07
BERT.;4.897784047223462e-07
â€˜**;4.722859126753632e-07
from;4.837365310577197e-07
transformers;3.3148712093165217e-07
import;3.0879810556554507e-07
AutoTokenizer,;5.203074616878124e-07
BertModel;3.6012263622857746e-07
import;3.7801380396414066e-07
torch;2.510466455041885e-07
tokenizer;3.405897953927162e-07
=;2.8767405969656246e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.8501975646796468e-05
model;2.286153101310793e-07
=;2.2413141064129043e-07
"BertModel.from_pretrained(""bert-base-uncased"")";3.107526476771622e-06
inputs;2.7673359171823515e-07
=;2.5609566421673256e-07
"tokenizer(""Hello,";1.0726268920995436e-06
my;2.3311732260331765e-07
dog;2.5996959924038005e-07
is;2.3290995745269717e-07
"cute"",";3.14842500223966e-07
"return_tensors=""pt"")";6.958847455495282e-07
outputs;2.1066769462287202e-07
=;2.2203039465453662e-07
model(**inputs);5.285397652589041e-07
last_hidden_states;4.66551125873343e-07
=;2.1154165825122368e-07
outputs.last_hidden_state;5.432823871971152e-07
***;1.9472280448919307e-07
