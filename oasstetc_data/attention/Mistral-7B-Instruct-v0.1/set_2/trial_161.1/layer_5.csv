text;attention
The;0.017170675049123896
easiest;0.013106289163147378
way;0.010623416459418483
to;0.009330728377381262
import;0.016952959327267928
the;0.009852460570092167
BERT;0.02423453291708301
language;0.010338789798604349
model;0.011590521992133033
into;0.010484910439244854
python;0.010743847272977702
for;0.009531119346371738
use;0.00881609299975748
with;0.00929482304634606
PyTorch;0.01312773312752821
is;0.010721933325545737
using;0.009579522725307469
the;0.00882639066815542
Hugging;0.01739305168630432
Face;0.01681145512942057
Transformer's;0.018341908441192014
library,;0.012427330231311423
which;0.009205827234136462
has;0.00925275469808067
built;0.00874648577972028
in;0.0107768259683564
methods;0.010573900661033316
for;0.010092008136931115
pre-training,;0.019127517878252374
inference,;0.01147008119813139
and;0.008338138289742962
deploying;0.010626707893703857
BERT.;0.011455162586997221
â€˜**;0.013398920315711228
from;0.0145266163350377
transformers;0.013548540956056017
import;0.011108113467028951
AutoTokenizer,;0.019596091648144413
BertModel;0.013175632192135142
import;0.011411512184047339
torch;0.00999782659035396
tokenizer;0.010936399969526129
=;0.01312521712831455
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.20576911241180804
model;0.010052128067565242
=;0.01182118426595521
"BertModel.from_pretrained(""bert-base-uncased"")";0.06682036362521093
inputs;0.010881289980532012
=;0.010653947483676297
"tokenizer(""Hello,";0.023539425476833912
my;0.008388509229145965
dog;0.00904818195472618
is;0.008095786507431492
"cute"",";0.009577031774443492
"return_tensors=""pt"")";0.021113322485182957
outputs;0.00962275411571394
=;0.008590315038082535
model(**inputs);0.013396889932737488
last_hidden_states;0.016008170818722255
=;0.008478382459365464
outputs.last_hidden_state;0.010533161589309845
***;0.007819269578400786
