text;attention
The;0.9201925046285985
easiest;0.00019670939238050015
way;0.00017613668418566008
to;0.00014656395788375831
import;0.0002430127802119887
the;0.0001637081444860177
BERT;0.0031613379239953062
language;0.00017521618483048228
model;0.0003528331874075598
into;0.0002192142229675096
python;0.0003022222841242693
for;0.0001582427140557621
use;0.00015037326888876857
with;0.00016237966287806972
PyTorch;0.0012470783393282682
is;0.00022552883274546982
using;0.00019008055362297065
the;0.00019332595199712425
Hugging;0.0004442657907400992
Face;0.0002245797871006266
Transformer's;0.0010226713494378647
library,;0.00022922835939699743
which;0.00013148843450353932
has;0.0001299328181578788
built;0.00012791898552284704
in;0.00015128732980964463
methods;0.00014426017710432974
for;0.0001400192680053796
pre-training,;0.0003679101252295048
inference,;0.00022471012086753441
and;0.0001245195989952964
deploying;0.00015628908680635615
BERT.;0.0006054432979436132
â€˜**;0.0002827139461802039
from;0.0002920256026231216
transformers;0.0003665569923813483
import;0.00018982335136453208
AutoTokenizer,;0.0022320555273008798
BertModel;0.0005337761030609411
import;0.00020750557568767337
torch;0.0002691465780436377
tokenizer;0.0002951552447584781
=;0.0002578840035368079
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.05652972562608745
model;0.00015276679688890047
=;0.0001376580779184308
"BertModel.from_pretrained(""bert-base-uncased"")";0.00279643406836415
inputs;0.00018628964828506348
=;0.00014897631297015247
"tokenizer(""Hello,";0.000482145108307068
my;0.00012936373059579771
dog;0.00015530141137546418
is;0.00012433159101792512
"cute"",";0.00015917653382502604
"return_tensors=""pt"")";0.0005710555637372552
outputs;0.00015447405281887526
=;0.00012656365530337775
model(**inputs);0.0003650035060784106
last_hidden_states;0.00044468556604973557
=;0.00013432304793192643
outputs.last_hidden_state;0.0002747536553672368
***;0.00011933587793079509
