text;attention
The;0.9999754692135802
easiest;2.0221141936339118e-07
way;2.567212880273291e-07
to;1.8030762594190711e-07
import;1.8601360804967403e-07
the;1.7785660205818385e-07
BERT;4.3735481942861063e-07
language;1.6611296578468122e-07
model;1.795125564748841e-07
into;1.9343647794174237e-07
python;2.171749946947617e-07
for;1.873672767881289e-07
use;1.5496340321168752e-07
with;1.7622129063139206e-07
PyTorch;3.798174227753833e-07
is;1.6394944799510205e-07
using;2.3392493142207473e-07
the;1.8996918061675237e-07
Hugging;2.9518399995825703e-07
Face;1.948532933253486e-07
Transformer's;4.327146541975556e-07
library,;2.525367898773984e-07
which;1.6248706941565625e-07
has;1.5956572469399788e-07
built;1.6269350027953157e-07
in;1.8556434625107175e-07
methods;1.837693002354276e-07
for;1.7140755917113484e-07
pre-training,;3.63875494111276e-07
inference,;2.1242327650623357e-07
and;1.4927475608039603e-07
deploying;1.7812581329259435e-07
BERT.;3.5199337968938587e-07
â€˜**;3.574523941835322e-07
from;2.817922398076791e-07
transformers;2.442512801872385e-07
import;2.0471239573856717e-07
AutoTokenizer,;4.4557309473733433e-07
BertModel;2.9511124754260954e-07
import;2.4356071442004066e-07
torch;2.1089518392160606e-07
tokenizer;2.518854273955628e-07
=;2.03753536252237e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";9.49421371941559e-06
model;1.8654445779349736e-07
=;1.5913235519598735e-07
"BertModel.from_pretrained(""bert-base-uncased"")";1.1849748571612635e-06
inputs;1.872716308796299e-07
=;1.7345447058535352e-07
"tokenizer(""Hello,";5.446877543411371e-07
my;1.5496112873074892e-07
dog;1.8211060071048932e-07
is;1.4990346133747478e-07
"cute"",";1.9959234728155872e-07
"return_tensors=""pt"")";3.927068396997189e-07
outputs;1.7113622739109005e-07
=;1.591077292277231e-07
model(**inputs);3.950945119361017e-07
last_hidden_states;3.0879386692575084e-07
=;1.494175219120347e-07
outputs.last_hidden_state;3.161603977683547e-07
***;1.4315075900131963e-07
