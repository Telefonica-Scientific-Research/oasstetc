text;attention
The;0.009377438848092425
easiest;0.009987407639269886
way;0.008319282148132242
to;0.0071115505360073526
import;0.009920234557544122
the;0.007159906656027585
BERT;0.011364940713958013
language;0.00814118316221874
model;0.007419528881844129
into;0.007759091628297287
python;0.008407206683163426
for;0.00762430662826666
use;0.007513430311208698
with;0.007296599226466957
PyTorch;0.013048939104842512
is;0.007810669495468031
using;0.007916219832775157
the;0.006973180915024319
Hugging;0.010778459896470381
Face;0.008184211947132619
Transformer's;0.016999349667556687
library,;0.009577171835251303
which;0.007469171740664826
has;0.007092691033440966
built;0.007801038275493456
in;0.0072802132373454165
methods;0.007873168599685938
for;0.0072046242823678745
pre-training,;0.01733003843097309
inference,;0.011409562436390882
and;0.006799375324097165
deploying;0.008587733735581663
BERT.;0.012024709557946503
â€˜**;0.011834884630862855
from;0.00783304379560928
transformers;0.008695753635826593
import;0.007984450104363917
AutoTokenizer,;0.015633500716480578
BertModel;0.008858780782074752
import;0.007316980068362268
torch;0.00811973637666817
tokenizer;0.009148920845219512
=;0.007916197184320942
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.28634095405271576
model;0.00683695226836295
=;0.007694295373000613
"BertModel.from_pretrained(""bert-base-uncased"")";0.14884522326574431
inputs;0.00723945093115581
=;0.0073904936332991516
"tokenizer(""Hello,";0.01954784423042254
my;0.006929286038847338
dog;0.00692114415638231
is;0.0066192678006343035
"cute"",";0.009582119618202194
"return_tensors=""pt"")";0.022466979021336157
outputs;0.0072549177864947864
=;0.007120305575545284
model(**inputs);0.013878604914220547
last_hidden_states;0.013900358118419508
=;0.006760110450461184
outputs.last_hidden_state;0.015721823972859598
***;0.006044983683100391
