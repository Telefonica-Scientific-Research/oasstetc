text;attention
The;0.9976336692837423
easiest;1.4560183169693156e-05
way;1.111110031649602e-05
to;9.163983805035503e-06
import;1.3352893667929889e-05
the;1.0317040706388751e-05
BERT;5.4020916665573784e-05
language;1.231442955417516e-05
model;2.1611937863037913e-05
into;1.8661610681576013e-05
python;2.2669683435495757e-05
for;1.466453472060041e-05
use;1.1311963690027337e-05
with;1.07423777595845e-05
PyTorch;3.252540086883367e-05
is;2.090609138003379e-05
using;1.2351876927838036e-05
the;1.0828528672307072e-05
Hugging;2.0302229976756703e-05
Face;1.2300748112146776e-05
Transformer's;4.8529084746051006e-05
library,;2.350699662483655e-05
which;9.615367826363225e-06
has;9.130292744664865e-06
built;8.151548433256676e-06
in;1.0635578483150165e-05
methods;1.0245023839331126e-05
for;9.280268634523611e-06
pre-training,;2.3562480746163926e-05
inference,;2.0876224555127258e-05
and;8.787337568120918e-06
deploying;1.107703035643253e-05
BERT.;2.9275987853006925e-05
â€˜**;1.565937348526277e-05
from;2.175715742325778e-05
transformers;1.2820517357441336e-05
import;1.5221020911108164e-05
AutoTokenizer,;8.884228919780381e-05
BertModel;2.5712287798319095e-05
import;1.978689677795266e-05
torch;1.3776086679091063e-05
tokenizer;3.329215615001207e-05
=;2.641377895251786e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0011708096070428766
model;1.2663615537190102e-05
=;1.3413758714214466e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.00010259299254378595
inputs;1.2215947232567917e-05
=;1.049474900704765e-05
"tokenizer(""Hello,";4.558833849266818e-05
my;8.692537417669607e-06
dog;9.204596478729912e-06
is;8.371973170229565e-06
"cute"",";1.198676814202068e-05
"return_tensors=""pt"")";3.3805013524741134e-05
outputs;1.0461145654030997e-05
=;9.230627625302015e-06
model(**inputs);2.5417851587936464e-05
last_hidden_states;2.1101370231606877e-05
=;9.294814792834568e-06
outputs.last_hidden_state;1.371355197193739e-05
***;7.5991379732083136e-06
