text;attention
The;3.3671511876125145e-12
easiest;3.4299101002825116e-12
way;3.019363278781223e-12
to;2.9421938329559982e-12
import;2.993857571172932e-12
the;3.2421219766002805e-12
BERT;3.5362879912636024e-12
language;2.589990855110537e-12
model;3.291679645398257e-12
into;4.295759739137892e-12
python;3.1714918859994856e-12
for;3.11023708768422e-12
use;2.7501572788074115e-12
with;2.987634434670967e-12
PyTorch;7.038210429146273e-12
is;3.0065533168589295e-12
using;3.0563762077939265e-12
the;3.58652266302417e-12
Hugging;3.5577544535229134e-12
Face;2.7966255627258973e-12
Transformer's;1.0217210606656859e-11
library,;3.825393428829346e-12
which;2.6851995896375358e-12
has;2.833694487116829e-12
built;3.382379350090256e-12
in;3.0737625374077397e-12
methods;2.9823564513186626e-12
for;3.203553001050863e-12
pre-training,;1.3323060473463702e-11
inference,;3.292166746435697e-12
and;2.950607653847989e-12
deploying;2.8042476651281884e-12
BERT.;0.9999999963377759
â€˜**;8.969170594996751e-12
from;3.2647363887175554e-12
transformers;3.24466763260152e-12
import;8.097140313138964e-12
AutoTokenizer,;1.2533410116279895e-11
BertModel;4.614535822822491e-12
import;3.4253672153343703e-12
torch;2.1618058088618986e-12
tokenizer;3.650774291005104e-12
=;2.9608246143343887e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.1770741108085936e-09
model;3.939656870663966e-12
=;2.555859779127083e-12
"BertModel.from_pretrained(""bert-base-uncased"")";1.1372608387374249e-09
inputs;3.1468775388903135e-12
=;3.1062467439162964e-12
"tokenizer(""Hello,";1.2941965882892136e-11
my;3.2380356717715195e-12
dog;3.508856951225043e-12
is;3.3978495396317606e-12
"cute"",";4.692332201653008e-12
"return_tensors=""pt"")";6.590650884937606e-11
outputs;2.512045142168726e-12
=;3.348195248230967e-12
model(**inputs);1.777743657737184e-11
last_hidden_states;1.3284363508527987e-11
=;2.481559683597068e-12
outputs.last_hidden_state;2.3745355043185367e-11
***;3.0402233899128334e-12
