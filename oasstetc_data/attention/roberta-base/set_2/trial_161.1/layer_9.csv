text;attention
The;1.3254950201769305e-16
easiest;1.3064747240559046e-16
way;1.1581503491519477e-16
to;1.2907058444992866e-16
import;1.355740511712183e-16
the;1.3395462611392168e-16
BERT;1.5507402006797313e-16
language;1.3296734070408208e-16
model;1.8288845865718926e-16
into;1.303134556111468e-16
python;1.2004277295393092e-16
for;1.1760212713325084e-16
use;1.204237093801308e-16
with;1.33222833573704e-16
PyTorch;2.040721236731825e-16
is;1.3575664349839944e-16
using;1.5252085365561992e-16
the;1.3098211893774332e-16
Hugging;2.640189752546496e-16
Face;1.636366274806141e-16
Transformer's;2.690939266095446e-16
library,;1.7370533652272575e-16
which;1.1599208483997526e-16
has;1.2405310693390316e-16
built;1.281830345453766e-16
in;1.2271939145620483e-16
methods;1.3876068410147238e-16
for;1.5522787201158604e-16
pre-training,;3.6474095585151205e-16
inference,;1.6609070944695553e-16
and;1.271371269143583e-16
deploying;1.4799811776192165e-16
BERT.;0.9999999999999296
â€˜**;2.0762930367006858e-16
from;1.1922739865945513e-16
transformers;1.6399238740765463e-16
import;1.4336897018446495e-16
AutoTokenizer,;2.941151744394974e-16
BertModel;2.321850333775515e-16
import;1.3932293060062235e-16
torch;1.247320042658053e-16
tokenizer;1.876238880904368e-16
=;1.396910304946634e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";3.8013281838824726e-14
model;1.5376151913779897e-16
=;1.314609727234858e-16
"BertModel.from_pretrained(""bert-base-uncased"")";1.9988585418469614e-14
inputs;1.4639831995157477e-16
=;1.251881035737364e-16
"tokenizer(""Hello,";5.130851749120689e-16
my;1.4492129415507308e-16
dog;1.2838142706999105e-16
is;1.3312574626983357e-16
"cute"",";1.7992912132552101e-16
"return_tensors=""pt"")";1.2628760314078067e-15
outputs;1.1646271354631513e-16
=;1.2215102879528026e-16
model(**inputs);5.552133413276999e-16
last_hidden_states;3.8048257423901276e-16
=;1.2598284906028792e-16
outputs.last_hidden_state;7.02111235477858e-16
***;1.1848926254118257e-16
