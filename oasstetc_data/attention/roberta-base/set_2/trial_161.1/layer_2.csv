text;attention
The;0.00011818564292354817
easiest;0.0001210524147058343
way;0.0001279078934794191
to;0.00010886349844040186
import;0.00014664765991966958
the;0.00014978605627773795
BERT;0.00020221903968584056
language;0.0001356785565369616
model;0.00014235096663808207
into;0.00012844877582334242
python;0.00014522622865212955
for;0.0001362550192290791
use;0.00014700450282620974
with;0.0001294295437893767
PyTorch;0.0002511838960544634
is;0.00014405447964327682
using;0.00013270687984784908
the;0.00015442330239254356
Hugging;0.00018541098658763567
Face;0.00013498931327553127
Transformer's;0.0003593247198335131
library,;0.00024510756220307256
which;0.0001253948426319964
has;0.00013477117786466283
built;0.00014587645030991143
in;0.00013088966037446561
methods;0.00014377409195726645
for;0.00014611226417955605
pre-training,;0.0005710578114068265
inference,;0.00021706934990087346
and;0.00014694685917611959
deploying;0.0001542332392540159
BERT.;0.000556320073352731
â€˜**;0.00035042625371316667
from;0.0001425505666238052
transformers;0.00020367964014117684
import;0.00020331981775548164
AutoTokenizer,;0.0006946513213224783
BertModel;0.0002514475292149898
import;0.0001846162016097315
torch;0.00011446314948035861
tokenizer;0.00017816468934014266
=;0.00019482264290111395
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6776164281430302
model;0.0001347002010115475
=;0.00018209170090405816
"BertModel.from_pretrained(""bert-base-uncased"")";0.3001400772591606
inputs;0.00014598058296976855
=;0.00016962496033680322
"tokenizer(""Hello,";0.0006620896803477739
my;0.00012021924552307868
dog;0.0001422123674151633
is;0.00011819978895190708
"cute"",";0.00024410971403320884
"return_tensors=""pt"")";0.0053725839861986984
outputs;0.00014047596018244306
=;0.00018866854494799175
model(**inputs);0.001716360621859273
last_hidden_states;0.001622684669239449
=;0.00016188665665324225
outputs.last_hidden_state;0.0024556711269446965
***;0.00012909021901360348
