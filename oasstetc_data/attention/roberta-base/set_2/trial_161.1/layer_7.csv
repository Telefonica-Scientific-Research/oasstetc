text;attention
The;3.959126111716148e-10
easiest;4.50730424684469e-10
way;3.067424956404747e-10
to;3.696491413895953e-10
import;5.389087901386356e-10
the;3.995099344458283e-10
BERT;5.267726790962038e-10
language;3.307411007586345e-10
model;4.702147220796419e-10
into;3.760634198731486e-10
python;3.7343603931546156e-10
for;3.5360771117245073e-10
use;2.993057963024238e-10
with;3.4575739660070505e-10
PyTorch;5.477959256652472e-10
is;3.4956362958061087e-10
using;3.775433915981245e-10
the;5.768364011825086e-10
Hugging;6.963992556148435e-10
Face;4.174193113450109e-10
Transformer's;9.89298824254947e-10
library,;6.717255448698848e-10
which;3.485229322096082e-10
has;4.324059865149052e-10
built;3.569885547190697e-10
in;3.456279978903735e-10
methods;5.169379885333994e-10
for;5.318402974963319e-10
pre-training,;2.076241878155538e-09
inference,;5.564698598815788e-10
and;3.697159068198665e-10
deploying;4.4770045985802525e-10
BERT.;0.9999990379648094
â€˜**;7.965461207381277e-10
from;4.38311147382166e-10
transformers;4.796424744276428e-10
import;4.893253379866237e-10
AutoTokenizer,;9.857542673263221e-10
BertModel;5.018393320268502e-10
import;4.0646245136426187e-10
torch;4.656195134076801e-10
tokenizer;5.510285141144716e-10
=;3.705782848889187e-10
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";5.632058359118499e-07
model;4.954262448234251e-10
=;3.684513339017935e-10
"BertModel.from_pretrained(""bert-base-uncased"")";3.5133612715058944e-07
inputs;4.1129125718475294e-10
=;3.475126840617742e-10
"tokenizer(""Hello,";3.3072989442546566e-09
my;3.4737260706552167e-10
dog;3.8758101652977913e-10
is;3.767165579741233e-10
"cute"",";6.112629640883378e-10
"return_tensors=""pt"")";8.315226370947994e-09
outputs;3.3533107856295207e-10
=;3.4554533582728196e-10
model(**inputs);2.848316211852306e-09
last_hidden_states;2.3947381218101535e-09
=;3.4879151346439546e-10
outputs.last_hidden_state;4.289029574048286e-09
***;3.3184204703077977e-10
