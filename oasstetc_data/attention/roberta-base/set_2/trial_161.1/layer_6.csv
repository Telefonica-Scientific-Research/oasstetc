text;attention
The;2.438411008106259e-12
easiest;2.3403443442726687e-12
way;1.9940704546386763e-12
to;2.1263491166047384e-12
import;3.3130594572476306e-12
the;3.028477534784566e-12
BERT;4.099925275525355e-12
language;2.9599727769692133e-12
model;3.649656951366026e-12
into;2.837563523679453e-12
python;3.9034216220608755e-12
for;2.05530960751423e-12
use;1.86998068375637e-12
with;2.5684913217005636e-12
PyTorch;4.196711863324841e-12
is;2.2617050658929095e-12
using;2.3025172910374272e-12
the;2.9466851904620735e-12
Hugging;2.8573686673976585e-12
Face;2.2922552702711703e-12
Transformer's;7.105191674804751e-12
library,;4.466156412700447e-12
which;2.225785971917317e-12
has;2.5508988819911812e-12
built;2.0521167660879715e-12
in;2.2479845602372055e-12
methods;3.0276023618353025e-12
for;2.553713425916057e-12
pre-training,;8.564884817080092e-12
inference,;3.632286904993527e-12
and;2.0393923702048453e-12
deploying;2.7766567411708888e-12
BERT.;0.9999999936223494
â€˜**;4.099503384504549e-12
from;2.535683036566999e-12
transformers;5.62513043386862e-12
import;2.829904589528802e-12
AutoTokenizer,;1.2659162684480711e-11
BertModel;6.579635885660802e-12
import;2.8693623051970727e-12
torch;3.2302599902469598e-12
tokenizer;5.70776934632661e-12
=;3.4868646824712635e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.8709288485569046e-09
model;3.2374855576099332e-12
=;2.6003608855506485e-12
"BertModel.from_pretrained(""bert-base-uncased"")";3.1521021964346818e-09
inputs;3.240784115242968e-12
=;2.7029841974145774e-12
"tokenizer(""Hello,";1.6097524144899697e-11
my;2.241019902325339e-12
dog;2.1779602977596826e-12
is;2.2599739835062137e-12
"cute"",";3.6388580231926384e-12
"return_tensors=""pt"")";4.888944314515116e-11
outputs;3.605005184144743e-12
=;2.634682138390487e-12
model(**inputs);2.2941584195946573e-11
last_hidden_states;1.5417828008344168e-11
=;2.769579162082933e-12
outputs.last_hidden_state;6.884088598858178e-11
***;2.41500432824889e-12
