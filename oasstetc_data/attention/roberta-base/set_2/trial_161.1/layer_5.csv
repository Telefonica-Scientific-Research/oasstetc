text;attention
The;1.89776399958693e-12
easiest;1.676277475445078e-12
way;1.4504671235669067e-12
to;1.663240313294374e-12
import;2.578812946446295e-12
the;1.8603680787283068e-12
BERT;3.4780046067383705e-12
language;1.6725746148157576e-12
model;3.4678600229804625e-12
into;1.8138841036176002e-12
python;2.4732676202180813e-12
for;1.4568870115116013e-12
use;1.4605571564728617e-12
with;1.657236690893276e-12
PyTorch;3.3188357689636862e-12
is;1.7846897404867563e-12
using;1.7283547682507958e-12
the;2.172394600486105e-12
Hugging;2.1578249396564536e-12
Face;1.4177680542546507e-12
Transformer's;6.6610639046562664e-12
library,;3.0751471921002194e-12
which;1.4786317352907625e-12
has;1.8805687256410084e-12
built;1.5141941399155061e-12
in;1.7400677605846897e-12
methods;2.339515618952371e-12
for;1.896321419976278e-12
pre-training,;5.143141507986043e-12
inference,;2.3604147677180023e-12
and;1.4957022803272432e-12
deploying;2.1334963347621114e-12
BERT.;0.9999999977006035
â€˜**;4.660894583012061e-12
from;2.052574959021851e-12
transformers;3.4398407463977214e-12
import;3.2499033493850478e-12
AutoTokenizer,;7.695883653859885e-12
BertModel;9.39104761241049e-12
import;2.110831795839439e-12
torch;2.3657467701709348e-12
tokenizer;2.9300545187689072e-12
=;2.1564226087276764e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.324833766903739e-09
model;3.72358992650892e-12
=;1.946617984494082e-12
"BertModel.from_pretrained(""bert-base-uncased"")";7.223847044667402e-10
inputs;3.456232396772617e-12
=;2.108549355778678e-12
"tokenizer(""Hello,";9.621426604937562e-12
my;1.30192426499734e-12
dog;1.7392635242631012e-12
is;1.4901871295879774e-12
"cute"",";2.6020507200873754e-12
"return_tensors=""pt"")";3.370240753909496e-11
outputs;3.0520599119130287e-12
=;2.0421420931809036e-12
model(**inputs);2.085298937476561e-11
last_hidden_states;1.6663369596447024e-11
=;1.7749604200032646e-12
outputs.last_hidden_state;3.113457133342959e-11
***;2.006765524322128e-12
