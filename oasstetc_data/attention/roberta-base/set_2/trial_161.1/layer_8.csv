text;attention
The;2.0566469105059963e-17
easiest;2.0280884387434174e-17
way;1.9138255714994856e-17
to;2.1350865828692026e-17
import;2.1566126022415446e-17
the;2.177439948060669e-17
BERT;2.654528415758258e-17
language;1.795336303845093e-17
model;2.4696531519844958e-17
into;1.9392197341887155e-17
python;1.7962523858438615e-17
for;1.774224667602614e-17
use;1.626095453595162e-17
with;1.7105365578278956e-17
PyTorch;3.004436067361515e-17
is;2.0200391684135185e-17
using;1.980027830732588e-17
the;2.0507316629680574e-17
Hugging;3.1654414595769516e-17
Face;2.40181042576485e-17
Transformer's;3.994149722046018e-17
library,;2.9603503439532556e-17
which;1.8166674222272352e-17
has;2.1540447921290874e-17
built;1.8008790984508e-17
in;1.655007747064214e-17
methods;2.2240568197197907e-17
for;2.1260091217503924e-17
pre-training,;6.44770010545265e-17
inference,;2.5692841915960297e-17
and;1.706848765829393e-17
deploying;2.895334486606837e-17
BERT.;0.9999999999999956
â€˜**;3.1964954037554925e-17
from;1.9255364454829852e-17
transformers;2.2249108291178398e-17
import;2.2298554956299702e-17
AutoTokenizer,;3.8716553447010737e-17
BertModel;2.290984688724715e-17
import;2.4055308586886362e-17
torch;1.7276229657242496e-17
tokenizer;2.9542197037079196e-17
=;2.4265144421082376e-17
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.9501163276338785e-15
model;2.914677033638608e-17
=;1.9851063698892492e-17
"BertModel.from_pretrained(""bert-base-uncased"")";1.182794335846784e-15
inputs;2.092181846650988e-17
=;1.8722767849021006e-17
"tokenizer(""Hello,";8.758855509674653e-17
my;2.0304915745879253e-17
dog;1.922409471311888e-17
is;1.782433937388474e-17
"cute"",";2.6399438320894027e-17
"return_tensors=""pt"")";1.3091165111264207e-16
outputs;1.8566729215359038e-17
=;1.9788760068059173e-17
model(**inputs);7.079925099230723e-17
last_hidden_states;5.0247357833059494e-17
=;1.9755390046573375e-17
outputs.last_hidden_state;1.1022327021576122e-16
***;1.8021626534148352e-17
