text;attention
The;4.0700507732070444e-05
easiest;4.4015837278067196e-05
way;3.4410816217463e-05
to;3.815142768187885e-05
import;8.560702299562366e-05
the;4.8327293181273414e-05
BERT;7.438566156334837e-05
language;5.565687459512805e-05
model;6.992768590458596e-05
into;4.877323077433678e-05
python;5.870339953407976e-05
for;3.955532927526288e-05
use;4.0844703244158914e-05
with;4.649094262170121e-05
PyTorch;8.910853962388867e-05
is;6.53893223883878e-05
using;4.975657778944212e-05
the;6.151014068224164e-05
Hugging;6.836100884972405e-05
Face;3.725740302676761e-05
Transformer's;0.00023065316691566577
library,;0.00010516718135505602
which;3.85561463846434e-05
has;5.58865802218578e-05
built;4.079333039264684e-05
in;3.7969501485930556e-05
methods;6.526162847989275e-05
for;6.12710000368623e-05
pre-training,;0.0002170142615760783
inference,;6.217814702987605e-05
and;3.7080233979637685e-05
deploying;4.568245533244943e-05
BERT.;0.5697309086631868
â€˜**;0.00021094404968569487
from;8.602525857646721e-05
transformers;7.924969532178481e-05
import;0.0001309635954815757
AutoTokenizer,;0.00026521121162171956
BertModel;0.00013681976667540778
import;0.00010079441808112202
torch;4.485557727720916e-05
tokenizer;8.531776300435998e-05
=;7.895688476178849e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.17120200577710096
model;0.00010561212698225553
=;6.764411993507743e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.24699594409218179
inputs;7.942795348617907e-05
=;6.52391215325889e-05
"tokenizer(""Hello,";0.0005515689824490353
my;4.3467038202119e-05
dog;4.511170413718037e-05
is;4.264389502920642e-05
"cute"",";9.917894740729532e-05
"return_tensors=""pt"")";0.004573578168934553
outputs;5.507084316442759e-05
=;5.8111314045132896e-05
model(**inputs);0.0013900246118175009
last_hidden_states;0.0005252129569635813
=;5.83795334709259e-05
outputs.last_hidden_state;0.0009572161818237401
***;4.0068389512599905e-05
