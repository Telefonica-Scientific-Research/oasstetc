text;attention
The;0.015065716997850602
easiest;0.01345969783500254
way;0.01344911075829338
to;0.01308983552262029
import;0.015905413424342808
the;0.013832764933470215
BERT;0.019405722200006675
language;0.013634553790843754
model;0.01419898631562561
into;0.013734135918732902
python;0.01482715301100105
for;0.013407567377286204
use;0.012808452456807512
with;0.013537994461110437
PyTorch;0.02006817239255061
is;0.014104472979943416
using;0.013784194361971312
the;0.013843571712864107
Hugging;0.014866687038176632
Face;0.014960231549465667
Transformer's;0.01605566502890346
library,;0.014629017215235783
which;0.013203647438781326
has;0.012909656956523278
built;0.01281687896492273
in;0.013136352280041765
methods;0.013932071922989803
for;0.013191620692874224
pre-training,;0.015339783945004197
inference,;0.013798331826354357
and;0.012517673291868765
deploying;0.013438592785056783
BERT.;0.02014074431583804
â€˜**;0.019086354598461794
from;0.014276614105077025
transformers;0.013911241158233391
import;0.014116129366079588
AutoTokenizer,;0.020982783429103386
BertModel;0.016560653117872436
import;0.01413190555497095
torch;0.015395759838744513
tokenizer;0.014648764670927069
=;0.013665071622965467
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.056507000552351035
model;0.013738427786280238
=;0.013369616655568172
"BertModel.from_pretrained(""bert-base-uncased"")";0.057423269597527804
inputs;0.01480042659950696
=;0.013373543116778177
"tokenizer(""Hello,";0.022268980393081714
my;0.01271232298252109
dog;0.013944120863882586
is;0.012688564515711542
"cute"",";0.014856864060212626
"return_tensors=""pt"")";0.020616947224469783
outputs;0.014068012082724545
=;0.01299797636666993
model(**inputs);0.016069018167627014
last_hidden_states;0.015526165543024974
=;0.012728915556496745
outputs.last_hidden_state;0.015902957073662637
***;0.012537121697106664
