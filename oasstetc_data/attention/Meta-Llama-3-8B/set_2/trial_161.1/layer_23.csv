text;attention
The;0.016749143892724576
easiest;0.015065009173919237
way;0.014338546986911537
to;0.014252312352929267
import;0.01625302163026598
the;0.014453335951725479
BERT;0.02289279242111141
language;0.01477896299866136
model;0.01576350210072233
into;0.015714346427177835
python;0.015257051835789428
for;0.01457332410901008
use;0.01349951272448609
with;0.014383321467913658
PyTorch;0.020087458232122296
is;0.014357018757993731
using;0.01528184035611929
the;0.014557603517955806
Hugging;0.014655073822597934
Face;0.016743950403881477
Transformer's;0.01636233250001936
library,;0.01591141029033847
which;0.014023384196621356
has;0.013847952355987085
built;0.013236148314122785
in;0.014356353003817749
methods;0.01409363641795359
for;0.014649753308262563
pre-training,;0.016234821947335366
inference,;0.01556773427138943
and;0.013170940504557664
deploying;0.014463961115623323
BERT.;0.023748020292904414
â€˜**;0.017607000060144852
from;0.014770964656627346
transformers;0.014248138607640933
import;0.014827402453892878
AutoTokenizer,;0.022083064880445796
BertModel;0.018603502643698002
import;0.014335346275101938
torch;0.01603893380287121
tokenizer;0.015754652586981534
=;0.014105435077405576
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04280270849837253
model;0.013990531589175573
=;0.013922132064581991
"BertModel.from_pretrained(""bert-base-uncased"")";0.030679886929156002
inputs;0.015722310421198742
=;0.013818260259483
"tokenizer(""Hello,";0.019628579104385827
my;0.013346038082756397
dog;0.014141509344070003
is;0.013133589845477274
"cute"",";0.015060525842963492
"return_tensors=""pt"")";0.018849136765456923
outputs;0.014365816585706076
=;0.013354126994856962
model(**inputs);0.01569503782616423
last_hidden_states;0.015412162255025685
=;0.013198925895166823
outputs.last_hidden_state;0.014094371753194381
***;0.01308633121504624
