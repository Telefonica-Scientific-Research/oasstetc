text;attention
The;0.014837319600988217
easiest;0.013548750639316442
way;0.01360580643809318
to;0.012927409316850831
import;0.01724278253640914
the;0.013167897324547453
BERT;0.02636139186036637
language;0.013411101003404657
model;0.016099853358340698
into;0.015000300569433063
python;0.016835404062251532
for;0.014250019838781726
use;0.013053143214007322
with;0.013379519407879533
PyTorch;0.022104807092454142
is;0.01393870583257259
using;0.014432761102527995
the;0.01344723007062637
Hugging;0.013821073977961654
Face;0.014472502287897406
Transformer's;0.018433398885370565
library,;0.014900239260210401
which;0.013245257351581226
has;0.0129941419743548
built;0.012262561668169141
in;0.013863902845262356
methods;0.013776939270868864
for;0.013578618726266766
pre-training,;0.016875200661449537
inference,;0.014098859381679733
and;0.012125674680150335
deploying;0.012703762712739033
BERT.;0.016958016966050274
â€˜**;0.017503902524799858
from;0.015476835749952532
transformers;0.014394804831148595
import;0.015173635595582552
AutoTokenizer,;0.026455472458390788
BertModel;0.020745731549937238
import;0.014238597667399764
torch;0.015278762392555928
tokenizer;0.01534524639799534
=;0.013281996081240305
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0642861053505439
model;0.013866410226988616
=;0.013055647148126303
"BertModel.from_pretrained(""bert-base-uncased"")";0.03007907725139557
inputs;0.01602716457466379
=;0.01274681197993819
"tokenizer(""Hello,";0.020747253634355648
my;0.012324753823228387
dog;0.012652868334839496
is;0.011980484478083512
"cute"",";0.012982083108866435
"return_tensors=""pt"")";0.018972702059486157
outputs;0.015088939756632566
=;0.012822946131679683
model(**inputs);0.01691330990696922
last_hidden_states;0.017349403941793753
=;0.012515233562313124
outputs.last_hidden_state;0.013948891486118428
***;0.011990574076111093
