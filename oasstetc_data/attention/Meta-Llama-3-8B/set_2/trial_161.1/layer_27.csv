text;attention
The;0.013654163724151016
easiest;0.011313858534176451
way;0.010929715269813855
to;0.011028992800479344
import;0.012477355759184695
the;0.011560282589548172
BERT;0.020126040228854572
language;0.012434409285431759
model;0.012459335901431718
into;0.01184908586455962
python;0.014748096606088862
for;0.011553264197980894
use;0.01081094325698177
with;0.011480690635094614
PyTorch;0.019855359273707674
is;0.012099069461145593
using;0.012049758271278509
the;0.011645366186383177
Hugging;0.01482716873304364
Face;0.012875955032848347
Transformer's;0.015295152614739483
library,;0.013870610350742496
which;0.01113539589560977
has;0.010819588202206552
built;0.010479781365827456
in;0.011316773318508623
methods;0.011273335906175743
for;0.011517549239563768
pre-training,;0.014863134474628137
inference,;0.0129162985954415
and;0.010454598711081611
deploying;0.010918509360772658
BERT.;0.018859935544422765
â€˜**;0.016917212036967888
from;0.012509938443071845
transformers;0.012296513493937095
import;0.012179333603850756
AutoTokenizer,;0.024927921034294448
BertModel;0.01874978336066926
import;0.01183848169607151
torch;0.014282336239101101
tokenizer;0.014549620077543905
=;0.011712312764059965
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.14261883702464154
model;0.011339264138671786
=;0.011032370521059098
"BertModel.from_pretrained(""bert-base-uncased"")";0.07028185165874895
inputs;0.012124763717549255
=;0.011673714224302325
"tokenizer(""Hello,";0.019787841884418948
my;0.010513912306980478
dog;0.012074041357968128
is;0.01053767109387096
"cute"",";0.012854318486885433
"return_tensors=""pt"")";0.01837702262792906
outputs;0.011574220750174101
=;0.010643317274900277
model(**inputs);0.014991238631922843
last_hidden_states;0.013277361103992656
=;0.010411441778442372
outputs.last_hidden_state;0.011881760047413613
***;0.010542023428655732
