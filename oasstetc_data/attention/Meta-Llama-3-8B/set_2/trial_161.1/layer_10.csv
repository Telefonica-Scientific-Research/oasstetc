text;attention
The;0.010499175121835146
easiest;0.007583374504669694
way;0.007544053659605359
to;0.0071510578890990025
import;0.010238006643130864
the;0.0085644274477984
BERT;0.04365890249145704
language;0.00915922871382145
model;0.021050319558517293
into;0.016184359452712252
python;0.016457810642859998
for;0.01283440223260611
use;0.00808004973335306
with;0.007598601525946967
PyTorch;0.02254047556859427
is;0.013793034497767039
using;0.009275905316739878
the;0.00857899564100424
Hugging;0.010516013858667642
Face;0.011242409647574877
Transformer's;0.020207167848241274
library,;0.012925493857410826
which;0.007953192667766925
has;0.006893471715220087
built;0.006408620550406595
in;0.007758530590924468
methods;0.007571743417601697
for;0.007653989056104303
pre-training,;0.014207226147627218
inference,;0.01129014407660153
and;0.006901732634572521
deploying;0.007099948983051802
BERT.;0.01728117588952299
â€˜**;0.014077515025315986
from;0.01095960467221205
transformers;0.012104662196780367
import;0.009729680472403765
AutoTokenizer,;0.05884881379205275
BertModel;0.02635024600609462
import;0.011775527256154298
torch;0.009087357905756177
tokenizer;0.010391750502047449
=;0.010327056677703898
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.20305955346435245
model;0.00911264251417239
=;0.008341282270112509
"BertModel.from_pretrained(""bert-base-uncased"")";0.04839875803914906
inputs;0.009276449650002905
=;0.008041861776802663
"tokenizer(""Hello,";0.026233880592347485
my;0.006818681519206894
dog;0.00725281999584278
is;0.006479486033473271
"cute"",";0.00818812882052557
"return_tensors=""pt"")";0.022670469995122018
outputs;0.008997326065971376
=;0.007213969064660672
model(**inputs);0.01523413938409575
last_hidden_states;0.012668025322606651
=;0.007154009011858296
outputs.last_hidden_state;0.007947168506581677
***;0.006556091883781421
