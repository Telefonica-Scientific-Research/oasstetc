text;attention
The;0.01149238822405651
easiest;0.004701877288435147
way;0.0034798772484600903
to;0.01064241448006185
import;0.006329425290687097
the;0.011338732560939912
BERT;0.008829269086888447
language;0.003576584205472478
model;0.00361193185883876
into;0.004881082044329486
python;0.007630861699509498
for;0.011342730189906466
use;0.0034041055674662014
with;0.011051733084813536
PyTorch;0.009792739862175114
is;0.0058944797777561505
using;0.003925603100680849
the;0.00933741987711242
Hugging;0.008266975126023448
Face;0.003268235564315597
Transformer's;0.009314052486755976
library,;0.06035351450740244
which;0.0033769116661850514
has;0.004488223904129433
built;0.0035073076083454746
in;0.008285326507949256
methods;0.0033485108500164334
for;0.008699809001851008
pre-training,;0.0496468835091518
inference,;0.03701504385395313
and;0.009331312030269216
deploying;0.003592977655996022
BERT.;0.03094649911762012
â€˜**;0.004614278619171395
from;0.0055013194535855365
transformers;0.003901101832394078
import;0.004208301764640307
AutoTokenizer,;0.03894604149097678
BertModel;0.003895983101386565
import;0.004155124100504489
torch;0.0047560908945796345
tokenizer;0.0037338141769760595
=;0.007081383551415109
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2790995539051319
model;0.002986567161308585
=;0.006583060047342708
"BertModel.from_pretrained(""bert-base-uncased"")";0.15760851825900205
inputs;0.0030077455501840915
=;0.005606073197271177
"tokenizer(""Hello,";0.033734297222117786
my;0.0038564306209987946
dog;0.002859106918630425
is;0.00395042012944707
"cute"",";0.005064051090547072
"return_tensors=""pt"")";0.011953992690521643
outputs;0.0028326802640876382
=;0.0046133761884834075
model(**inputs);0.006809335005087873
last_hidden_states;0.00380957141707891
=;0.0037980139297330543
outputs.last_hidden_state;0.0038062378697310034
***;0.002522690710110384
