text;attention
The;0.008659531543988078
easiest;0.007129776139069102
way;0.0071475346735237425
to;0.006270465647032131
import;0.009196863575279561
the;0.007024789107454534
BERT;0.05687466177953856
language;0.008042619199881843
model;0.011631067107600123
into;0.008155948659458817
python;0.01206722350815908
for;0.00788986412987003
use;0.006418442454177541
with;0.00641551245345925
PyTorch;0.025779940553775926
is;0.008761714942154981
using;0.007703348464588442
the;0.0073065295348448
Hugging;0.010619129208170265
Face;0.010551516928120718
Transformer's;0.02023816149234938
library,;0.009490704740195815
which;0.007001941535525834
has;0.0063196380792014065
built;0.006025101950235569
in;0.006787617624469301
methods;0.0069318021581853
for;0.006634281848628986
pre-training,;0.012520386930213373
inference,;0.008191144655963746
and;0.005916278902454794
deploying;0.006351438875588875
BERT.;0.01244110290368702
â€˜**;0.014597696097047086
from;0.009825121695398115
transformers;0.010668209464012911
import;0.007786614721725512
AutoTokenizer,;0.02464599780624795
BertModel;0.021598970269497395
import;0.00924007110570028
torch;0.009193044679496217
tokenizer;0.010310208373145478
=;0.007823419228729438
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.31124361883985274
model;0.00786517004604925
=;0.007131749112409704
"BertModel.from_pretrained(""bert-base-uncased"")";0.05814775042252176
inputs;0.008126886334373466
=;0.00624540721441498
"tokenizer(""Hello,";0.01704716693678641
my;0.0058199530997155935
dog;0.006606202257039984
is;0.005553979479147972
"cute"",";0.006909973217690617
"return_tensors=""pt"")";0.02661175830668872
outputs;0.007858458982084517
=;0.006226291161461557
model(**inputs);0.012700171818386775
last_hidden_states;0.011381951466204403
=;0.00609289891267867
outputs.last_hidden_state;0.008436563019990089
***;0.0058086146246557014
