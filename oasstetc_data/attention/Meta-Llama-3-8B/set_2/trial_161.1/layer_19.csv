text;attention
The;0.013906711280006909
easiest;0.012910634125689937
way;0.012647801037293824
to;0.011967457583150747
import;0.014921588698743396
the;0.012618916835786845
BERT;0.033130001977681546
language;0.012968662904007687
model;0.016041499177286762
into;0.01332282329479114
python;0.015406024742685594
for;0.013314756667374633
use;0.012302974004066278
with;0.01249605167107024
PyTorch;0.02303689097824272
is;0.013566740045280445
using;0.013453716902418984
the;0.01277710219754363
Hugging;0.014809235296543224
Face;0.014372437283604435
Transformer's;0.017989664421634993
library,;0.013915636264435472
which;0.012399481836901049
has;0.01216154884967049
built;0.011527481427612682
in;0.012452419313181948
methods;0.012791240650819378
for;0.012298741949764205
pre-training,;0.016189122269073652
inference,;0.014008911039390603
and;0.01161378659923465
deploying;0.012059842245347762
BERT.;0.017516691950659302
â€˜**;0.016249479030822002
from;0.014281036120163147
transformers;0.015004521916420092
import;0.013832437193847448
AutoTokenizer,;0.02562377743505035
BertModel;0.020196879192685226
import;0.014499979318879771
torch;0.013566123875043867
tokenizer;0.016866704443722357
=;0.013458452176871088
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08101537420341709
model;0.013243285173273212
=;0.012612580844315324
"BertModel.from_pretrained(""bert-base-uncased"")";0.03276756191384309
inputs;0.018017282013111233
=;0.012413447524986989
"tokenizer(""Hello,";0.02254942155131418
my;0.011705686504935593
dog;0.012405502072431941
is;0.011620928555824216
"cute"",";0.013558450271634635
"return_tensors=""pt"")";0.02412702141678466
outputs;0.013387987941383149
=;0.012080094267148754
model(**inputs);0.0158392374377288
last_hidden_states;0.015856954908397402
=;0.011795212055968908
outputs.last_hidden_state;0.013121646938552065
***;0.011406338180448354
