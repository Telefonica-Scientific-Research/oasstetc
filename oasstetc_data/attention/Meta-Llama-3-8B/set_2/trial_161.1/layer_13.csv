text;attention
The;0.009614321762616491
easiest;0.00516874504247762
way;0.004908292588920117
to;0.0047748829100317585
import;0.006671571040217092
the;0.0054083332276511804
BERT;0.12381825234462976
language;0.00715743017428702
model;0.015400549983846747
into;0.009442411394412764
python;0.012941681999420428
for;0.008512209250717271
use;0.006043217176550434
with;0.005842131481579619
PyTorch;0.055096540918062174
is;0.007910985899802736
using;0.006866867347322979
the;0.007365094449292514
Hugging;0.00853221970428577
Face;0.008558731607324479
Transformer's;0.020460144842692474
library,;0.007126181448036836
which;0.004820783824830997
has;0.004587338216498239
built;0.004384747780997171
in;0.00487487406006449
methods;0.0048473338584758534
for;0.004770851188213426
pre-training,;0.010221615786306353
inference,;0.007427946381158993
and;0.004725273119776178
deploying;0.0049509862490170675
BERT.;0.01158740982187204
â€˜**;0.00882856925287981
from;0.006994509955395711
transformers;0.007987589736307536
import;0.006444815659019911
AutoTokenizer,;0.03500080548521863
BertModel;0.06876588701099702
import;0.007405551769122203
torch;0.008623697906323995
tokenizer;0.00790447625793112
=;0.006855134237028959
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2340037105516891
model;0.006602081119937267
=;0.005208035651344271
"BertModel.from_pretrained(""bert-base-uncased"")";0.058345741760036596
inputs;0.006869972977867745
=;0.005052269968143958
"tokenizer(""Hello,";0.018213885309410878
my;0.004455750814422243
dog;0.005135517915054924
is;0.004295376065870796
"cute"",";0.0055275487878686585
"return_tensors=""pt"")";0.023682297335457164
outputs;0.006017140453128361
=;0.004558950600504856
model(**inputs);0.008664488504098585
last_hidden_states;0.0097014829485949
=;0.004627630753743325
outputs.last_hidden_state;0.0052221139020278105
***;0.004185010429184547
