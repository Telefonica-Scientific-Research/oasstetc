text;attention
The;0.014741247973295377
easiest;0.013300727746369893
way;0.012680821884938565
to;0.01219002104962975
import;0.01564562290996792
the;0.013066358730905463
BERT;0.030120058847784802
language;0.014271481935138467
model;0.01895926725123372
into;0.013449044485485756
python;0.014677993220408542
for;0.013178828846310763
use;0.012207924628182586
with;0.012623022027878253
PyTorch;0.022427536598736506
is;0.013872377471635264
using;0.014158107093042818
the;0.01321047844730026
Hugging;0.014794544066029331
Face;0.014127683453922023
Transformer's;0.018300917564841167
library,;0.015126154389091423
which;0.012820297945023729
has;0.012477495419014474
built;0.011616817833776027
in;0.012873885561971866
methods;0.01292396015361511
for;0.012684051037955462
pre-training,;0.016033107192724213
inference,;0.014347119238924925
and;0.011484603185149283
deploying;0.01201326024772762
BERT.;0.019855280913111215
â€˜**;0.0251757488492316
from;0.013207716122629982
transformers;0.013734243221327247
import;0.013299325779720083
AutoTokenizer,;0.02295204339078646
BertModel;0.021262168981260973
import;0.012978378098221137
torch;0.014312506225362493
tokenizer;0.015083939024053701
=;0.012845577779947853
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06664684882306401
model;0.013491624152907406
=;0.012427007998597037
"BertModel.from_pretrained(""bert-base-uncased"")";0.037798835231698684
inputs;0.015471156849174456
=;0.012593054195129058
"tokenizer(""Hello,";0.021876139678999693
my;0.011445057304295589
dog;0.011733765383013612
is;0.011206321536215463
"cute"",";0.013245710725255096
"return_tensors=""pt"")";0.022997803566564695
outputs;0.014664941493836976
=;0.012134354951971673
model(**inputs);0.015952156670974132
last_hidden_states;0.017461252589918926
=;0.011704351216738835
outputs.last_hidden_state;0.014385773177703396
***;0.011652097630277304
