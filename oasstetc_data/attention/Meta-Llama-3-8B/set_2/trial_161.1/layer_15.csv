text;attention
The;0.008294804703523685
easiest;0.006883150081793124
way;0.006510201147082634
to;0.0058764620088599066
import;0.00929087522033141
the;0.006904009069776791
BERT;0.03312599998649755
language;0.008644803129385014
model;0.011396259428604054
into;0.009065622150727861
python;0.012428579444079537
for;0.007622403608709136
use;0.006115004178015568
with;0.006226758958592604
PyTorch;0.019629960860809214
is;0.007650746206325986
using;0.007834054155130828
the;0.007511323608683179
Hugging;0.009558111311553171
Face;0.010004734093831402
Transformer's;0.01975396822940845
library,;0.00897124219812312
which;0.006575210725201151
has;0.006209329990162683
built;0.005615897886283876
in;0.006218512228571662
methods;0.006362008857431684
for;0.006062212089256098
pre-training,;0.013973881622508446
inference,;0.008094615878662191
and;0.005240092694439734
deploying;0.005897343520511042
BERT.;0.011146217451844031
â€˜**;0.00986534707423638
from;0.00758104537036495
transformers;0.010249619580377314
import;0.007592918839806739
AutoTokenizer,;0.033598135748198345
BertModel;0.02221088899177578
import;0.007091972551656345
torch;0.008163557246134616
tokenizer;0.00891395336531579
=;0.007053650868416074
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3668096183473717
model;0.007256031219764867
=;0.006270786039357906
"BertModel.from_pretrained(""bert-base-uncased"")";0.05193534632837394
inputs;0.007574758011038866
=;0.0057398808380398425
"tokenizer(""Hello,";0.02120569157872636
my;0.005416321633692719
dog;0.00622284777556109
is;0.005262658776811633
"cute"",";0.006466122641785694
"return_tensors=""pt"")";0.021847713045917327
outputs;0.007187858468891741
=;0.00552080893396548
model(**inputs);0.010594475512951299
last_hidden_states;0.013045560605450765
=;0.005565676343739329
outputs.last_hidden_state;0.008047842498778456
***;0.005014515038781833
