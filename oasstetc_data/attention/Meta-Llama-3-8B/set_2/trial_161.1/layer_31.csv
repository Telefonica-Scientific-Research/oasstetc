text;attention
The;0.004112805380906477
easiest;0.0036578703044541547
way;0.003635474070708444
to;0.003388098333092711
import;0.00431235701292571
the;0.0037460901527372795
BERT;0.006358982918174831
language;0.0036716569352683217
model;0.004186630958339196
into;0.003897930077121213
python;0.004101282215272722
for;0.003954491478220109
use;0.0036656732649415903
with;0.0037954639194841626
PyTorch;0.009017643357479468
is;0.003929162065553539
using;0.00464634067199412
the;0.004059155849243938
Hugging;0.004275370459541625
Face;0.004165311970309432
Transformer's;0.006179989573881211
library,;0.0049405039858778374
which;0.0036078624963998876
has;0.003605789409797902
built;0.003515813016052486
in;0.003652928269869779
methods;0.0037364035598733926
for;0.0039003869777779797
pre-training,;0.007146033571208615
inference,;0.004687072599619371
and;0.0031598760319324474
deploying;0.003787521257853706
BERT.;0.009481885423075683
â€˜**;0.007990843217357397
from;0.004072555515130194
transformers;0.003960911660346202
import;0.00453685205162057
AutoTokenizer,;0.008947851758678559
BertModel;0.007375461446204399
import;0.003898369098329394
torch;0.0042031504975363855
tokenizer;0.004336640280333772
=;0.0037386436614219335
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.41839587288310515
model;0.003970175265745776
=;0.0036173411151385285
"BertModel.from_pretrained(""bert-base-uncased"")";0.29541062072526675
inputs;0.004056363681236956
=;0.0034666549029031637
"tokenizer(""Hello,";0.010661768015257152
my;0.00348025267025
dog;0.0036109364808724074
is;0.0033852240382318687
"cute"",";0.004722249468379515
"return_tensors=""pt"")";0.011163968269009956
outputs;0.0036915006640520854
=;0.0031722748718237583
model(**inputs);0.00811860398821941
last_hidden_states;0.005656512809850725
=;0.0031393104757596823
outputs.last_hidden_state;0.006079952372376942
***;0.003159280546571763
