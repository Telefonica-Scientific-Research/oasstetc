text;attention
The;0.016790689355244817
easiest;0.016096803053092978
way;0.015602286961248095
to;0.014740868940556477
import;0.0192631197167861
the;0.014843337595854371
BERT;0.019200849185214076
language;0.01595516374039894
model;0.015988269842826794
into;0.015406286058534743
python;0.015547828164885858
for;0.014669728609297968
use;0.014878833831026249
with;0.014480287388756747
PyTorch;0.01792768000026345
is;0.015640807708465276
using;0.015666272485354484
the;0.014248467117884864
Hugging;0.015671719602437628
Face;0.015490386442453817
Transformer's;0.01555432306757168
library,;0.01598282987603803
which;0.014723130423562861
has;0.01438612810542092
built;0.014889417796446449
in;0.014797082491248118
methods;0.014730901793517054
for;0.014360091054724373
pre-training,;0.018546253955659984
inference,;0.014844608208135733
and;0.013744006732360283
deploying;0.014349164583627274
BERT.;0.01657051796786588
â€˜**;0.017593865018887433
from;0.015454622027475898
transformers;0.016198399099461497
import;0.015612748198111951
AutoTokenizer,;0.018584501037703524
BertModel;0.015270728673186934
import;0.015036566898318459
torch;0.014375060689461046
tokenizer;0.01535556989621002
=;0.016433843963825728
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.03549523838162829
model;0.01597210269873496
=;0.015820711158460277
"BertModel.from_pretrained(""bert-base-uncased"")";0.023826343735615558
inputs;0.015287618010521923
=;0.016524768512305072
"tokenizer(""Hello,";0.022817976454565538
my;0.014252731327764178
dog;0.013877508392501226
is;0.013638329886704548
"cute"",";0.014398672770513453
"return_tensors=""pt"")";0.019131392709209202
outputs;0.014193962835750864
=;0.01512436099775495
model(**inputs);0.016348739188380017
last_hidden_states;0.015622118583768033
=;0.014037496257456065
outputs.last_hidden_state;0.014616668314928055
***;0.013509212424033055
