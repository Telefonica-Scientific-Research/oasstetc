text;attention
The;0.009645110985349514
easiest;0.008276664547593111
way;0.008604949698785037
to;0.0075194783545308825
import;0.012393825645133848
the;0.008730914259414592
BERT;0.04219830632485847
language;0.009762962583167663
model;0.014586085830691275
into;0.009825109365047118
python;0.013733029983069894
for;0.009079497939320166
use;0.00844600047509134
with;0.00853739157912694
PyTorch;0.02579863427127779
is;0.010394529817076727
using;0.011154094122163062
the;0.010030491903232176
Hugging;0.010795481849216495
Face;0.012920113623098388
Transformer's;0.019322059403695496
library,;0.011542753923160913
which;0.008470289395000179
has;0.007996794579388984
built;0.007304701171010757
in;0.009025922572336283
methods;0.008894947059612178
for;0.008946037441746171
pre-training,;0.014454833266013892
inference,;0.0106799417253642
and;0.007334255866164053
deploying;0.008446792896692984
BERT.;0.014374346035809208
â€˜**;0.013497216169501409
from;0.012262976436512654
transformers;0.011782765846199885
import;0.011887110495155104
AutoTokenizer,;0.03197351353208083
BertModel;0.024816296602355042
import;0.012625160796965207
torch;0.011356774421883485
tokenizer;0.011372398238279381
=;0.009467502205825555
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.24214638205826605
model;0.008920592189498934
=;0.008447893046107907
"BertModel.from_pretrained(""bert-base-uncased"")";0.04552377185967895
inputs;0.009428317058689007
=;0.007570429441603071
"tokenizer(""Hello,";0.018912422503209746
my;0.007250226825462494
dog;0.007761892149436134
is;0.007059454027414282
"cute"",";0.008254883550486652
"return_tensors=""pt"")";0.024169309441111342
outputs;0.009227029412950099
=;0.007343799769886399
model(**inputs);0.012388141686089725
last_hidden_states;0.011017250163033017
=;0.00731506180274565
outputs.last_hidden_state;0.00981887377325926
***;0.0071762060030731096
