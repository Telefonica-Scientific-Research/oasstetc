text;attention
The;0.014893861241293434
easiest;0.014654105199292338
way;0.012835319452699289
to;0.012554744167531762
import;0.01624470131721496
the;0.012747444888968213
BERT;0.015197367220065247
language;0.013063995480603491
model;0.012305895889813102
into;0.01245639307607678
python;0.012203464543633072
for;0.012416450598240624
use;0.011964858733883671
with;0.011619995115808126
PyTorch;0.01972558659235861
is;0.012512236448630117
using;0.012096692638864478
the;0.012569769335894442
Hugging;0.014225383712376834
Face;0.012494562222014557
Transformer's;0.014757259290624113
library,;0.015575579820751341
which;0.011484214716076848
has;0.011074752707695649
built;0.011284574444065481
in;0.011109459443688408
methods;0.011710491783819783
for;0.01116821813311862
pre-training,;0.01841819165862972
inference,;0.013231529616414961
and;0.01108109501295374
deploying;0.011375366527845631
BERT.;0.018391376335965586
â€˜**;0.01678811556008152
from;0.011473135581021364
transformers;0.013073794103920417
import;0.01359496557196366
AutoTokenizer,;0.018486375448961166
BertModel;0.012991922904462029
import;0.011210750847752396
torch;0.011651689301924546
tokenizer;0.01259232673782645
=;0.011973828669737134
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.10719420333660866
model;0.010546143466578192
=;0.011402087966495084
"BertModel.from_pretrained(""bert-base-uncased"")";0.08381507207333184
inputs;0.011004250447016856
=;0.011142608133677198
"tokenizer(""Hello,";0.025677330141721744
my;0.010943317844151965
dog;0.0107899541152039
is;0.010492835154447811
"cute"",";0.012421089719318431
"return_tensors=""pt"")";0.0314768315655639
outputs;0.011061745358505813
=;0.010863829457546239
model(**inputs);0.018636003980397987
last_hidden_states;0.0147751448261425
=;0.01034347715386971
outputs.last_hidden_state;0.014491252955863884
***;0.009640980208994416
