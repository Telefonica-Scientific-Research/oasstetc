text;attention
The;0.011506472459723568
easiest;0.007791813464609725
way;0.0073671203386201116
to;0.006914792022340558
import;0.010862212344626792
the;0.0075867474011253695
BERT;0.03690205570096522
language;0.010688498572944235
model;0.017419165185149223
into;0.016760728354175487
python;0.022263553567755903
for;0.009948592001793266
use;0.008055195947032825
with;0.007314587773741934
PyTorch;0.030471505991799108
is;0.012648656043158816
using;0.008371436207357497
the;0.007816878948030383
Hugging;0.009434297851750434
Face;0.012820669736828286
Transformer's;0.019496806985230444
library,;0.013666585347125757
which;0.007593025251189603
has;0.006623714229743497
built;0.005762172177721391
in;0.006911381158383004
methods;0.007365361455567433
for;0.006687043167038088
pre-training,;0.013689221422161902
inference,;0.009304579933738585
and;0.006368611447909876
deploying;0.0067101471506307514
BERT.;0.017247358540839648
â€˜**;0.01985842419730443
from;0.010354153900988877
transformers;0.012825596532499822
import;0.009425340005116628
AutoTokenizer,;0.04846732573133221
BertModel;0.023619681577090438
import;0.01204130620611497
torch;0.008162716772340473
tokenizer;0.009952160022938668
=;0.008589431272624739
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.21415782541993839
model;0.008594554803954633
=;0.007374451269281003
"BertModel.from_pretrained(""bert-base-uncased"")";0.061087366631134885
inputs;0.009541024897838393
=;0.007155138423605189
"tokenizer(""Hello,";0.029090976858243467
my;0.006327409271496891
dog;0.006937097410879112
is;0.006072571399505739
"cute"",";0.007857211245991622
"return_tensors=""pt"")";0.023960291849683808
outputs;0.008491371964591618
=;0.006596792910875496
model(**inputs);0.012691454932101899
last_hidden_states;0.010504221016976662
=;0.006297306087239978
outputs.last_hidden_state;0.007812994296400096
***;0.005782814911100949
