text;attention
The;0.0123697974760109
easiest;0.009302428179330254
way;0.008648480657946041
to;0.008116438626903809
import;0.011561653196128587
the;0.00936055062492331
BERT;0.033009549047581216
language;0.010164731960808105
model;0.013791266312890339
into;0.011004508659679136
python;0.011579902260849425
for;0.009519325359750799
use;0.008238230642924031
with;0.009317442244087017
PyTorch;0.015774579913102275
is;0.009130648853607705
using;0.009392625466870214
the;0.008837387457086128
Hugging;0.013581395523052993
Face;0.011223531797384396
Transformer's;0.02947009735323535
library,;0.011076029374557423
which;0.008261238692781846
has;0.00856870563254648
built;0.008111080867660913
in;0.009222950383459274
methods;0.009355813583937196
for;0.009596016908489538
pre-training,;0.01989621869666774
inference,;0.012018453226584886
and;0.007774284472536608
deploying;0.009965984499686192
BERT.;0.015334754448591503
â€˜**;0.012094558372494237
from;0.011671657784060986
transformers;0.011350796683662281
import;0.00967134540601111
AutoTokenizer,;0.03542714880996407
BertModel;0.013866562347194085
import;0.009170192750647154
torch;0.008920349781313827
tokenizer;0.015738069294160134
=;0.009184928513433588
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.20519640159804337
model;0.009702351324987637
=;0.00889138659982198
"BertModel.from_pretrained(""bert-base-uncased"")";0.07185260850782814
inputs;0.009851264381122327
=;0.00890234924720609
"tokenizer(""Hello,";0.021580238097004448
my;0.007805398008598663
dog;0.008578612201457787
is;0.007981693021629734
"cute"",";0.009522213630729167
"return_tensors=""pt"")";0.02196283722185171
outputs;0.008425202543170881
=;0.008031466569592952
model(**inputs);0.017341531589115833
last_hidden_states;0.015532405935625663
=;0.008105777133317933
outputs.last_hidden_state;0.01366386239134076
***;0.007400687852961791
