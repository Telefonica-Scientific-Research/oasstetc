text;attention
The;0.016312958720100898
easiest;0.013438201908612718
way;0.011857247109103556
to;0.011181329852993782
import;0.016475345058476358
the;0.01171560452354161
BERT;0.02239593746576144
language;0.013849861033599389
model;0.015330547621621476
into;0.015143750838164107
python;0.014753468913227322
for;0.012224123505714744
use;0.0110798941494157
with;0.01149186257969264
PyTorch;0.016021442795185272
is;0.013526671257546027
using;0.01205351719890992
the;0.01082028412989684
Hugging;0.015459171715349355
Face;0.016941884126202303
Transformer's;0.0200156055600006
library,;0.0161967586241558
which;0.01175138850685434
has;0.011207347653066822
built;0.01108951096414498
in;0.011943809952506714
methods;0.011835911260540153
for;0.01171436701884991
pre-training,;0.01746951807216457
inference,;0.012926542305978367
and;0.01027212752874091
deploying;0.011755326762670758
BERT.;0.014337483538727576
â€˜**;0.019573491601755436
from;0.01570765492761091
transformers;0.013742153868863214
import;0.012999978939565237
AutoTokenizer,;0.019991945801644536
BertModel;0.015136477549614483
import;0.013186487769585545
torch;0.012191941870512512
tokenizer;0.013544108222167358
=;0.014765030630630758
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.11872451405100101
model;0.013160106190109115
=;0.013833961646948464
"BertModel.from_pretrained(""bert-base-uncased"")";0.049836104262536256
inputs;0.012159112223979071
=;0.01270625198134675
"tokenizer(""Hello,";0.024076173804825633
my;0.010347071775389478
dog;0.010493044827043059
is;0.010001689267356152
"cute"",";0.011064835084974368
"return_tensors=""pt"")";0.017371810978520013
outputs;0.010956570091150001
=;0.010792343449825874
model(**inputs);0.01741753326283837
last_hidden_states;0.017556312285756628
=;0.011283829883692895
outputs.last_hidden_state;0.013173573733926539
***;0.009617087765313398
