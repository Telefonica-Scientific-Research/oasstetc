text;attention
The;0.01033362762788748
easiest;0.008353704306546746
way;0.01002622176900561
to;0.007929624936956496
import;0.009842873950583437
the;0.0087105096722033
BERT;0.015453471591359142
language;0.008206901730493333
model;0.009710622919703328
into;0.009987545176647012
python;0.01079771032697143
for;0.008537666454370582
use;0.00781137188875737
with;0.008591869371300423
PyTorch;0.012819195173033775
is;0.008535414173927785
using;0.010098747843286375
the;0.008423062052510432
Hugging;0.012042077249724444
Face;0.009443796984536568
Transformer's;0.02169042687841899
library,;0.010789367428769526
which;0.00846811604350669
has;0.008040621693232799
built;0.008157158055055192
in;0.008845483914752724
methods;0.009536803605889587
for;0.008767369665972696
pre-training,;0.017063792160341987
inference,;0.010023946554755638
and;0.007526139492616886
deploying;0.008704412140867946
BERT.;0.014557717192513054
â€˜**;0.019037221052553448
from;0.013059737789699818
transformers;0.012335980690321605
import;0.010478559142854905
AutoTokenizer,;0.01916447262882967
BertModel;0.01370509272824364
import;0.010109937638826425
torch;0.009137193686920954
tokenizer;0.013893105573933669
=;0.009923111633451945
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.18341229875793794
model;0.00897581455667999
=;0.008301955769165394
"BertModel.from_pretrained(""bert-base-uncased"")";0.16675424351251975
inputs;0.00889644343930436
=;0.008886896823305056
"tokenizer(""Hello,";0.02116513810409366
my;0.007750858880907008
dog;0.007964277895095887
is;0.007491095559174155
"cute"",";0.009778973733538069
"return_tensors=""pt"")";0.017761588015894208
outputs;0.008389647429463094
=;0.00787689333230178
model(**inputs);0.015186114811107453
last_hidden_states;0.013438536217011417
=;0.007672773701859039
outputs.last_hidden_state;0.014254271263166904
***;0.007370395605340078
