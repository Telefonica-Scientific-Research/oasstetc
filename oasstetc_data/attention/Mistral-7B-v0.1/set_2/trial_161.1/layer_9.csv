text;attention
The;0.02262476342160255
easiest;0.005740967151387032
way;0.005597727179994151
to;0.005016044058226663
import;0.009895921826457528
the;0.005810066464942666
BERT;0.04759376322255451
language;0.006279670683790451
model;0.018159200134185267
into;0.014830486643735173
python;0.012185799936701663
for;0.008031635360832373
use;0.005949467176357114
with;0.005891682341307822
PyTorch;0.022855350407180047
is;0.010906769242981453
using;0.006704559907311307
the;0.005785200790885109
Hugging;0.01570227851316639
Face;0.010202754350814894
Transformer's;0.035552648056112135
library,;0.012046740579792365
which;0.00574578341781311
has;0.005434858240304159
built;0.004744982095965129
in;0.005898800321379192
methods;0.0057381889125965365
for;0.005672091199820233
pre-training,;0.017809666928153393
inference,;0.009432327749488628
and;0.00516012942173994
deploying;0.006749687664202548
BERT.;0.024310701431295642
â€˜**;0.008849687225161799
from;0.006242971418300583
transformers;0.01155745774568078
import;0.007117341229807216
AutoTokenizer,;0.05451070129132343
BertModel;0.015488828991921562
import;0.007704200991017879
torch;0.0069951725517982
tokenizer;0.016675566963755026
=;0.007109581056149237
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.27290965866507255
model;0.006949865635221737
=;0.006199143318486859
"BertModel.from_pretrained(""bert-base-uncased"")";0.05271019230825915
inputs;0.007992664044767502
=;0.005781955520434467
"tokenizer(""Hello,";0.019226620195026533
my;0.004865270836447621
dog;0.005437628981177326
is;0.004917867029601027
"cute"",";0.005911113429501371
"return_tensors=""pt"")";0.020827984201087883
outputs;0.005942851951470383
=;0.005273046896752068
model(**inputs);0.013797421222797902
last_hidden_states;0.011987132080672772
=;0.005496815773613174
outputs.last_hidden_state;0.006781983470843747
***;0.004678560140773036
