text;attention
The;0.014171846742786258
easiest;0.013237567120403354
way;0.012276835804322572
to;0.011129677490848451
import;0.020484462027319773
the;0.011765710767187038
BERT;0.026409686576390715
language;0.011058275359164422
model;0.014903617310260587
into;0.014477851162573656
python;0.013747806710227973
for;0.01242485546953665
use;0.01182860824494873
with;0.011484656846537282
PyTorch;0.018662311814553627
is;0.012231196862259965
using;0.012983273179187581
the;0.011570787733538791
Hugging;0.021923453718513987
Face;0.014728485842403756
Transformer's;0.029665756662603786
library,;0.013852549295681416
which;0.011925058223768435
has;0.011495272787343993
built;0.0112093367601514
in;0.012843483271248679
methods;0.012956978426956808
for;0.01168742079617236
pre-training,;0.01851372531970919
inference,;0.015569752577730772
and;0.011431937124436901
deploying;0.01195937508137619
BERT.;0.015180672613471221
â€˜**;0.016489598323124764
from;0.014291094682798076
transformers;0.01656976441356836
import;0.013158253478420806
AutoTokenizer,;0.019403193772147474
BertModel;0.013699362566047885
import;0.012449555899446864
torch;0.011927035802331783
tokenizer;0.015426381122549125
=;0.01196270314399085
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0967761320641687
model;0.011739070349858693
=;0.011850732618843554
"BertModel.from_pretrained(""bert-base-uncased"")";0.044192925785718894
inputs;0.013826128319308736
=;0.01125613653695125
"tokenizer(""Hello,";0.023846852951516395
my;0.010798896150460636
dog;0.010947164983418073
is;0.010594795703681707
"cute"",";0.012605277537505833
"return_tensors=""pt"")";0.02284015484014889
outputs;0.011434142506723716
=;0.010644427828591261
model(**inputs);0.019041110120720715
last_hidden_states;0.01567727896200039
=;0.01101062736909377
outputs.last_hidden_state;0.015624128395980945
***;0.010124788047265547
