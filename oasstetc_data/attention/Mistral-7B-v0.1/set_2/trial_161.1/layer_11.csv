text;attention
The;0.01576721978025031
easiest;0.0038258824004996288
way;0.0034795190102323514
to;0.003087796271508103
import;0.005529959701350094
the;0.0035846766960818434
BERT;0.01770745513163197
language;0.003883694124901068
model;0.008196702485471486
into;0.008329675303999843
python;0.011324572920442328
for;0.006158987560568154
use;0.00410027686391553
with;0.003849143162592522
PyTorch;0.014722330078952723
is;0.006586902880086492
using;0.004335788111711467
the;0.003944069557398553
Hugging;0.00918621951348586
Face;0.005354238339291869
Transformer's;0.022990917334005212
library,;0.00712532344840543
which;0.0035688473499990363
has;0.0034040161949598297
built;0.0029366115240682562
in;0.003977915842670901
methods;0.0036978640097321395
for;0.0035754549015948513
pre-training,;0.011475755904676213
inference,;0.006825392093657698
and;0.00329574350297301
deploying;0.00393611978108419
BERT.;0.011382824292302281
â€˜**;0.007704877839323688
from;0.006864804649629469
transformers;0.006454203070590715
import;0.006189098499207344
AutoTokenizer,;0.0361190695143332
BertModel;0.009873898565276477
import;0.006326042434967738
torch;0.004648212599909439
tokenizer;0.010521024840964066
=;0.005728927882287969
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.48474756735367935
model;0.004590190364379426
=;0.004284381914617232
"BertModel.from_pretrained(""bert-base-uncased"")";0.07832066948143282
inputs;0.004701320672935802
=;0.004089040805135901
"tokenizer(""Hello,";0.01633962145111058
my;0.003167906890501519
dog;0.0035940789766667354
is;0.003159136942608548
"cute"",";0.0043693716084880225
"return_tensors=""pt"")";0.014436873970625392
outputs;0.003958087601552242
=;0.003335215747162962
model(**inputs);0.014226590449388335
last_hidden_states;0.009627071447682038
=;0.003432046764277806
outputs.last_hidden_state;0.005243730513023373
***;0.002799041073740498
