text;attention
The;0.008839521407401246
easiest;0.004187242901896981
way;0.004034724096764356
to;0.0036354721380806544
import;0.006288465376143091
the;0.003981390124877802
BERT;0.014117155673507418
language;0.004487525458507748
model;0.007648694275828376
into;0.007507561436949766
python;0.008992786134349456
for;0.005397949084087959
use;0.004023808935280048
with;0.0038178835244763428
PyTorch;0.010872625954660936
is;0.006536238929713177
using;0.004605354610679472
the;0.004387931204792265
Hugging;0.007511039428648146
Face;0.006588013746311358
Transformer's;0.021750500565917083
library,;0.008291231266217409
which;0.004172820037479861
has;0.0036939278668065272
built;0.003227984431876955
in;0.004139973773867081
methods;0.004157041336303263
for;0.0038982929035611905
pre-training,;0.013030534539226668
inference,;0.00613170726817685
and;0.003480264977886642
deploying;0.004906726513322298
BERT.;0.010068309313664255
â€˜**;0.008115207230706993
from;0.00602316585151467
transformers;0.008427221643619469
import;0.005231521461384519
AutoTokenizer,;0.02172602153116773
BertModel;0.007713000999998574
import;0.005288372562665877
torch;0.005176863909910903
tokenizer;0.006987855701922806
=;0.005270900312992994
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4650041321742073
model;0.004630471438855973
=;0.004574395016204727
"BertModel.from_pretrained(""bert-base-uncased"")";0.12361055457130324
inputs;0.0054788198712187675
=;0.004258704611436201
"tokenizer(""Hello,";0.022945330379876824
my;0.0036289253574737254
dog;0.0038269016944326726
is;0.0033966494860476325
"cute"",";0.004825498259801464
"return_tensors=""pt"")";0.016994834341171605
outputs;0.004000963776771627
=;0.003562527981330303
model(**inputs);0.009969663450583649
last_hidden_states;0.008107831014710968
=;0.003580185359447442
outputs.last_hidden_state;0.006128468912876881
***;0.003104311859101677
