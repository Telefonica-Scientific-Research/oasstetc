text;attention
The;0.015082670350169481
easiest;0.015032362049440941
way;0.014506933085084387
to;0.014041529301538604
import;0.01578916515799128
the;0.013677957350702487
BERT;0.019324083260399277
language;0.016115876209951193
model;0.015219463678969954
into;0.014395023125778792
python;0.01610626846132704
for;0.014355350091725786
use;0.013999434166363992
with;0.013652852376558075
PyTorch;0.01831105487647086
is;0.014606978696428276
using;0.014772392041201865
the;0.013551641676635789
Hugging;0.015412693802689365
Face;0.013706210110604213
Transformer's;0.01770127296020457
library,;0.01572722012165529
which;0.013692127552494498
has;0.013266000987587904
built;0.013586701706644269
in;0.01401315873148982
methods;0.014065641937568641
for;0.013530651640221344
pre-training,;0.01922721164318763
inference,;0.015458568972849595
and;0.012874100400197385
deploying;0.013690829533066044
BERT.;0.015847368773311415
â€˜**;0.019505151270693598
from;0.015224909825398855
transformers;0.014519398268596105
import;0.015161482965319096
AutoTokenizer,;0.0193471593336508
BertModel;0.014725834005545927
import;0.014743610052509666
torch;0.013845907411318994
tokenizer;0.014229817039992957
=;0.015022899251872918
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06527023968543834
model;0.013555320930454348
=;0.014791153159140114
"BertModel.from_pretrained(""bert-base-uncased"")";0.03285906969084142
inputs;0.013714411102830292
=;0.014678843274496882
"tokenizer(""Hello,";0.02043949435204151
my;0.012880660376464088
dog;0.013173980706771005
is;0.012594317975794371
"cute"",";0.013993148138213686
"return_tensors=""pt"")";0.019294127571022973
outputs;0.013159110264928874
=;0.013796845174446685
model(**inputs);0.016976007443300283
last_hidden_states;0.01803474270275992
=;0.013005987068473564
outputs.last_hidden_state;0.01496640595008444
***;0.012149170177088324
