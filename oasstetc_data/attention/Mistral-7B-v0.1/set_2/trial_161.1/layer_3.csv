text;attention
The;0.013579595300030245
easiest;0.014257120469741433
way;0.012700975051782543
to;0.011938962279459175
import;0.01745967381173395
the;0.01228763545765646
BERT;0.016206851371895634
language;0.014071007866340045
model;0.012971789908643476
into;0.013309998436880936
python;0.014845436672439637
for;0.012234580639645816
use;0.012400447990318033
with;0.012399758960835304
PyTorch;0.01783791056924188
is;0.013536906829935883
using;0.012339531840169813
the;0.011668547718745263
Hugging;0.016108562950609914
Face;0.014556617045998718
Transformer's;0.01836713460975492
library,;0.01483147709970791
which;0.011922500047748504
has;0.011767637755616902
built;0.012393894798659063
in;0.013015259956591266
methods;0.01322519144004295
for;0.012183767239373562
pre-training,;0.019951941812879924
inference,;0.014334336416301881
and;0.01151339051695392
deploying;0.013335064906308739
BERT.;0.014617412757091587
â€˜**;0.01769227592127548
from;0.013189206500358387
transformers;0.014536609535689735
import;0.012957525454209357
AutoTokenizer,;0.019037578715527873
BertModel;0.013999479352424096
import;0.012150485590185278
torch;0.01353737885274118
tokenizer;0.013774180067117022
=;0.012413940463357769
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0951251634628208
model;0.012112099220516013
=;0.012079780669845629
"BertModel.from_pretrained(""bert-base-uncased"")";0.05567119496786597
inputs;0.012727025828306903
=;0.012079945013482474
"tokenizer(""Hello,";0.025116216976573756
my;0.011469247217938542
dog;0.011959213035921023
is;0.011025337663728338
"cute"",";0.013142983178972268
"return_tensors=""pt"")";0.02495384391656989
outputs;0.012361261060368244
=;0.011555310750558287
model(**inputs);0.01943423694684665
last_hidden_states;0.018430439021880907
=;0.011329180826485574
outputs.last_hidden_state;0.017307635959018953
***;0.010660303300278442
