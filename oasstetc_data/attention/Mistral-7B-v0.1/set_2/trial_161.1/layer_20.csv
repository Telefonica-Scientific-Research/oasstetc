text;attention
The;0.00659892225412873
easiest;0.006380995282776553
way;0.005763576743360694
to;0.005676583487161089
import;0.010359211833849918
the;0.006518765803700529
BERT;0.03800943770305204
language;0.006955095372790568
model;0.011658312865092257
into;0.007650676516273688
python;0.008590306045234919
for;0.0069811543485185975
use;0.006285995291547628
with;0.006313597007453312
PyTorch;0.014239571705980173
is;0.006968857512620502
using;0.007411697693257978
the;0.006286729602817856
Hugging;0.0132187246395369
Face;0.007960643073572506
Transformer's;0.03296714788202479
library,;0.007860341777571073
which;0.005780539782451292
has;0.005947420072187498
built;0.005680395441730408
in;0.006083856787903757
methods;0.0062880150179278164
for;0.006373740109129102
pre-training,;0.016905192308175537
inference,;0.009257066040374119
and;0.005391734961017111
deploying;0.0071113232660116946
BERT.;0.011348975096787306
â€˜**;0.008990714650068508
from;0.007828273478636115
transformers;0.010928731038162432
import;0.007433055151238004
AutoTokenizer,;0.038049184130522315
BertModel;0.010737343948742192
import;0.006494938190143495
torch;0.007853680708611683
tokenizer;0.013191368430035775
=;0.006657698227174786
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3499085317688363
model;0.007133030708878245
=;0.006149814453935385
"BertModel.from_pretrained(""bert-base-uncased"")";0.06166410580740438
inputs;0.009008480802682751
=;0.006432840933758937
"tokenizer(""Hello,";0.018093753111588444
my;0.005618157558904089
dog;0.005828740284291484
is;0.00535002291904477
"cute"",";0.0063910391304145935
"return_tensors=""pt"")";0.02078939247394879
outputs;0.006887885656705017
=;0.005583485111031202
model(**inputs);0.013895618579735312
last_hidden_states;0.013897350855262338
=;0.0058044537308308
outputs.last_hidden_state;0.011234986627827478
***;0.005338718205566425
