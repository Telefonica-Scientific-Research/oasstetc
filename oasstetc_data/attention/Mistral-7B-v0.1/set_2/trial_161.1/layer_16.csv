text;attention
The;0.006073318940485914
easiest;0.002615647913857161
way;0.0026226968067744037
to;0.0022037233510153113
import;0.004065233929659052
the;0.002522055851949386
BERT;0.013793206096333704
language;0.0028381034083118494
model;0.007145890671756207
into;0.004058997621115583
python;0.006466949080142232
for;0.0033051819906217157
use;0.002502562332386242
with;0.002601219271233873
PyTorch;0.01205178588613696
is;0.003828654236294996
using;0.002973948391084185
the;0.0029874678914510572
Hugging;0.0088306676909671
Face;0.004363442895305668
Transformer's;0.01695714427371459
library,;0.004194666572458908
which;0.0023922730600457257
has;0.002290633584137153
built;0.00208679280374402
in;0.002509800388284574
methods;0.002601198495320256
for;0.0024714710558747526
pre-training,;0.011328015589412882
inference,;0.004527736593311726
and;0.00215715935206437
deploying;0.002760046982352071
BERT.;0.006353270453893937
â€˜**;0.004527582835136238
from;0.005802153380470756
transformers;0.004880522867409476
import;0.003454172568874406
AutoTokenizer,;0.07095384975914822
BertModel;0.007783423696139978
import;0.0043482367417198384
torch;0.004079278248813904
tokenizer;0.009462559145094118
=;0.0032821037165635756
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5687439963679127
model;0.0031120959827986633
=;0.0028866729858976034
"BertModel.from_pretrained(""bert-base-uncased"")";0.057396450825522125
inputs;0.004349669438510039
=;0.0029475526597829657
"tokenizer(""Hello,";0.017754405343309607
my;0.0021748606413173706
dog;0.0023044379916407585
is;0.0020635521439583983
"cute"",";0.0028223470178375017
"return_tensors=""pt"")";0.02189778625993112
outputs;0.002747389984620168
=;0.00240311282651577
model(**inputs);0.013076181986658272
last_hidden_states;0.005873317884553902
=;0.0024400239919891487
outputs.last_hidden_state;0.003931516854149318
***;0.002019782392226583
