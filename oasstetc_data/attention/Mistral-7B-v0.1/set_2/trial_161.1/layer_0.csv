text;attention
The;0.004611723055380934
easiest;0.0032128637759834123
way;0.0022842070822501107
to;0.0026180452167624893
import;0.005415312484244516
the;0.015867924982026545
BERT;0.004448860652153531
language;0.0024327807668382355
model;0.0024707810246923684
into;0.002144365859258195
python;0.021992054300134574
for;0.002325579350398334
use;0.0021357050701463014
with;0.002358211951081012
PyTorch;0.008236749574166764
is;0.001966000498785033
using;0.0022551730814122095
the;0.009926063889782228
Hugging;0.0036617820757309694
Face;0.002055556283082691
Transformer's;0.009791720970139646
library,;0.004499683255067931
which;0.0019155005843212167
has;0.0018597323515635417
built;0.002027592125753969
in;0.0018945070213257098
methods;0.0019829723796209054
for;0.001971857680898984
pre-training,;0.011325153968218158
inference,;0.005847133312738593
and;0.001946180597331473
deploying;0.0033659521137219196
BERT.;0.0052096024146384996
â€˜**;0.0032130720407276413
from;0.0018136808452163726
transformers;0.0026551590511467165
import;0.0024905914064323165
AutoTokenizer,;0.007238136573936977
BertModel;0.002876710902086811
import;0.002370936229224456
torch;0.0022326524444256325
tokenizer;0.0024082392376041134
=;0.0019704237259910566
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6161092663354918
model;0.0015745300350513925
=;0.0018098825470739383
"BertModel.from_pretrained(""bert-base-uncased"")";0.1595687901268397
inputs;0.0015292364552312223
=;0.0016967146456789777
"tokenizer(""Hello,";0.0056637324924738545
my;0.0014763206949003674
dog;0.0014334448524854877
is;0.0014286763653742246
"cute"",";0.002227794989373289
"return_tensors=""pt"")";0.005777505520076738
outputs;0.001408835355548402
=;0.0015472013289930513
model(**inputs);0.0035659179734771145
last_hidden_states;0.0025842080527444123
=;0.0014250984147249157
outputs.last_hidden_state;0.002654934302052482
***;0.0011909773059654847
