text;attention
The;0.0015532211803796383
easiest;0.0009398979534193715
way;0.0008531315546233491
to;0.0008386827203957426
import;0.0014787497087498803
the;0.0014518016903215208
BERT;0.004345453005852683
language;0.000995736626335947
model;0.001353063359622632
into;0.0011656085790654714
python;0.0015742496041381792
for;0.001049596556972254
use;0.0008219736703477597
with;0.0009473224575664418
PyTorch;0.002538715642946004
is;0.0010309833147602947
using;0.001257569144547762
the;0.0010433194057127165
Hugging;0.0025222933496078276
Face;0.001302127383389346
Transformer's;0.033440146330699354
library,;0.0012728608041382395
which;0.0008089105428902396
has;0.0008036713715722198
built;0.0007715059347129036
in;0.000859855907356396
methods;0.0008648412031651249
for;0.0009465453124232505
pre-training,;0.004022570179142841
inference,;0.0012404093221554172
and;0.0007180782019273884
deploying;0.0010770166433840933
BERT.;0.01083732123436721
â€˜**;0.0062201527568394544
from;0.001363466202271505
transformers;0.0014683673012345843
import;0.0016767843180877254
AutoTokenizer,;0.016328144146829728
BertModel;0.006112009583767838
import;0.0011772237667246783
torch;0.001412965518577192
tokenizer;0.0018431495098632973
=;0.0012003021198690924
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.591744845813569
model;0.0011849450751347816
=;0.0009554523360225038
"BertModel.from_pretrained(""bert-base-uncased"")";0.25274896967892374
inputs;0.001111795279731151
=;0.0008959403091447573
"tokenizer(""Hello,";0.005279910465240194
my;0.0007871746037916264
dog;0.0007933203789526027
is;0.0007068274568859281
"cute"",";0.0011266016547266742
"return_tensors=""pt"")";0.005119798897610306
outputs;0.0008103987831498569
=;0.0007470546898567149
model(**inputs);0.003437438633580592
last_hidden_states;0.0023628766463879296
=;0.0007229762082854085
outputs.last_hidden_state;0.0032741869571808035
***;0.0006596910110708145
