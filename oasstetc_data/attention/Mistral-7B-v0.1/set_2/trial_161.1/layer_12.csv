text;attention
The;0.007981985000016954
easiest;0.003241774463961695
way;0.0030044180662070815
to;0.0026381301729010856
import;0.004629547570218546
the;0.0031287979152350826
BERT;0.017226488859827277
language;0.0031618375517868105
model;0.009220993806122494
into;0.006769908511048696
python;0.007852282035725185
for;0.005791156350244283
use;0.0035628101513743354
with;0.003075614457152617
PyTorch;0.012537445286787942
is;0.006398640114915522
using;0.003581379568595222
the;0.00330843087863663
Hugging;0.007555621873766172
Face;0.00503045576528805
Transformer's;0.013725638648997819
library,;0.004758150791057157
which;0.003045835082740731
has;0.0028730543626032394
built;0.0024452947769906253
in;0.0033177981946056817
methods;0.003334107252877497
for;0.00341675790628181
pre-training,;0.011175815439844743
inference,;0.006066212325969889
and;0.002924806532918298
deploying;0.0035666110329129488
BERT.;0.011817963227277744
â€˜**;0.004995794162776147
from;0.00535322038360049
transformers;0.00487084090465701
import;0.004109510240718512
AutoTokenizer,;0.06697207847559598
BertModel;0.009319677551173649
import;0.0045991051253214356
torch;0.00403781851223533
tokenizer;0.0085550678620783
=;0.003777736622642554
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5353666973474764
model;0.004360292201258431
=;0.0032451932530649335
"BertModel.from_pretrained(""bert-base-uncased"")";0.0711331941509482
inputs;0.004779793484720464
=;0.003390431590378719
"tokenizer(""Hello,";0.015883492423102843
my;0.0025419578436953376
dog;0.002667305923124123
is;0.002470109262017202
"cute"",";0.0031054695919653123
"return_tensors=""pt"")";0.009405642086964892
outputs;0.0030328054325846824
=;0.0026683775700677512
model(**inputs);0.010301413196345682
last_hidden_states;0.008022018914826437
=;0.0029223187399816932
outputs.last_hidden_state;0.003604027198231754
***;0.0023428459735557574
