text;attention
The;0.0019436809897823658
easiest;0.00077713580294922
way;0.0007442524697517554
to;0.0006540436894476836
import;0.0010469759036285812
the;0.0007531954092102208
BERT;0.0028832718753134512
language;0.000802428708168768
model;0.001571338884139983
into;0.001316732211309763
python;0.001998074805020864
for;0.0011135734778504654
use;0.0007978391815828781
with;0.000782718809668162
PyTorch;0.0037986938375966306
is;0.0012753082743884635
using;0.0010266175484398147
the;0.0009088817185727667
Hugging;0.0022871173666465356
Face;0.0012459736427124376
Transformer's;0.005126836132831482
library,;0.001360489847247422
which;0.0007775969021584467
has;0.0007412029336513647
built;0.0006384360895191223
in;0.0008084282460075998
methods;0.0008110952295534223
for;0.0007339100613200907
pre-training,;0.0034767421315451833
inference,;0.0016361537907445718
and;0.000724363418669551
deploying;0.0008652565488409935
BERT.;0.0020342265444647004
â€˜**;0.0013203264264205741
from;0.0013430294748556187
transformers;0.0015105042681587974
import;0.0012984753147015427
AutoTokenizer,;0.02245358888350942
BertModel;0.0027449604508163607
import;0.0014730990077814173
torch;0.0008424544990285814
tokenizer;0.003915058315622212
=;0.001352595913628225
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.8636614072506336
model;0.0011360844363449182
=;0.0009163879016544891
"BertModel.from_pretrained(""bert-base-uncased"")";0.024144946175251283
inputs;0.0011806532973954768
=;0.0009740887728564398
"tokenizer(""Hello,";0.00549620771574656
my;0.0006754363036240094
dog;0.0007209240092779347
is;0.0006407153062471521
"cute"",";0.0009005918221246987
"return_tensors=""pt"")";0.004537184168443772
outputs;0.0007877710061302916
=;0.0007312981023788413
model(**inputs);0.0032153821602282973
last_hidden_states;0.0019703176242462377
=;0.0007888532120007116
outputs.last_hidden_state;0.0012156927405325116
***;0.0005893729576250673
