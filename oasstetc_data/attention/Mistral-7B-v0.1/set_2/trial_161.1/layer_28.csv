text;attention
The;0.002771732714441276
easiest;0.0026496008570410547
way;0.0026552242544547124
to;0.0024561026921171973
import;0.0036132020783319545
the;0.003315632542455201
BERT;0.007754011231588352
language;0.002797757364421245
model;0.003274181075301858
into;0.003139489443772428
python;0.003625580583367817
for;0.002927741717194088
use;0.002343538623911345
with;0.0027336186817542526
PyTorch;0.005498978079703443
is;0.0034257826014713923
using;0.0032537835091217874
the;0.003160506794404795
Hugging;0.0052665745802986195
Face;0.003402567738265475
Transformer's;0.023038053111328474
library,;0.0038155484325971692
which;0.002607971640094037
has;0.0025119016918329884
built;0.0023342675565840945
in;0.0026967640379512246
methods;0.002809908077792512
for;0.0027925554796879894
pre-training,;0.007264069698982173
inference,;0.0037462693248754126
and;0.0022830054714781446
deploying;0.002693469803554188
BERT.;0.006594911901014725
â€˜**;0.013170451979818788
from;0.004098460500657178
transformers;0.0038255104367949347
import;0.0036700822625731827
AutoTokenizer,;0.015742790581983745
BertModel;0.005919430401727467
import;0.003002489003177688
torch;0.003736434554143065
tokenizer;0.003786790037206185
=;0.003285625093903189
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5502575349398033
model;0.0024191273048487314
=;0.002785040852897717
"BertModel.from_pretrained(""bert-base-uncased"")";0.17563795232651136
inputs;0.002922598719316148
=;0.0027344389450165815
"tokenizer(""Hello,";0.01591383333968344
my;0.0023938861845470916
dog;0.002422875572726491
is;0.002298142506401454
"cute"",";0.003474025253626777
"return_tensors=""pt"")";0.010354080744618197
outputs;0.002592593683837253
=;0.002370414607741569
model(**inputs);0.007271338608527951
last_hidden_states;0.007077760390504642
=;0.0023498065205309554
outputs.last_hidden_state;0.007119028336229599
***;0.0021131529194539996
