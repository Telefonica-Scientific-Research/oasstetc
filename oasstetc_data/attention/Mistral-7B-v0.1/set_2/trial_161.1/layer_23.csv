text;attention
The;0.01322616711977805
easiest;0.01224089632565035
way;0.012267524794795582
to;0.011430043666780437
import;0.017268262569733826
the;0.011393210268018759
BERT;0.01802077782621995
language;0.010419345627156629
model;0.015252601936801197
into;0.013255747563592022
python;0.01418902285772605
for;0.01205994208508859
use;0.01127237908011986
with;0.010890633252152948
PyTorch;0.01548817455734407
is;0.01312552685821162
using;0.013719189138334744
the;0.011354277750044384
Hugging;0.013786117053513725
Face;0.012607789996077976
Transformer's;0.02434443323434473
library,;0.01577136646272352
which;0.012642096313156145
has;0.012072777124776094
built;0.01116041603746744
in;0.012120514830131582
methods;0.013115066808046178
for;0.011707170211506898
pre-training,;0.018057794727333917
inference,;0.01324813334503037
and;0.010536032505370638
deploying;0.014211706155579255
BERT.;0.015674880072679354
â€˜**;0.017338057968466738
from;0.018016073382967304
transformers;0.014249204049627935
import;0.013265577731707893
AutoTokenizer,;0.018865852084484516
BertModel;0.011852430989499405
import;0.012662816447018199
torch;0.011217025199091235
tokenizer;0.014299912399113691
=;0.012333330014131005
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08027737062304015
model;0.01084049805042136
=;0.011211746778587951
"BertModel.from_pretrained(""bert-base-uncased"")";0.09197596803237908
inputs;0.011184979897052017
=;0.011735661352742852
"tokenizer(""Hello,";0.03352811168115455
my;0.010639899309371505
dog;0.011264566912636912
is;0.010390654662512558
"cute"",";0.01189556289120175
"return_tensors=""pt"")";0.019451522447793003
outputs;0.010895551215045117
=;0.010758114969717186
model(**inputs);0.020303351311850407
last_hidden_states;0.014865546816429046
=;0.011025474884692103
outputs.last_hidden_state;0.01606636412671553
***;0.009658755615262188
