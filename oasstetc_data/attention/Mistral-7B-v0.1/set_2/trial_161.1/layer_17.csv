text;attention
The;0.0024789207119767787
easiest;0.0018462378516693899
way;0.0016420727500313628
to;0.0014793397514097956
import;0.003310421984627572
the;0.0017425625763376647
BERT;0.006146563697501586
language;0.0016509664892965044
model;0.002814752299070015
into;0.0021290244621859143
python;0.002958416139897609
for;0.0019152105109684237
use;0.0016691769909363296
with;0.0017733016331316998
PyTorch;0.004313043310669587
is;0.002114878185910075
using;0.002304362080151354
the;0.001885865508746087
Hugging;0.003948791698241581
Face;0.002513884654125768
Transformer's;0.008847304820807195
library,;0.002571635899681288
which;0.0017801005934404582
has;0.0017831397786395864
built;0.0015277410714302512
in;0.0018170751423054615
methods;0.0017236877265540797
for;0.0021078822981375595
pre-training,;0.009228948617139895
inference,;0.002723678641277062
and;0.001466451963280182
deploying;0.0018712415055998522
BERT.;0.0036363363496222214
â€˜**;0.0027389875937858395
from;0.00280827921252653
transformers;0.0028769824566768835
import;0.002666286150962231
AutoTokenizer,;0.011992807373897598
BertModel;0.003028174491784501
import;0.0024309573820253867
torch;0.0019560951937802125
tokenizer;0.005481455459851195
=;0.00225892778555026
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6689636501614872
model;0.0020277878767299322
=;0.0020703723909974658
"BertModel.from_pretrained(""bert-base-uncased"")";0.14237230298843623
inputs;0.0023862593624854266
=;0.001998069277142079
"tokenizer(""Hello,";0.008868281412742965
my;0.0014429125407705173
dog;0.001568051996825618
is;0.0013813160094011039
"cute"",";0.0020699401217295042
"return_tensors=""pt"")";0.009643615884221026
outputs;0.001844779075123089
=;0.0016139626308410266
model(**inputs);0.00770524568029325
last_hidden_states;0.006604201995689846
=;0.001813283894105279
outputs.last_hidden_state;0.004406696050954711
***;0.001257299854382846
