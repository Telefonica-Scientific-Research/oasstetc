text;attention
The;0.007334033210167072
easiest;0.0038535800149785324
way;0.003522801109371181
to;0.003191472842526318
import;0.004540897433508268
the;0.004019513342790714
BERT;0.02301544586382203
language;0.0037235973210777936
model;0.0072594685229967655
into;0.005365642148776856
python;0.008687252068859517
for;0.0043093217351929825
use;0.003628908003555466
with;0.0031074878619109375
PyTorch;0.015083662722133045
is;0.006043534832203778
using;0.0041301353785534595
the;0.0036740581742945974
Hugging;0.0062137107143573934
Face;0.0056923612979284215
Transformer's;0.012833380110299374
library,;0.005798120302329208
which;0.0034398211285938504
has;0.0032313912337034634
built;0.0034524198225645134
in;0.0035988170257143025
methods;0.0032329166473612546
for;0.0032918413844242444
pre-training,;0.00958205103387259
inference,;0.005203839445738876
and;0.0032754163419701633
deploying;0.003098861441544529
BERT.;0.008867521740367411
â€˜**;0.011746749986018684
from;0.006758110749660833
transformers;0.004783936336181928
import;0.005096148428495607
AutoTokenizer,;0.02141955803997797
BertModel;0.009327929892923905
import;0.006454719443992609
torch;0.00425463093376043
tokenizer;0.0054744629729485345
=;0.005656636243608491
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5426654409887993
model;0.004703873115584948
=;0.004154642665714041
"BertModel.from_pretrained(""bert-base-uncased"")";0.07962076344342918
inputs;0.004827022399852299
=;0.004107258881995756
"tokenizer(""Hello,";0.02330453482789432
my;0.003118812188116499
dog;0.003419603276631692
is;0.0028619599112022017
"cute"",";0.0037701444951585175
"return_tensors=""pt"")";0.012998088321410846
outputs;0.003979036953765905
=;0.0032251193380596647
model(**inputs);0.010589159795690698
last_hidden_states;0.0062761186163862726
=;0.003306852436572054
outputs.last_hidden_state;0.004100313760519371
***;0.0026950892981584262
