text;attention
The;0.012314258396398775
easiest;0.005471877552197694
way;0.005520418767850985
to;0.004782633192127316
import;0.0066753934633935275
the;0.00645048562083411
BERT;0.045136479596583
language;0.005179391762916084
model;0.012040975634592407
into;0.010293174299844651
python;0.014649736955951403
for;0.007481782652594158
use;0.005812924155813288
with;0.004806306552312899
PyTorch;0.037283110128532616
is;0.010344990978447299
using;0.006846951893506317
the;0.006702983929030191
Hugging;0.00782432579102263
Face;0.008564883872788425
Transformer's;0.01621293151669187
library,;0.010980448943410935
which;0.005422943057880895
has;0.004640193443686735
built;0.006081809148322806
in;0.005224092190674416
methods;0.005006163820041718
for;0.004790711398141773
pre-training,;0.010681562758984998
inference,;0.007415931008937463
and;0.004648776298365443
deploying;0.0044505099854182315
BERT.;0.017086839024189983
â€˜**;0.014682434278272472
from;0.0077756224166149485
transformers;0.0064355747190042445
import;0.007085801505490602
AutoTokenizer,;0.041028317203629934
BertModel;0.01827624017922778
import;0.010869527238053114
torch;0.0074348325721582675
tokenizer;0.007732146892150134
=;0.008432301711021604
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.38665311859850077
model;0.005888433099529673
=;0.005874492112260321
"BertModel.from_pretrained(""bert-base-uncased"")";0.04144613133007547
inputs;0.005977417267593832
=;0.005611536756585426
"tokenizer(""Hello,";0.020792674270030825
my;0.004482532772914931
dog;0.004580297725563416
is;0.004063590328421627
"cute"",";0.005010659152241105
"return_tensors=""pt"")";0.012788661292640483
outputs;0.005424802933860288
=;0.004395916388939861
model(**inputs);0.010416413643442327
last_hidden_states;0.0071003135012316565
=;0.0043023377857866075
outputs.last_hidden_state;0.004768741931980656
***;0.003843132601292934
