text;attention
The;0.01062790515203596
easiest;0.008380108586827218
way;0.008290790686469384
to;0.0074489998094088355
import;0.013033079070629302
the;0.008919809275917073
BERT;0.04306800404211666
language;0.008963099834485788
model;0.01446314359717596
into;0.009729801111277832
python;0.015225468185085371
for;0.008655151292249478
use;0.008596170334637391
with;0.008343877959285878
PyTorch;0.02771579102484722
is;0.010408696761936882
using;0.010537880982735314
the;0.009724048634479785
Hugging;0.010670694550073887
Face;0.013251388836326101
Transformer's;0.018215250329178447
library,;0.01115137227550715
which;0.008015755596308126
has;0.008340391879886881
built;0.007308516594957733
in;0.008641468924011539
methods;0.008587984485749442
for;0.008609747879650539
pre-training,;0.013563988286723215
inference,;0.010468404688994112
and;0.007213294295608228
deploying;0.007883957218029867
BERT.;0.012870364226939428
â€˜**;0.013699981051178766
from;0.011534068581529073
transformers;0.01078278691111258
import;0.01095960207255688
AutoTokenizer,;0.03584640363941831
BertModel;0.022985361062360772
import;0.0122490411235325
torch;0.010346046010522027
tokenizer;0.010833808126870034
=;0.00891319280423281
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.26114274912732055
model;0.009073823266795355
=;0.00829626035337779
"BertModel.from_pretrained(""bert-base-uncased"")";0.0367937408980848
inputs;0.010245558018191757
=;0.00759934559270557
"tokenizer(""Hello,";0.01787526394462186
my;0.007136913720762416
dog;0.007655341401078394
is;0.0069124491567617305
"cute"",";0.008145674496166029
"return_tensors=""pt"")";0.019695688766522723
outputs;0.008627128363789573
=;0.007169479754143803
model(**inputs);0.01220260831058565
last_hidden_states;0.012499317558110713
=;0.007222279897778542
outputs.last_hidden_state;0.009640130202193207
***;0.00699154937814972
