text;attention
The;0.009283341773779364
easiest;0.0047143073908271875
way;0.004596289390848977
to;0.004401258623514163
import;0.006803867423134447
the;0.005091263373124825
BERT;0.06400622799147201
language;0.006097726380584892
model;0.01021018089525451
into;0.006472475129443901
python;0.011640232396410138
for;0.00584286970635481
use;0.005072575759317104
with;0.004720035571401092
PyTorch;0.03700062871436913
is;0.006201277027563266
using;0.005689838046107048
the;0.0059972602035359745
Hugging;0.007866708449381175
Face;0.008618194781095772
Transformer's;0.017289496831965154
library,;0.0070386915641882705
which;0.0046454617693755945
has;0.004688097828017256
built;0.004651918368526077
in;0.004469413844269684
methods;0.004319503368227765
for;0.004357533978039758
pre-training,;0.01148430465214348
inference,;0.0063624589080485145
and;0.004190757024044706
deploying;0.004310782468267787
BERT.;0.009745639613075797
â€˜**;0.009116165201826508
from;0.0072329349845120065
transformers;0.005765637075662129
import;0.006237644564582973
AutoTokenizer,;0.03097153894746869
BertModel;0.02627855225806295
import;0.007287332019894819
torch;0.006501835681907341
tokenizer;0.006938272179085025
=;0.005886081930233908
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.42340603401375404
model;0.005820797550301654
=;0.004877975702534334
"BertModel.from_pretrained(""bert-base-uncased"")";0.04023352987418978
inputs;0.00688283717109278
=;0.004672333527313261
"tokenizer(""Hello,";0.016769218814210358
my;0.004120693984380448
dog;0.004464023806475208
is;0.003906495955639733
"cute"",";0.004810314621186288
"return_tensors=""pt"")";0.014455307727439741
outputs;0.006118791335028976
=;0.004487768678917165
model(**inputs);0.009292087125700635
last_hidden_states;0.01175503161871439
=;0.00453394541912917
outputs.last_hidden_state;0.005410775507437608
***;0.0038854234776084
