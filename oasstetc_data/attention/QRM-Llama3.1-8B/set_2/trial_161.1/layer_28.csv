text;attention
The;0.010870808487339729
easiest;0.00978384390831567
way;0.009885020682210486
to;0.009499430276474452
import;0.011960803770992817
the;0.010816204297946922
BERT;0.01978908002901179
language;0.009728389096853387
model;0.012530924636612598
into;0.011302716698263606
python;0.011807398700200797
for;0.010587474577943477
use;0.009503273639055117
with;0.009973731566291362
PyTorch;0.01670557057178615
is;0.010306689525231904
using;0.01065897515626861
the;0.010789066075069634
Hugging;0.011523584452723534
Face;0.013106294958415789
Transformer's;0.01632940848346528
library,;0.011935576488422318
which;0.009618254246096851
has;0.010020390735164891
built;0.009264408024066185
in;0.010033397976542601
methods;0.00973316205699832
for;0.010931151175470201
pre-training,;0.01809678749181176
inference,;0.01232560978222981
and;0.008964453343280292
deploying;0.00952417823891229
BERT.;0.015945568364216713
â€˜**;0.017748058189489477
from;0.010328073963845304
transformers;0.010735575694974308
import;0.01046630872993012
AutoTokenizer,;0.020820574717733877
BertModel;0.01631913990123994
import;0.009875511115703534
torch;0.01165469260558618
tokenizer;0.010974473153995802
=;0.009922651571505273
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.21622167211818996
model;0.010294245872248732
=;0.009780784824382745
"BertModel.from_pretrained(""bert-base-uncased"")";0.07299467037986068
inputs;0.010525540309251263
=;0.009559190845673736
"tokenizer(""Hello,";0.020703361510180155
my;0.009334553096729353
dog;0.010187392438773738
is;0.009039727739110362
"cute"",";0.01144692607805663
"return_tensors=""pt"")";0.018770667338765232
outputs;0.010071467652314143
=;0.008751047661938079
model(**inputs);0.015903153725337
last_hidden_states;0.014142001733786448
=;0.008937012854812796
outputs.last_hidden_state;0.011582227636445347
***;0.009057669026454567
