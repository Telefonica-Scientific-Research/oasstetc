text;attention
The;0.014006985990185416
easiest;0.013432624139310637
way;0.01264666275422532
to;0.012180170346365376
import;0.016793772298226126
the;0.013393314368269299
BERT;0.02683896515213629
language;0.01333462391891917
model;0.015329865384377295
into;0.015259327305287248
python;0.015552257544873568
for;0.013841825206000434
use;0.01268675614896918
with;0.013693974604476904
PyTorch;0.021071708025329483
is;0.013249389335431386
using;0.01359048167026925
the;0.01325406928324524
Hugging;0.014239694633380054
Face;0.015625549493647785
Transformer's;0.021051208613718068
library,;0.014969835669227277
which;0.013273055409610317
has;0.01399108677245733
built;0.011688206071854895
in;0.013090370371435846
methods;0.012810275594116143
for;0.01389807264041184
pre-training,;0.017207777364374687
inference,;0.014507448724179937
and;0.011837814956504157
deploying;0.012384162608262837
BERT.;0.0168124917930105
â€˜**;0.02185438163305116
from;0.013291605461886843
transformers;0.013048467905579262
import;0.012955433903836707
AutoTokenizer,;0.022014684344411846
BertModel;0.018658835978707045
import;0.013667976649069288
torch;0.014332471331300662
tokenizer;0.014333367073823081
=;0.013258564181033255
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07574794716045141
model;0.012714250894395264
=;0.012740296057535412
"BertModel.from_pretrained(""bert-base-uncased"")";0.03842480860196602
inputs;0.014195895660156664
=;0.012620910374225405
"tokenizer(""Hello,";0.022298770261221394
my;0.011583666581383392
dog;0.012348547259480027
is;0.011640789784845024
"cute"",";0.013038689672503052
"return_tensors=""pt"")";0.020505104931316565
outputs;0.013326288816399488
=;0.011892305245086743
model(**inputs);0.016697092134251044
last_hidden_states;0.01689297919112437
=;0.011869423534480792
outputs.last_hidden_state;0.014739912807723177
***;0.011762708376666549
