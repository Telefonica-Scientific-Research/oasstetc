text;attention
The;0.013872488276797216
easiest;0.00571918634812236
way;0.005269321404036207
to;0.005250890283175777
import;0.006975575063627745
the;0.005942001503968607
BERT;0.12754918437636467
language;0.007578635093618406
model;0.017825980427693292
into;0.009532363410273586
python;0.015721530133075007
for;0.00813832164435555
use;0.006575052039212346
with;0.006017782894287022
PyTorch;0.07210161333631869
is;0.008970675455007451
using;0.006897602798620588
the;0.007242458279439066
Hugging;0.008844601876421034
Face;0.009361411290244837
Transformer's;0.02303916919861586
library,;0.007441049276782928
which;0.004986959943364575
has;0.0049490369493747355
built;0.00619636104753231
in;0.0052122024762475076
methods;0.005248745652064442
for;0.005118303392253542
pre-training,;0.01016533134716279
inference,;0.007483519742599749
and;0.005150521178768083
deploying;0.00485598585693504
BERT.;0.011106406040834507
â€˜**;0.010684695838387577
from;0.008098243755448936
transformers;0.006425447309853427
import;0.00650898402149105
AutoTokenizer,;0.029333082431124098
BertModel;0.039141964464960745
import;0.008821468448910012
torch;0.008548542364495888
tokenizer;0.007657877391418366
=;0.008098854867770768
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2403217713833892
model;0.007711081563426152
=;0.005535846244830868
"BertModel.from_pretrained(""bert-base-uncased"")";0.04296585975506509
inputs;0.008154589090457978
=;0.005781455977052337
"tokenizer(""Hello,";0.01677975090939013
my;0.004891475863438804
dog;0.005406352172231432
is;0.004558479520645805
"cute"",";0.0056640523227501445
"return_tensors=""pt"")";0.016830863934583256
outputs;0.006987011634719103
=;0.004925045630168211
model(**inputs);0.008920887886631947
last_hidden_states;0.009699848758614226
=;0.004948150891992032
outputs.last_hidden_state;0.005815316294433008
***;0.004442731235123833
