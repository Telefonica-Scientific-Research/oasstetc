text;attention
The;0.012846958852435517
easiest;0.011846325037257627
way;0.012269508336213836
to;0.010874513117469644
import;0.013053660808755741
the;0.01184127982035584
BERT;0.025423707518104022
language;0.01163753151471154
model;0.01318670106383165
into;0.012120647034342368
python;0.014146520540625564
for;0.011501198463280536
use;0.011320566320670378
with;0.011433727788307108
PyTorch;0.02055252587683193
is;0.012942109049060013
using;0.012511481637171511
the;0.012667647091581353
Hugging;0.012753266334636776
Face;0.014413921037339184
Transformer's;0.01597481515501948
library,;0.013559221579746986
which;0.011553865707610142
has;0.011295766803298645
built;0.011000461443966818
in;0.011429297512490036
methods;0.012176178306363555
for;0.011326425751021735
pre-training,;0.014171840598640692
inference,;0.01278440913021575
and;0.010418039772941862
deploying;0.010838034822869977
BERT.;0.018648037600533258
â€˜**;0.026147606366754186
from;0.0124506747600719
transformers;0.011967767265422244
import;0.012437916870486662
AutoTokenizer,;0.02166337810520202
BertModel;0.020710171603573323
import;0.011992587226422722
torch;0.013080963785141503
tokenizer;0.012943385795271442
=;0.01157007444520164
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.14204525257600034
model;0.012090386714114488
=;0.0111726355245404
"BertModel.from_pretrained(""bert-base-uncased"")";0.04974940194883389
inputs;0.012677201705698306
=;0.01103486481978094
"tokenizer(""Hello,";0.018901391219397334
my;0.010747358204007
dog;0.011332596057880328
is;0.010378781397635059
"cute"",";0.012461008958595395
"return_tensors=""pt"")";0.01906282604046775
outputs;0.012617535229594022
=;0.010463145077957235
model(**inputs);0.015440294451397645
last_hidden_states;0.016558317213029368
=;0.01053169707946748
outputs.last_hidden_state;0.012136263202075712
***;0.011114324928278478
