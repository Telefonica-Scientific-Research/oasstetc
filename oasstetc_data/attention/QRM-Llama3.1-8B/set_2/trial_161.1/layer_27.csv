text;attention
The;0.014122764648870936
easiest;0.011637820608607805
way;0.0111751953727024
to;0.010973394219811448
import;0.013156866719531682
the;0.011845562489607891
BERT;0.020191809643594447
language;0.011873442233629794
model;0.013206620285644555
into;0.011907648260863763
python;0.015277560807107245
for;0.011393622490414007
use;0.011067093258651831
with;0.011243106800227142
PyTorch;0.020100905587961355
is;0.013132058117262771
using;0.011620598658169634
the;0.011930199751590804
Hugging;0.015422313395299278
Face;0.01310137844634511
Transformer's;0.01565431898936375
library,;0.014815207987431652
which;0.011190346773630332
has;0.010880936378224928
built;0.010718929712177219
in;0.011260703726327726
methods;0.01161099378523642
for;0.011301550087665539
pre-training,;0.015329969684712935
inference,;0.013540235704146687
and;0.010524288150567787
deploying;0.010766967000192766
BERT.;0.017878863998902025
â€˜**;0.017995809060128078
from;0.012477474740592841
transformers;0.012299894582857063
import;0.01180900077802636
AutoTokenizer,;0.025565989337181138
BertModel;0.019477031868142376
import;0.01179202716607096
torch;0.014839983588038261
tokenizer;0.015707596542382747
=;0.011526741174042105
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.158379878669984
model;0.011723869349014806
=;0.011147412234581492
"BertModel.from_pretrained(""bert-base-uncased"")";0.044689209950173774
inputs;0.012752832591549227
=;0.011479659597123612
"tokenizer(""Hello,";0.018795974165829003
my;0.010553634555486044
dog;0.011915607940089761
is;0.010423929384859235
"cute"",";0.012671967862608253
"return_tensors=""pt"")";0.017575796919151707
outputs;0.011789988170988995
=;0.010434398936154781
model(**inputs);0.015503940473746787
last_hidden_states;0.014180709119525783
=;0.01036266954164031
outputs.last_hidden_state;0.011747600418079509
***;0.010526097507477374
