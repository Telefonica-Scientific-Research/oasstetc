text;attention
The;0.012487718261773369
easiest;0.007240510989437686
way;0.007218543902253518
to;0.006486757764765752
import;0.009384962932591763
the;0.007838432097420276
BERT;0.041345224191156986
language;0.007700554570962013
model;0.01865572561542781
into;0.014658022875825082
python;0.028466307225036333
for;0.010991360413280705
use;0.008241476179455497
with;0.007642192441696367
PyTorch;0.049418917401371054
is;0.013838932980300738
using;0.009047321363477544
the;0.00738589791259993
Hugging;0.011962168405220781
Face;0.014267230599992995
Transformer's;0.021683144423047036
library,;0.013831885481792526
which;0.007287746535967495
has;0.00621673921589533
built;0.006218678032347566
in;0.007318079068140806
methods;0.00784285358904165
for;0.006613013012085493
pre-training,;0.014605909704516525
inference,;0.01034367092861248
and;0.006434464242192144
deploying;0.006844784169438015
BERT.;0.01716721097114069
â€˜**;0.015910843579652847
from;0.00906575106577155
transformers;0.011236030184563291
import;0.009496626840144882
AutoTokenizer,;0.04009700657631278
BertModel;0.022892382648931266
import;0.011570109138511394
torch;0.011118179195383616
tokenizer;0.00818778865875608
=;0.010963725537344695
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.22364076307612143
model;0.007677868565886078
=;0.007602583837152325
"BertModel.from_pretrained(""bert-base-uncased"")";0.04524247472176939
inputs;0.00840680403916773
=;0.006875737491698001
"tokenizer(""Hello,";0.021631506133118542
my;0.005941309664215324
dog;0.006573356188997027
is;0.005850954860918862
"cute"",";0.007592195921899294
"return_tensors=""pt"")";0.020539905164696576
outputs;0.007631115637090242
=;0.00637617686328855
model(**inputs);0.012636858914065553
last_hidden_states;0.008927685395349714
=;0.0062885566749375085
outputs.last_hidden_state;0.007700581995592829
***;0.005638683930398986
