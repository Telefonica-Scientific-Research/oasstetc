text;attention
The;0.013377174388836663
easiest;0.007513342592754409
way;0.007051681837560055
to;0.006350310516424561
import;0.01044843347479549
the;0.007395653945705527
BERT;0.04970169476664878
language;0.008463067805256354
model;0.018782592916324822
into;0.016759765681580732
python;0.024242754887966275
for;0.009603314213300341
use;0.008059816503789494
with;0.007179403875290759
PyTorch;0.041556689334033926
is;0.014335990520899955
using;0.008018132055042721
the;0.007132231789373337
Hugging;0.0090573352887793
Face;0.01237082192634107
Transformer's;0.019489531681099733
library,;0.013280328307390173
which;0.0068643068789010836
has;0.0061701338086731505
built;0.005773268878111498
in;0.006546098795156523
methods;0.007126361467222925
for;0.006236416819014611
pre-training,;0.013494653084324036
inference,;0.01018808176905608
and;0.006207113911176988
deploying;0.0064627880501282745
BERT.;0.019445019895523195
â€˜**;0.01838765105121184
from;0.009247370032060581
transformers;0.00991434194059562
import;0.008244433131213052
AutoTokenizer,;0.04530796000828684
BertModel;0.03113351640080765
import;0.01023117769089838
torch;0.008176929337607296
tokenizer;0.009288663675878987
=;0.008577996541760597
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.22380535151719286
model;0.00812618565259822
=;0.007266344844491275
"BertModel.from_pretrained(""bert-base-uncased"")";0.04689717063202297
inputs;0.010530871767791472
=;0.006976210974040382
"tokenizer(""Hello,";0.022518151710883134
my;0.006065054202673398
dog;0.006761635577199985
is;0.005856221007845589
"cute"",";0.008045178976564399
"return_tensors=""pt"")";0.01938229622591671
outputs;0.0076644563943029905
=;0.0062345166930614405
model(**inputs);0.012363300761043692
last_hidden_states;0.009334307564106827
=;0.006082375023976827
outputs.last_hidden_state;0.007453172676741398
***;0.005442846320742611
