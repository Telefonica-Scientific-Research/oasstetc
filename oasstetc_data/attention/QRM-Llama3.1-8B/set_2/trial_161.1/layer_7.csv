text;attention
The;0.010177162450058837
easiest;0.008019218424944352
way;0.008011091582559549
to;0.007336370190713738
import;0.012435543037398305
the;0.008454415052364363
BERT;0.0387095333743769
language;0.008986273916545506
model;0.022743772960086794
into;0.016051475809109993
python;0.021119579602545267
for;0.01024587618201303
use;0.00781767097661599
with;0.007479914325225296
PyTorch;0.021352544385476544
is;0.0171191121422741
using;0.008989306861630372
the;0.007849454889061039
Hugging;0.00907269229533061
Face;0.010737043331760251
Transformer's;0.02064364694519302
library,;0.019368827054719533
which;0.008363481229423589
has;0.007402577313464119
built;0.006346906409671345
in;0.0076457899414496605
methods;0.007892884055308929
for;0.007418579613102724
pre-training,;0.012386497842565685
inference,;0.010104852567557396
and;0.006662966052708325
deploying;0.0071585466268086545
BERT.;0.02732362267499271
â€˜**;0.02052319833591985
from;0.010056624424554923
transformers;0.011126548789951564
import;0.009996044860911543
AutoTokenizer,;0.024704812233432006
BertModel;0.014997291987579184
import;0.012410342797303194
torch;0.009698027367478648
tokenizer;0.009745176175864964
=;0.01131613415198504
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2138048246882095
model;0.009231606591677806
=;0.008158069892497092
"BertModel.from_pretrained(""bert-base-uncased"")";0.0571965889084274
inputs;0.010534139066686819
=;0.007779850314389782
"tokenizer(""Hello,";0.038001451443539246
my;0.0068652180948350905
dog;0.007205849806894226
is;0.006583786552223417
"cute"",";0.008254860044244826
"return_tensors=""pt"")";0.01743526513339983
outputs;0.009314100884274841
=;0.006883992291121227
model(**inputs);0.014513468313713074
last_hidden_states;0.01100595498326975
=;0.006796894931639366
outputs.last_hidden_state;0.008327586828777673
***;0.006105059988141308
