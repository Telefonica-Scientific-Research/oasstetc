text;attention
The;0.009245393419062135
easiest;0.006779821301750888
way;0.006438839306339387
to;0.005860373392632434
import;0.008570699220186126
the;0.006926041878356493
BERT;0.029616833257053266
language;0.007446386256901297
model;0.010505365504321636
into;0.008247658130119942
python;0.01219679814403928
for;0.007173061398394194
use;0.006144143972373928
with;0.0059123966493984095
PyTorch;0.020060922872532565
is;0.007297573660461626
using;0.007395149453810661
the;0.0076536839884792035
Hugging;0.009631379097259507
Face;0.010676647649312488
Transformer's;0.01852559421300522
library,;0.009465057947683554
which;0.006207394082807077
has;0.006446712470446361
built;0.006062113259420493
in;0.006200664109463289
methods;0.006124024657639061
for;0.005946946507994481
pre-training,;0.013587419173842711
inference,;0.007817410752075208
and;0.005233919380393706
deploying;0.005414940704423079
BERT.;0.010190607973232166
â€˜**;0.009353650943196637
from;0.007865274801732257
transformers;0.008439970140925781
import;0.007280015533345427
AutoTokenizer,;0.03005549060446831
BertModel;0.02209030362093948
import;0.007331998908931959
torch;0.007754076930183739
tokenizer;0.00901897580553939
=;0.006668196415241281
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3882850625881956
model;0.006824764672110791
=;0.006088814930269587
"BertModel.from_pretrained(""bert-base-uncased"")";0.04841178311892528
inputs;0.00786138279241451
=;0.006009827759435737
"tokenizer(""Hello,";0.02088046269453711
my;0.0053310967546379645
dog;0.006317598669669346
is;0.005174531139898253
"cute"",";0.006245371646696767
"return_tensors=""pt"")";0.020237612280479995
outputs;0.007017176460978946
=;0.005525627979465119
model(**inputs);0.01151801619608433
last_hidden_states;0.013850187398237209
=;0.005518118895534744
outputs.last_hidden_state;0.007118623921623568
***;0.0049240126110893165
