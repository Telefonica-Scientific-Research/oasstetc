text;attention
The;0.012271250147907548
easiest;0.013398006481625221
way;0.010783350655259191
to;0.010588004400917376
import;0.018173595362714148
the;0.011517780273399374
BERT;0.023482931113748513
language;0.011349771922899444
model;0.016060455561629924
into;0.020602533000442142
python;0.015988460687850956
for;0.012928196109940492
use;0.010296351022069888
with;0.009830681445809997
PyTorch;0.014955950958038687
is;0.01862790494081864
using;0.011109028500450561
the;0.010023069246005677
Hugging;0.011373556134572737
Face;0.013131588332028252
Transformer's;0.019868137818973196
library,;0.016613830975356927
which;0.010197603821229164
has;0.010266374488164797
built;0.008527987145217226
in;0.010512735963830086
methods;0.01019345747748168
for;0.010444123270957947
pre-training,;0.01665115817093113
inference,;0.011406972361182535
and;0.008915588942249445
deploying;0.009670025996389723
BERT.;0.017444211246526283
â€˜**;0.01520674475519795
from;0.011717836650915317
transformers;0.013119142201491317
import;0.014061591605187544
AutoTokenizer,;0.028123018151598404
BertModel;0.013997780336888695
import;0.016405423661595423
torch;0.010001027456562237
tokenizer;0.012366490047215135
=;0.013458416144811197
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.139251392989506
model;0.012526221573541146
=;0.010886600170591398
"BertModel.from_pretrained(""bert-base-uncased"")";0.05553964115331739
inputs;0.011994651120835111
=;0.009747510418771674
"tokenizer(""Hello,";0.03214301419209972
my;0.009415904463092723
dog;0.009560370661208996
is;0.00898656055068542
"cute"",";0.011597371516641544
"return_tensors=""pt"")";0.023686184848318297
outputs;0.011273201374297506
=;0.009045659246179772
model(**inputs);0.01728836191768239
last_hidden_states;0.013019909004036939
=;0.009146653140320213
outputs.last_hidden_state;0.01098067850196355
***;0.00824796816882817
