text;attention
The;0.014094942034285636
easiest;0.011716117970815522
way;0.011786290853163961
to;0.01116015253935423
import;0.015907181985125866
the;0.012594588950407127
BERT;0.04542339169816766
language;0.012295354497865098
model;0.021988537387540225
into;0.01372326009007211
python;0.016543234704132034
for;0.012561177453404342
use;0.011830091293973909
with;0.011676464626924704
PyTorch;0.026519474074028278
is;0.012259119870747355
using;0.012197368055677253
the;0.01237090077130571
Hugging;0.013802437171831798
Face;0.01597338114007557
Transformer's;0.020912973399352226
library,;0.014479037862343608
which;0.011742511724359644
has;0.01194641137215044
built;0.010981569398208685
in;0.011662710419172717
methods;0.011420840257015166
for;0.011580655716821701
pre-training,;0.014917814714740397
inference,;0.012835952931116208
and;0.010745338494071421
deploying;0.010662133382555242
BERT.;0.016474670935200247
â€˜**;0.014165726352193545
from;0.01294244915897572
transformers;0.01540987383530569
import;0.012372838713430104
AutoTokenizer,;0.03346877065045645
BertModel;0.024625210633323593
import;0.01249938177175131
torch;0.01372762138123902
tokenizer;0.014141515474230397
=;0.01180241780740777
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09063806815441268
model;0.013171813918607842
=;0.011737846712969709
"BertModel.from_pretrained(""bert-base-uncased"")";0.02846177672746443
inputs;0.01562238753093046
=;0.011248096856513885
"tokenizer(""Hello,";0.0219511335359474
my;0.011238508055962505
dog;0.01194460945814839
is;0.010438983259496685
"cute"",";0.011834826208862347
"return_tensors=""pt"")";0.020984156736050877
outputs;0.01313319850652819
=;0.010840192192258276
model(**inputs);0.014985090928219797
last_hidden_states;0.015856116845999135
=;0.010847147059085233
outputs.last_hidden_state;0.01263752954303654
***;0.010486624215185978
