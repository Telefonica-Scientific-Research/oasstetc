text;attention
The;0.010927019842815336
easiest;0.009887781290165631
way;0.009969507486462796
to;0.009572830229589808
import;0.0120459783139538
the;0.010763828443476064
BERT;0.019459810540122347
language;0.009740647456029098
model;0.012217814523505482
into;0.011244890355898469
python;0.011802939715033538
for;0.010568722138933324
use;0.0095365812898275
with;0.010062641317287128
PyTorch;0.01646075559690466
is;0.010294692111468476
using;0.010757772404445434
the;0.010747220328212084
Hugging;0.011327938357600835
Face;0.013041586657309544
Transformer's;0.015782684230896016
library,;0.011782789237629822
which;0.00962955460061558
has;0.010064546099446785
built;0.00932605446616542
in;0.010092086136725865
methods;0.009728824434323914
for;0.010965024397384185
pre-training,;0.0179012477385285
inference,;0.012364859282303287
and;0.009024638384848703
deploying;0.009659746968587821
BERT.;0.01537862111712302
â€˜**;0.017516962139679175
from;0.010243838674604043
transformers;0.010686126914525057
import;0.010542459662007647
AutoTokenizer,;0.02099857067860758
BertModel;0.015721532419126223
import;0.009951778700875376
torch;0.011339924305756347
tokenizer;0.010950190314121604
=;0.01000299447644203
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.20935202588321555
model;0.010207875876309462
=;0.009861710563609688
"BertModel.from_pretrained(""bert-base-uncased"")";0.08038598007043928
inputs;0.010577564278006974
=;0.009640342593718194
"tokenizer(""Hello,";0.02059183118326866
my;0.009378903345573607
dog;0.010298446564068114
is;0.00911451793345027
"cute"",";0.011524671286821489
"return_tensors=""pt"")";0.018984054444940748
outputs;0.010283910165160469
=;0.008830500901235936
model(**inputs);0.016059323929994734
last_hidden_states;0.014710851767274992
=;0.008973471171239226
outputs.last_hidden_state;0.012012162560876113
***;0.009125841701431095
