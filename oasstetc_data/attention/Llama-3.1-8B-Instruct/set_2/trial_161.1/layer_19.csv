text;attention
The;0.014306358904325564
easiest;0.01300412545682382
way;0.012652644324467317
to;0.012049329901678918
import;0.015363723654958712
the;0.012695427196856662
BERT;0.029294564991205915
language;0.012743585624401635
model;0.015737888287987892
into;0.0132747790879296
python;0.015427462076140071
for;0.013010776212389644
use;0.012620547673077935
with;0.0123355713987769
PyTorch;0.02286312792704283
is;0.013571072710538316
using;0.013086643738776754
the;0.012697569235089646
Hugging;0.014897007368214952
Face;0.014587687464509228
Transformer's;0.017739313716788347
library,;0.01415771586817407
which;0.012290286863657848
has;0.012332502265018459
built;0.011662599467103216
in;0.01231055760521762
methods;0.012811080859411097
for;0.012326891248678567
pre-training,;0.015980996398700684
inference,;0.013734174678887129
and;0.011771540337376908
deploying;0.012053760100140509
BERT.;0.01679854915865521
â€˜**;0.015971068641992717
from;0.014039866543197662
transformers;0.015209256143947145
import;0.013895280085993802
AutoTokenizer,;0.026923199664651617
BertModel;0.01967266926338073
import;0.014620139110076124
torch;0.013730247089620895
tokenizer;0.016360773823974578
=;0.013738433598810544
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08701197459653875
model;0.013419776981525017
=;0.012623902371559687
"BertModel.from_pretrained(""bert-base-uncased"")";0.030729827950940793
inputs;0.017438922053562456
=;0.012613313167226883
"tokenizer(""Hello,";0.021275185715423164
my;0.011755183713848885
dog;0.012555041225257331
is;0.011610287256169624
"cute"",";0.013301679337451494
"return_tensors=""pt"")";0.023564745349970814
outputs;0.013355694826260959
=;0.012103961854991173
model(**inputs);0.01635294952545809
last_hidden_states;0.01685674739529485
=;0.012086490890020332
outputs.last_hidden_state;0.013374866214462558
***;0.01161865380538937
