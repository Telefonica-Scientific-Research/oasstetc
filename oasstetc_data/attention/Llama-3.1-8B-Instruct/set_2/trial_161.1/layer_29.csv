text;attention
The;0.012878397043229064
easiest;0.01149219617416883
way;0.011547143817646299
to;0.010542577719392212
import;0.012391325985653385
the;0.011620739698554504
BERT;0.021354420386985973
language;0.011043285989607495
model;0.013261680904260768
into;0.011462839517064363
python;0.013486434936446103
for;0.011192615376125523
use;0.010722749325824624
with;0.010996817796910405
PyTorch;0.021731879688318013
is;0.012122557617463323
using;0.011437749290999928
the;0.012015717393777832
Hugging;0.01272923262109783
Face;0.013807008279525945
Transformer's;0.01453883480900281
library,;0.011887910577559849
which;0.01089208230138889
has;0.010920195768681215
built;0.01038092017141362
in;0.010920488186900853
methods;0.010685708530826548
for;0.011401110386409284
pre-training,;0.015755496488555554
inference,;0.013004882021569698
and;0.01032117561014656
deploying;0.011011661717573002
BERT.;0.019102369719049987
â€˜**;0.020071871426316855
from;0.01144328365557427
transformers;0.012429708408445053
import;0.011923272051397305
AutoTokenizer,;0.019635784452116882
BertModel;0.016234029288364676
import;0.011462553414522392
torch;0.012728998368024564
tokenizer;0.01180317038054095
=;0.010623253912668404
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.15201073707207757
model;0.010613996201563516
=;0.010701076775827956
"BertModel.from_pretrained(""bert-base-uncased"")";0.0777111769042155
inputs;0.013962808256402994
=;0.010755576072613758
"tokenizer(""Hello,";0.018425456075410754
my;0.010334574756504436
dog;0.011920743001009764
is;0.010408012208629178
"cute"",";0.01209301033226997
"return_tensors=""pt"")";0.021736675333688178
outputs;0.011368473622201015
=;0.010069504992077854
model(**inputs);0.013710580159478201
last_hidden_states;0.014412330984945319
=;0.010127252407468819
outputs.last_hidden_state;0.012573313732283162
***;0.01004856990123057
