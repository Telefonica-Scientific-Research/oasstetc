text;attention
The;0.014484960719687672
easiest;0.005031310431172677
way;0.0041578179395145055
to;0.013251431288325129
import;0.009272094091722773
the;0.011242814239423273
BERT;0.010636188971265231
language;0.004123573243572324
model;0.0041746390618514284
into;0.005725180158422013
python;0.009507852804653806
for;0.013708722289900749
use;0.004098617061593094
with;0.0129534245537822
PyTorch;0.011316398899759088
is;0.0072517083166067295
using;0.004726758216701363
the;0.00948493881769025
Hugging;0.010318663051245664
Face;0.0036727171972572217
Transformer's;0.011198776067720101
library,;0.030269319162593195
which;0.003914600919264027
has;0.005439226761538051
built;0.004067242433040803
in;0.00944882106631085
methods;0.0038710447575560553
for;0.01051586516167488
pre-training,;0.028910346350958047
inference,;0.02427679901495396
and;0.010939572851746836
deploying;0.004282827935116036
BERT.;0.029035480089365785
â€˜**;0.005363750985985541
from;0.006544181638482249
transformers;0.004535408535070008
import;0.005501547330645696
AutoTokenizer,;0.025109663586762816
BertModel;0.004393595251034464
import;0.00540114363721146
torch;0.005117480021543061
tokenizer;0.004827618274590297
=;0.008014492454067003
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.29014272950668285
model;0.0034944789214685764
=;0.007459392649229713
"BertModel.from_pretrained(""bert-base-uncased"")";0.17161408281755514
inputs;0.003560448838356746
=;0.006434065921841035
"tokenizer(""Hello,";0.034054503475637904
my;0.004481920086439685
dog;0.0033524229262710536
is;0.0046382602771305225
"cute"",";0.0058733165872576236
"return_tensors=""pt"")";0.013165298351158565
outputs;0.0031991991874528642
=;0.005314661705617582
model(**inputs);0.007447185865549352
last_hidden_states;0.004369304980924287
=;0.004252091789625192
outputs.last_hidden_state;0.0040979813289641325
***;0.0029300391414586206
