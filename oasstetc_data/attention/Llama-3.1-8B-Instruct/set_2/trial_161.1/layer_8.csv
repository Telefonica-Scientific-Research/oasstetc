text;attention
The;0.01211846657397202
easiest;0.006878823972890413
way;0.006480966252287624
to;0.00588619211230716
import;0.009509540268228669
the;0.006692067729595285
BERT;0.04269706931766736
language;0.00796350472674877
model;0.015871758067861747
into;0.01511579709864195
python;0.021028793878025445
for;0.00850099225123359
use;0.007536296458931113
with;0.006730102353580311
PyTorch;0.04013910167782614
is;0.011721495637358135
using;0.007521564371481439
the;0.006746631152943441
Hugging;0.008470945299138773
Face;0.011358408011888732
Transformer's;0.01880512420354369
library,;0.012080387832020305
which;0.006322914178644286
has;0.005713628618797828
built;0.005370751042823568
in;0.006080755641375969
methods;0.00676085964112873
for;0.005821802151807018
pre-training,;0.014083818714409928
inference,;0.009810423641059201
and;0.0058212258893686145
deploying;0.0062066404967445745
BERT.;0.01881005164515549
â€˜**;0.017483025045192835
from;0.008283826992311746
transformers;0.009482615739632813
import;0.0077146637210590025
AutoTokenizer,;0.047452326029204114
BertModel;0.030650827945503126
import;0.00991064901714312
torch;0.0074620087184152275
tokenizer;0.008257464516645907
=;0.007938455478012293
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2700103150939631
model;0.0077459461983426766
=;0.0069019604954581445
"BertModel.from_pretrained(""bert-base-uncased"")";0.04834664338997447
inputs;0.009595597676073592
=;0.006576638064988425
"tokenizer(""Hello,";0.021322954185326076
my;0.00564309152471758
dog;0.0063684576976197884
is;0.005428216349333783
"cute"",";0.007419124791104959
"return_tensors=""pt"")";0.01788114951543242
outputs;0.00707710251881793
=;0.005825940198869929
model(**inputs);0.011919962988170923
last_hidden_states;0.008954207441067393
=;0.005663673414257096
outputs.last_hidden_state;0.006997886055396245
***;0.005028368288478015
