text;attention
The;0.017070266302839993
easiest;0.01531280657491171
way;0.014400668573696889
to;0.01442950857312075
import;0.01688775087508942
the;0.0146915735431532
BERT;0.024744995410521907
language;0.014429034577179103
model;0.01606534525136247
into;0.015250120859922538
python;0.01563315908336234
for;0.01470507537870809
use;0.013851837625423429
with;0.014508527023444118
PyTorch;0.01805492123821098
is;0.014693071918340746
using;0.014771412982925078
the;0.014747707778281225
Hugging;0.01459391963106949
Face;0.016375588471342406
Transformer's;0.016512005887143844
library,;0.016040705323049978
which;0.01396874037560925
has;0.014025480969029471
built;0.013407249485238176
in;0.01405087572655979
methods;0.014010445713975413
for;0.014470334194480664
pre-training,;0.016113556124813633
inference,;0.015956562744427444
and;0.013337726472163526
deploying;0.014326299477620865
BERT.;0.024520863054605956
â€˜**;0.017716808777373673
from;0.01478988326683722
transformers;0.014174420530089911
import;0.014533079079770467
AutoTokenizer,;0.02172627498828959
BertModel;0.01790672834102648
import;0.014336121679933263
torch;0.01587993471333823
tokenizer;0.015625588106346747
=;0.0140897696589533
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04229878754344044
model;0.014127286111032162
=;0.013939692418274117
"BertModel.from_pretrained(""bert-base-uncased"")";0.02777222540436543
inputs;0.015719605613557685
=;0.014139545913672098
"tokenizer(""Hello,";0.01919849720108965
my;0.013524030242768017
dog;0.014473113433538291
is;0.013313609857665729
"cute"",";0.014963173016599945
"return_tensors=""pt"")";0.0192260051626563
outputs;0.014231704780131223
=;0.013513613161145981
model(**inputs);0.016104468817525152
last_hidden_states;0.015781545174277167
=;0.013390622476043983
outputs.last_hidden_state;0.014322913308342117
***;0.013222814000291726
