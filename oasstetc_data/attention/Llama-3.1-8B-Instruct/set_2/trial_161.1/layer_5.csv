text;attention
The;0.014988611910049764
easiest;0.012970733633197111
way;0.01236362512384915
to;0.012038654719641512
import;0.017317743229573433
the;0.012741873294853762
BERT;0.02944381138614662
language;0.01658738364698805
model;0.015171862695346004
into;0.020260883044584937
python;0.0198513356846435
for;0.015327548680610878
use;0.011375981675065101
with;0.011376059991420226
PyTorch;0.017332246542144072
is;0.01679543539592981
using;0.013042782534016086
the;0.011346858311529356
Hugging;0.013483675770655597
Face;0.012979310323662405
Transformer's;0.021688853534498145
library,;0.01883149697719339
which;0.012352196947919503
has;0.011233005510387684
built;0.010289253255840231
in;0.011297181702507582
methods;0.011964050385339167
for;0.012155419687796936
pre-training,;0.021460895932944004
inference,;0.013262037289300418
and;0.010471909417487588
deploying;0.011220096181744745
BERT.;0.020248524176794867
â€˜**;0.018199341178844735
from;0.014716352614587527
transformers;0.014470206042599097
import;0.013296644974107145
AutoTokenizer,;0.026186466286448
BertModel;0.01665696006690316
import;0.014615436575351528
torch;0.0109775465887098
tokenizer;0.012949998632029672
=;0.01481132778018342
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07953559993662028
model;0.012567566074815108
=;0.013544419834421484
"BertModel.from_pretrained(""bert-base-uncased"")";0.04266376348664205
inputs;0.013781820240587274
=;0.012975310127247246
"tokenizer(""Hello,";0.02778603978298528
my;0.010839979607085737
dog;0.010822980306824234
is;0.010248213105655213
"cute"",";0.011709243034166575
"return_tensors=""pt"")";0.018672550670139846
outputs;0.01143568923423769
=;0.011209687096212171
model(**inputs);0.015843744592825896
last_hidden_states;0.014028631240632883
=;0.01100136205196477
outputs.last_hidden_state;0.011267130618745057
***;0.009914649624765655
