text;attention
The;0.009656077823350224
easiest;0.007659328724755592
way;0.007523114756502897
to;0.006681846449697982
import;0.009920083979357768
the;0.007474764215524856
BERT;0.042607067839406945
language;0.007897859004593808
model;0.011436832233119778
into;0.008230225047748308
python;0.013120646275978547
for;0.008053552310571466
use;0.006974899810094452
with;0.006692198803494186
PyTorch;0.028581384291143687
is;0.008941467513897924
using;0.00853276236858416
the;0.007890610208209246
Hugging;0.01175548540930298
Face;0.012961265686487105
Transformer's;0.018775434844881218
library,;0.010585534493156098
which;0.007209088651955487
has;0.007003123943565292
built;0.006617379692770326
in;0.007126171371508803
methods;0.007130419954251665
for;0.006920353826052852
pre-training,;0.012018980652224814
inference,;0.008496552249885941
and;0.00620106455387925
deploying;0.00635856279160858
BERT.;0.012335984587106788
â€˜**;0.015200678439711252
from;0.009405955775803016
transformers;0.009523893352749765
import;0.007920399269321742
AutoTokenizer,;0.02917795067809988
BertModel;0.01957450405819695
import;0.009891804270878005
torch;0.008077846912778429
tokenizer;0.009361549584099755
=;0.007932947269422732
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3138875101490451
model;0.008400379516542497
=;0.007577945049393934
"BertModel.from_pretrained(""bert-base-uncased"")";0.04914716203323512
inputs;0.009403079018246507
=;0.006976657141440243
"tokenizer(""Hello,";0.01663954328766235
my;0.006258190368098455
dog;0.006901401791470113
is;0.005925506387531666
"cute"",";0.007427778042983309
"return_tensors=""pt"")";0.022101883213473054
outputs;0.008621800394895122
=;0.006721747415061802
model(**inputs);0.012917944791780721
last_hidden_states;0.014419402874077326
=;0.006409542101973022
outputs.last_hidden_state;0.008762784046466457
***;0.006062088400892682
