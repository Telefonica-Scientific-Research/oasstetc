text;attention
The;0.01397530536521752
easiest;0.013462017382869727
way;0.0126310709897997
to;0.012179536489907572
import;0.01687868098759631
the;0.013318015693854844
BERT;0.0267739423801212
language;0.013237950184069031
model;0.014819170559117047
into;0.01511517381120969
python;0.015510468461529061
for;0.013787487954848843
use;0.012618021831221429
with;0.013703746472615718
PyTorch;0.020888968952737688
is;0.013190550029632035
using;0.01368914627626152
the;0.013245559931590694
Hugging;0.014036429883499865
Face;0.015423819869540206
Transformer's;0.020850446270746472
library,;0.014635736985706095
which;0.013254744173603821
has;0.01395802977858271
built;0.01171857531769563
in;0.013110384956511827
methods;0.012810439770605942
for;0.013960156855089051
pre-training,;0.017362362636754777
inference,;0.014510881831455756
and;0.01183909056417949
deploying;0.012486298691174859
BERT.;0.016365378451398824
â€˜**;0.021728082938857807
from;0.013098235488159634
transformers;0.013004078693099424
import;0.012820303438413498
AutoTokenizer,;0.021898079912963317
BertModel;0.018145283038871057
import;0.013580428762029454
torch;0.013772256330132581
tokenizer;0.014095108061572175
=;0.013160966616414017
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07889172579836934
model;0.012653851724487106
=;0.012518169571089076
"BertModel.from_pretrained(""bert-base-uncased"")";0.03991132703380137
inputs;0.013993112348542181
=;0.012576472587316492
"tokenizer(""Hello,";0.021923148904852745
my;0.01154879756979201
dog;0.012365567741179997
is;0.011598193495972886
"cute"",";0.013073142808802962
"return_tensors=""pt"")";0.020907123249239837
outputs;0.013312084037431633
=;0.011896993539328174
model(**inputs);0.01699387435511147
last_hidden_states;0.016859822852603866
=;0.011825085498397093
outputs.last_hidden_state;0.014790503607590457
***;0.0117105902048336
