text;attention
The;0.005106269621656308
easiest;0.004377429793670922
way;0.003988145240982404
to;0.0038231484384294435
import;0.0053089223594553255
the;0.004592134687191888
BERT;0.01342175500290113
language;0.0041406621293497425
model;0.00491118336481954
into;0.0046337860653451884
python;0.00572654304367093
for;0.00450237137003755
use;0.0038452323395055717
with;0.004180639090960174
PyTorch;0.009041817033064753
is;0.004793182613718463
using;0.004597549036967295
the;0.005018750078154363
Hugging;0.005317057521565396
Face;0.005864500084902205
Transformer's;0.007368253098457907
library,;0.005066081240423616
which;0.003952884620677732
has;0.004025027093217224
built;0.003726930309865894
in;0.003933254294366284
methods;0.004069465572712727
for;0.004066866412649924
pre-training,;0.007165207409510046
inference,;0.0051363352412004855
and;0.003452190087823013
deploying;0.0037378690154863375
BERT.;0.009296140561631869
â€˜**;0.017471895103712262
from;0.0047352685030079625
transformers;0.004407887393465561
import;0.0052079706675079735
AutoTokenizer,;0.01553414417442861
BertModel;0.008923141614953486
import;0.00533709736424432
torch;0.005472495816287125
tokenizer;0.005935984170588274
=;0.004093626644926885
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5347599802418778
model;0.004423019659190982
=;0.003887264072481046
"BertModel.from_pretrained(""bert-base-uncased"")";0.13113478323405925
inputs;0.004954395968865395
=;0.003950190157783456
"tokenizer(""Hello,";0.009011751213808403
my;0.003709301930182301
dog;0.00379476588142902
is;0.003480262866999012
"cute"",";0.004771191061616985
"return_tensors=""pt"")";0.009385753658081048
outputs;0.0040663388542862885
=;0.0034333978352107095
model(**inputs);0.007341518306941748
last_hidden_states;0.006757627008475308
=;0.0034271216741120973
outputs.last_hidden_state;0.004883463978969441
***;0.003520777068135725
