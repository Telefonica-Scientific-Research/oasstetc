text;attention
The;0.0157163113769673
easiest;0.013866099229145685
way;0.012924233346983371
to;0.012548222808176152
import;0.01665563080938624
the;0.01359299892364284
BERT;0.03133717815720493
language;0.013465797173746378
model;0.018233580434584586
into;0.013468302797826641
python;0.015285390625621084
for;0.013153257647954388
use;0.012776571944012164
with;0.012856058059584364
PyTorch;0.019567663588726872
is;0.014067821714166703
using;0.013577083651647214
the;0.013650769716891276
Hugging;0.01474392810450273
Face;0.014363079115038169
Transformer's;0.01832180191599992
library,;0.015343041535883358
which;0.01276434018708322
has;0.012987548463221844
built;0.01192807728243883
in;0.012822184685750726
methods;0.013055706650778308
for;0.01274694386817096
pre-training,;0.015376716527264129
inference,;0.014746834604364281
and;0.011838231856208683
deploying;0.01207221357615667
BERT.;0.018468284122049004
â€˜**;0.022758658325225875
from;0.013754286214934614
transformers;0.013291278443120618
import;0.013194657835139654
AutoTokenizer,;0.022961376768956893
BertModel;0.020134776966890165
import;0.01362453321128041
torch;0.014006301586363477
tokenizer;0.015385675520576592
=;0.013183955861605876
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06626922352485934
model;0.013763298776042533
=;0.012638248789921468
"BertModel.from_pretrained(""bert-base-uncased"")";0.034705777667571436
inputs;0.015759404402763893
=;0.01283909422665993
"tokenizer(""Hello,";0.02046344978550325
my;0.011788808163084306
dog;0.012197976822013842
is;0.011513048650784482
"cute"",";0.013551283432118668
"return_tensors=""pt"")";0.022339107692076036
outputs;0.01459111225590334
=;0.012318941719196595
model(**inputs);0.01682658221450741
last_hidden_states;0.019095917930374424
=;0.012143548688554457
outputs.last_hidden_state;0.014577877434965303
***;0.011999892587826104
