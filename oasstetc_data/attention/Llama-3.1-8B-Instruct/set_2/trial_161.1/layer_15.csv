text;attention
The;0.009724423092237502
easiest;0.007316797624494612
way;0.006942659600685504
to;0.006334537757193812
import;0.009214751195800428
the;0.0072402162094861505
BERT;0.030760116510762155
language;0.007836393444390015
model;0.010551521005833786
into;0.008542580322923548
python;0.01312606629067916
for;0.007757132780571379
use;0.006599035076474897
with;0.006399636182395482
PyTorch;0.019882707961877202
is;0.007723266379606643
using;0.00795847961090576
the;0.008566933795870372
Hugging;0.01022538083015792
Face;0.010992053858876477
Transformer's;0.017799700457367056
library,;0.009759660198301764
which;0.006551567095459544
has;0.006899628491717837
built;0.00666270863425171
in;0.006668616153571228
methods;0.00658997477636937
for;0.006387815738429736
pre-training,;0.014077528511137645
inference,;0.00849452742779713
and;0.005653939592351553
deploying;0.005884040752689004
BERT.;0.011082234155805962
â€˜**;0.009907085651365223
from;0.00790296290365443
transformers;0.008996512600945382
import;0.007579967980841564
AutoTokenizer,;0.030597825083679125
BertModel;0.020777823899824344
import;0.007985651590568637
torch;0.007789628818159584
tokenizer;0.009440369512336101
=;0.007231201772769296
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.35892296093674175
model;0.007280126631662491
=;0.006582394893279904
"BertModel.from_pretrained(""bert-base-uncased"")";0.0485465350336145
inputs;0.008413545798361543
=;0.006539315963037087
"tokenizer(""Hello,";0.023129450415856248
my;0.0057991432671822656
dog;0.006827449400918331
is;0.005586029323138814
"cute"",";0.006801833622139906
"return_tensors=""pt"")";0.021571109769229473
outputs;0.007598690669524373
=;0.0059795462474018955
model(**inputs);0.01249006511307104
last_hidden_states;0.014680600084317033
=;0.0059612346193166936
outputs.last_hidden_state;0.007519429798852805
***;0.0053548770817379975
