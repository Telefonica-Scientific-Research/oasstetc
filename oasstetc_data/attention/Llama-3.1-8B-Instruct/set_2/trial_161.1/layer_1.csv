text;attention
The;0.015058256728101726
easiest;0.01536462919394655
way;0.013288601557523866
to;0.012630030116431084
import;0.015743402013030407
the;0.012580085434047771
BERT;0.01483911314248754
language;0.013248670443443423
model;0.012241001506363407
into;0.012396731004374984
python;0.012471560367087655
for;0.01284609199891082
use;0.012172553264689662
with;0.011991545189298047
PyTorch;0.019263931324698287
is;0.012478250568802746
using;0.012297557874315532
the;0.012380879860708753
Hugging;0.01473423636321456
Face;0.01296427318594149
Transformer's;0.014566668755753728
library,;0.016469900272920328
which;0.011697932282924053
has;0.011648025225008414
built;0.011584817546483214
in;0.01156660589278466
methods;0.012128151723645742
for;0.011548293289098803
pre-training,;0.019404127701053223
inference,;0.013798840554263114
and;0.011169393137388253
deploying;0.01160202346566261
BERT.;0.01838188987523379
â€˜**;0.01623016024310281
from;0.012077095032827532
transformers;0.013607155077417395
import;0.013737021785373955
AutoTokenizer,;0.018174951719941844
BertModel;0.013381362246923918
import;0.01146611219828683
torch;0.01211289055546469
tokenizer;0.012533160233693216
=;0.012112493109496707
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.10069608409612014
model;0.010848918740150353
=;0.01162904201037929
"BertModel.from_pretrained(""bert-base-uncased"")";0.07946623735049803
inputs;0.011247549808171258
=;0.011328593838763147
"tokenizer(""Hello,";0.02425273012936347
my;0.011306404695886318
dog;0.011160034317527314
is;0.010874387478356038
"cute"",";0.013152638365453098
"return_tensors=""pt"")";0.031487136743459244
outputs;0.011545663644360006
=;0.011108797309292318
model(**inputs);0.01801624711390375
last_hidden_states;0.01463325939655501
=;0.010699335644302251
outputs.last_hidden_state;0.014573433815953307
***;0.009983032439338548
