text;attention
The;0.008447459690591306
easiest;0.004456120897782003
way;0.004367427121944745
to;0.004166557714856636
import;0.006336045399129796
the;0.004744273654461599
BERT;0.05326907926889097
language;0.005521411511587015
model;0.009013866194133175
into;0.005897998620911703
python;0.011159501444651632
for;0.005475841635148293
use;0.0048117011820296126
with;0.0045295556158602025
PyTorch;0.03218897718597933
is;0.005633747770968735
using;0.005464685894248777
the;0.005935204510219131
Hugging;0.007648218597880402
Face;0.007942283504305642
Transformer's;0.015240929415529373
library,;0.006415880057421899
which;0.004363967292433774
has;0.0044298895197578255
built;0.004407680433899157
in;0.0042649627926250235
methods;0.004156905451604037
for;0.004146452217729948
pre-training,;0.010961206628500798
inference,;0.006376339432643152
and;0.0039962580946768566
deploying;0.004176015086506485
BERT.;0.00908548663839905
â€˜**;0.008677156458528918
from;0.006537072931943347
transformers;0.005535522308127389
import;0.006009144806955907
AutoTokenizer,;0.03166123840135387
BertModel;0.024048962397627473
import;0.007090622034058216
torch;0.005969841925145967
tokenizer;0.006781792362762449
=;0.0057697684888210095
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4534661518917619
model;0.005694437310769389
=;0.004810310117609587
"BertModel.from_pretrained(""bert-base-uncased"")";0.04417504360638526
inputs;0.006763660818581569
=;0.004552722465922853
"tokenizer(""Hello,";0.016430726651565922
my;0.003944576263874408
dog;0.004315758345469101
is;0.0037350882970509413
"cute"",";0.004666517880848028
"return_tensors=""pt"")";0.014405103782373243
outputs;0.00609982642452332
=;0.004341268229890197
model(**inputs);0.00972883315735778
last_hidden_states;0.012371134828452222
=;0.004387046129227676
outputs.last_hidden_state;0.005290564432241626
***;0.003708176773462234
