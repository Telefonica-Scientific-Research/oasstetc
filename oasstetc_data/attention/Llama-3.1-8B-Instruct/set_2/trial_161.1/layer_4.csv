text;attention
The;0.013922595102657193
easiest;0.013453155974867785
way;0.012562755326520313
to;0.012087075615230657
import;0.017817355348248156
the;0.01303408474126726
BERT;0.019420828399855018
language;0.014611176505209381
model;0.014751539118252529
into;0.016672599653599258
python;0.01772987004991991
for;0.013648317203417972
use;0.012113979222932168
with;0.01227297617873968
PyTorch;0.018011875390663686
is;0.015442040017920312
using;0.01309417246802153
the;0.011666676300349161
Hugging;0.0143026723100589
Face;0.014743565504289598
Transformer's;0.019668585155132944
library,;0.016698594911307974
which;0.012163422723939952
has;0.011612744305841296
built;0.011090209549394422
in;0.012559365785546577
methods;0.012673012089384223
for;0.011891536081523654
pre-training,;0.01811456171449032
inference,;0.013720442136217485
and;0.01106617281159963
deploying;0.012035325062279152
BERT.;0.017493474036350983
â€˜**;0.01795250641454871
from;0.014082191226679886
transformers;0.015527026897180997
import;0.01334706364178098
AutoTokenizer,;0.021829789241658657
BertModel;0.014955647342531915
import;0.013538769120292888
torch;0.011673233913893473
tokenizer;0.013543510919933853
=;0.013422148946711565
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08926941350198345
model;0.014047093384830768
=;0.012795919120256478
"BertModel.from_pretrained(""bert-base-uncased"")";0.048906714751009536
inputs;0.01526739556031708
=;0.01231725927400287
"tokenizer(""Hello,";0.02470429691598452
my;0.011298997479560475
dog;0.011523864323654468
is;0.010889550535685676
"cute"",";0.012894686155055604
"return_tensors=""pt"")";0.022799965416756798
outputs;0.012775026687445738
=;0.011272590792044566
model(**inputs);0.016194742724387572
last_hidden_states;0.01648085734762452
=;0.01105804941566057
outputs.last_hidden_state;0.013055118370803103
***;0.010429813782694147
