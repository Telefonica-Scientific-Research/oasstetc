text;attention
The;0.010745365993673277
easiest;0.007536405589645816
way;0.007524615111090024
to;0.006821458307124837
import;0.010097166023236832
the;0.008625923927888813
BERT;0.04703947218559965
language;0.007843159065930872
model;0.01716919153102826
into;0.01526026598228724
python;0.01754996039102404
for;0.012965294466854313
use;0.008036616038618943
with;0.007262798146250909
PyTorch;0.029619025918366338
is;0.01321499829352414
using;0.009394267611834494
the;0.008764706835494466
Hugging;0.010821399051728306
Face;0.01153761693304684
Transformer's;0.01804661245649179
library,;0.012629558189171141
which;0.007225894290749347
has;0.006511916513183392
built;0.00794893500779147
in;0.007575611389661309
methods;0.007459277858337379
for;0.007242950819605157
pre-training,;0.015733650903054053
inference,;0.01230592752631176
and;0.0068204117215693895
deploying;0.006746970235443781
BERT.;0.016527372149482498
â€˜**;0.012360042244089653
from;0.009835383354874378
transformers;0.009022198347295346
import;0.009185803929870008
AutoTokenizer,;0.05419830263362732
BertModel;0.02677795266016801
import;0.010229631931522303
torch;0.008381146380816372
tokenizer;0.00926843914254826
=;0.009709608276032348
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2320150850628805
model;0.009208099812678248
=;0.008409305301728015
"BertModel.from_pretrained(""bert-base-uncased"")";0.04075111164863534
inputs;0.009704793793215094
=;0.008277948434503826
"tokenizer(""Hello,";0.026426933151941927
my;0.0067962256250087795
dog;0.007403591358256592
is;0.006380541833360041
"cute"",";0.007509401793457453
"return_tensors=""pt"")";0.016002792108214838
outputs;0.008012444007967095
=;0.00697400274631611
model(**inputs);0.013912146869232478
last_hidden_states;0.01146099688236042
=;0.007147683463254102
outputs.last_hidden_state;0.007872032645135698
***;0.00616155809590862
