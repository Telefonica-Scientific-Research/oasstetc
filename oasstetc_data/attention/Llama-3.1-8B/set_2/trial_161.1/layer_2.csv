text;attention
The;0.016842420791019674
easiest;0.01614398361493016
way;0.015567535320014536
to;0.014631307067375626
import;0.018813175698598767
the;0.01488108589446359
BERT;0.01907373488759194
language;0.016134485090538563
model;0.015999441384095103
into;0.015185395130171184
python;0.015508669505052156
for;0.014649642000905346
use;0.014819976813234433
with;0.014441104930443052
PyTorch;0.018003534823353762
is;0.01552010956655271
using;0.015409541691085014
the;0.014330966720441153
Hugging;0.015621916605237816
Face;0.01528259960943542
Transformer's;0.015750869985150108
library,;0.015998865395287047
which;0.014610572915417504
has;0.014312078681657315
built;0.014784794045426857
in;0.014757316146220249
methods;0.014694958188749389
for;0.014313783103104402
pre-training,;0.018237947261906622
inference,;0.014813078351175223
and;0.013692332071146444
deploying;0.014198492197132907
BERT.;0.01678946059372801
â€˜**;0.017558275660527242
from;0.015334281420137891
transformers;0.016043843136825928
import;0.015484545333515756
AutoTokenizer,;0.018493091501924633
BertModel;0.015247268927047832
import;0.015050776546619314
torch;0.014334051326776153
tokenizer;0.015272383027169451
=;0.016061648565146962
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.03785486457264443
model;0.015859378838477396
=;0.015752121136518766
"BertModel.from_pretrained(""bert-base-uncased"")";0.024730401088043273
inputs;0.015261959725058885
=;0.016213000375194
"tokenizer(""Hello,";0.022979565874358173
my;0.014275800000820989
dog;0.013879332066234573
is;0.013637300371363275
"cute"",";0.014409084495866297
"return_tensors=""pt"")";0.019646930329732325
outputs;0.014342976139418016
=;0.015093113207910952
model(**inputs);0.016045610154716998
last_hidden_states;0.015410048011645652
=;0.013924579764268788
outputs.last_hidden_state;0.014510847938641403
***;0.013477744382752456
