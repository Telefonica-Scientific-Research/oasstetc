text;attention
The;0.013351620763226548
easiest;0.012751244988022006
way;0.012505476150724436
to;0.011823383739568642
import;0.016151909758675812
the;0.012867274460353965
BERT;0.025889625579338268
language;0.013485533761117071
model;0.014779762913085333
into;0.015309199572516677
python;0.01598649309900397
for;0.013508451880493613
use;0.012020197400446854
with;0.013804346990560809
PyTorch;0.021603291560967224
is;0.012763402194208014
using;0.014243270697919832
the;0.013172950714250191
Hugging;0.013690323133429303
Face;0.015329069355342013
Transformer's;0.02124635893165068
library,;0.014246371243610762
which;0.013000998817382427
has;0.013260488271210158
built;0.01178407999012432
in;0.013142167283301967
methods;0.012620032919515552
for;0.014033594099552368
pre-training,;0.018414894304085442
inference,;0.014005880268642145
and;0.011461596872264172
deploying;0.012768886345563888
BERT.;0.017201366154384192
â€˜**;0.026305984310569384
from;0.01293900683854242
transformers;0.01334957248324727
import;0.012987995000835955
AutoTokenizer,;0.022095082146123136
BertModel;0.018581972878408962
import;0.012899693339143805
torch;0.014054484513093227
tokenizer;0.014357084903560639
=;0.012766277612930739
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08465964337278425
model;0.012370185290195498
=;0.012154880777794122
"BertModel.from_pretrained(""bert-base-uncased"")";0.03730683645710011
inputs;0.013737774989523093
=;0.012210958275952012
"tokenizer(""Hello,";0.02294395977271574
my;0.011218585142102474
dog;0.011678308069024625
is;0.011227917564319661
"cute"",";0.012722315166659736
"return_tensors=""pt"")";0.02074546636028784
outputs;0.013048975235263834
=;0.011478142919626816
model(**inputs);0.016346129191296303
last_hidden_states;0.016220584789036752
=;0.011517441204344598
outputs.last_hidden_state;0.014385403373899893
***;0.011465793807078243
