text;attention
The;0.010033333312758963
easiest;0.008137881232381666
way;0.008591820345816669
to;0.007479418438088247
import;0.012570273950471254
the;0.008915467686496316
BERT;0.03710585726852797
language;0.00944929849488565
model;0.015524345461542384
into;0.00980487965743164
python;0.01324598775156335
for;0.00892226453271476
use;0.008370126388375745
with;0.008636325374638727
PyTorch;0.022750169002230047
is;0.010220708032717858
using;0.011632208538382817
the;0.009792151739844185
Hugging;0.01057718481825572
Face;0.012409565717712614
Transformer's;0.019474991798943495
library,;0.011220859621989538
which;0.008248404740776448
has;0.007831507335140871
built;0.007285919838069866
in;0.00900468201969136
methods;0.008913160631984806
for;0.00915820758354169
pre-training,;0.014802836804426095
inference,;0.010560216030217801
and;0.00729878141836408
deploying;0.008334933950508215
BERT.;0.014475247769637151
â€˜**;0.013569508024916998
from;0.012129633188600466
transformers;0.01207306272479374
import;0.011773983554692561
AutoTokenizer,;0.032875315683951904
BertModel;0.0213926621758346
import;0.012423945046998393
torch;0.010623838050205791
tokenizer;0.011278608604941178
=;0.008952292990944025
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.25326815754685433
model;0.008882051793524479
=;0.008160672742501344
"BertModel.from_pretrained(""bert-base-uncased"")";0.04421583786539094
inputs;0.009774881671689687
=;0.007544890507723966
"tokenizer(""Hello,";0.01932512103035145
my;0.007183574765757085
dog;0.007654721410551526
is;0.006979470809173199
"cute"",";0.008090763872459062
"return_tensors=""pt"")";0.025672806814247457
outputs;0.009090368326167176
=;0.007256271541485129
model(**inputs);0.01307319175802917
last_hidden_states;0.01149967799424323
=;0.007214197482364251
outputs.last_hidden_state;0.01011052335567545
***;0.007130951377803543
