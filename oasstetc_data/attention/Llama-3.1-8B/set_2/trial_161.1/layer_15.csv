text;attention
The;0.008101758307822652
easiest;0.006504434020617121
way;0.006263949123534255
to;0.005610285252010381
import;0.0090860920012967
the;0.006541666187451937
BERT;0.028610318464605433
language;0.007805117687981463
model;0.011305261269509686
into;0.008667917911137043
python;0.012259602321218827
for;0.007262376569500082
use;0.005888951558654538
with;0.006032633307317144
PyTorch;0.017502540410941857
is;0.007064273106071337
using;0.00811631881184077
the;0.007254882673462369
Hugging;0.008824848708112443
Face;0.009202383232634169
Transformer's;0.0183250695774685
library,;0.008449504594243092
which;0.006002293043603081
has;0.0057920399455087665
built;0.005224730673988831
in;0.0059289362006516196
methods;0.00593241884242384
for;0.005844488780539105
pre-training,;0.013329477464604493
inference,;0.007833470213943997
and;0.005003313955051988
deploying;0.005570439482172571
BERT.;0.011035175606238163
â€˜**;0.00948769185368855
from;0.00724606246743549
transformers;0.009947313629443896
import;0.007370383254632151
AutoTokenizer,;0.03131116697773489
BertModel;0.0212015419685093
import;0.0067508506462027355
torch;0.007734988974811438
tokenizer;0.008743935686861727
=;0.006474840592755435
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3868199320278585
model;0.0068708936532667275
=;0.005946854228527709
"BertModel.from_pretrained(""bert-base-uncased"")";0.06145665899219997
inputs;0.0072410114425573045
=;0.005567545067551242
"tokenizer(""Hello,";0.01935133944530763
my;0.005170070728209671
dog;0.006011944337922636
is;0.005028427276868903
"cute"",";0.006087596012603548
"return_tensors=""pt"")";0.022914687539873566
outputs;0.0068760811702824704
=;0.00527476921473309
model(**inputs);0.0108096982848912
last_hidden_states;0.012607926111937376
=;0.0052030222455380385
outputs.last_hidden_state;0.007510245443978187
***;0.004805551417658412
