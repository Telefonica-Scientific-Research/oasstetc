text;attention
The;0.00399219503647402
easiest;0.0035269252791935795
way;0.0035751439705542253
to;0.0032538421099035063
import;0.004181300337161963
the;0.0036593253346523355
BERT;0.0061055969223507405
language;0.0035330464430345407
model;0.004121543389385665
into;0.003696079659405289
python;0.004045491932786402
for;0.003750800353706365
use;0.003515041573502875
with;0.003604456029181814
PyTorch;0.006629255694777248
is;0.0037617210570799493
using;0.004504655422185347
the;0.003950798116900697
Hugging;0.004069950821715748
Face;0.004041222326419531
Transformer's;0.006194306572031463
library,;0.004782576737953833
which;0.003447501626079204
has;0.0035512310621529366
built;0.0034398338866050045
in;0.003558094837872214
methods;0.0036543705522418908
for;0.0038105115021071314
pre-training,;0.006803410172350975
inference,;0.0045712987001400324
and;0.0030704975494230567
deploying;0.0036955000863023646
BERT.;0.008181120205978897
â€˜**;0.0083442201451982
from;0.003912430115825046
transformers;0.004008108270805997
import;0.004538088792043526
AutoTokenizer,;0.008734361315959992
BertModel;0.006589648363506227
import;0.003837028787563528
torch;0.004400272144738841
tokenizer;0.00430853344690423
=;0.003596463852540001
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4360845687899486
model;0.0037592856263367804
=;0.0034645652406795078
"BertModel.from_pretrained(""bert-base-uncased"")";0.28797014777558183
inputs;0.0040952815182311525
=;0.0034201222984257687
"tokenizer(""Hello,";0.010298674733549106
my;0.003488846439710093
dog;0.0033774734269677777
is;0.003138963873916445
"cute"",";0.004817976767954974
"return_tensors=""pt"")";0.011112008345490369
outputs;0.0035719969815909405
=;0.0031542146277183376
model(**inputs);0.008041785465973934
last_hidden_states;0.005765135942299687
=;0.003113200585521364
outputs.last_hidden_state;0.005693774867198751
***;0.003084176156208314
