text;attention
The;0.010804560296979725
easiest;0.007001426148571505
way;0.006657491442914479
to;0.006101116627188029
import;0.01034605718084734
the;0.007037569930554012
BERT;0.03447611691624783
language;0.009021828936911877
model;0.017636806094764104
into;0.013439359721359612
python;0.021050646578022345
for;0.008425584089650327
use;0.0074774045133680555
with;0.0067017049782304395
PyTorch;0.03141059425877971
is;0.011900232284227225
using;0.007536058384535687
the;0.007005268610336956
Hugging;0.00867997629764204
Face;0.011304819524046664
Transformer's;0.02025260295453719
library,;0.012773220527572265
which;0.006923608951415708
has;0.006047872321659305
built;0.005262763767199062
in;0.0062402529724733275
methods;0.006716042271342521
for;0.006161153134167189
pre-training,;0.013435771704453543
inference,;0.008878527496941348
and;0.005833318263019353
deploying;0.006175765665449155
BERT.;0.016631511090146232
â€˜**;0.018508580161316044
from;0.008887419885922258
transformers;0.013262940297394419
import;0.008521742772597221
AutoTokenizer,;0.045381114116021144
BertModel;0.02238237754653864
import;0.010362400812314945
torch;0.007388595499547911
tokenizer;0.008805010482779752
=;0.007939425056893037
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2760743609787991
model;0.00790508978921216
=;0.006757171778107798
"BertModel.from_pretrained(""bert-base-uncased"")";0.05112887986992985
inputs;0.00995196335384548
=;0.0064086126277862554
"tokenizer(""Hello,";0.024584260239333154
my;0.005773723204071733
dog;0.006282174736004803
is;0.005513354978231645
"cute"",";0.007177967515628926
"return_tensors=""pt"")";0.021366415073335927
outputs;0.00853175243539081
=;0.005994661836329847
model(**inputs);0.011692380273927614
last_hidden_states;0.009724109993205792
=;0.005775682213251274
outputs.last_hidden_state;0.007312459946769694
***;0.005258338589958372
