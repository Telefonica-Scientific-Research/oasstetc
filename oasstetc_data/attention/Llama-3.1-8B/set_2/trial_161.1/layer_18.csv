text;attention
The;0.013660149619112587
easiest;0.011182791369942673
way;0.01187718497640719
to;0.01083107091252464
import;0.015064793849519736
the;0.011984977546852616
BERT;0.042902034797146146
language;0.012163698744016349
model;0.02155025947767969
into;0.013661043242022204
python;0.015881511443576204
for;0.013065431288819181
use;0.01118237345321732
with;0.01161804234741392
PyTorch;0.023090096604212962
is;0.012052808878749632
using;0.012523324364458533
the;0.012480794700681095
Hugging;0.01332955616715648
Face;0.015094247377067898
Transformer's;0.02269828106677373
library,;0.013961893895166779
which;0.011334414206295764
has;0.011155669549449661
built;0.010354517233037142
in;0.011748081027194173
methods;0.011067014711402009
for;0.01157309313762887
pre-training,;0.015652299536450733
inference,;0.01234838179926155
and;0.010312867769306552
deploying;0.010330565325067334
BERT.;0.01784277440596832
â€˜**;0.014289656181757452
from;0.012284875256457065
transformers;0.016481054027001986
import;0.012303808236622528
AutoTokenizer,;0.03196163037113531
BertModel;0.02392225517631291
import;0.011552206278445113
torch;0.014043223807940331
tokenizer;0.014682380175669705
=;0.01153567293313901
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.10272646078629427
model;0.012586759413919806
=;0.011088498917650173
"BertModel.from_pretrained(""bert-base-uncased"")";0.033584490433745215
inputs;0.014646766375849328
=;0.010776147006536792
"tokenizer(""Hello,";0.02290569323013824
my;0.010826389436627556
dog;0.01164817208558404
is;0.010018898393345773
"cute"",";0.011317639094327986
"return_tensors=""pt"")";0.02333414057122376
outputs;0.013008636272590099
=;0.010372547470186504
model(**inputs);0.014927793986557487
last_hidden_states;0.01497407172429731
=;0.010388478753982544
outputs.last_hidden_state;0.012174177040832965
***;0.010061431738248942
