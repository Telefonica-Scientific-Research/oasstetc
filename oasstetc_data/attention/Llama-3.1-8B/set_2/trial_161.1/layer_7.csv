text;attention
The;0.009235103812841799
easiest;0.007577461786910993
way;0.007826636954792583
to;0.0072964543519506994
import;0.012727265275677087
the;0.00833913773452079
BERT;0.026982430368382312
language;0.008609542619487471
model;0.019524121348512575
into;0.014107701584682029
python;0.018864546692567114
for;0.009532036937472328
use;0.007496651502067194
with;0.007132167069050798
PyTorch;0.018728848848022586
is;0.016052246991107418
using;0.008811399278474769
the;0.0075901959612230336
Hugging;0.008467249498879923
Face;0.01039320806887874
Transformer's;0.018084090220374716
library,;0.015581494784369595
which;0.008185387613978412
has;0.007207663943059959
built;0.006058489523706911
in;0.007690063919000332
methods;0.007964997155249183
for;0.007462893373923768
pre-training,;0.012613558166478103
inference,;0.009266011728910723
and;0.00633872021726835
deploying;0.006780929457905786
BERT.;0.022718226870131766
â€˜**;0.01924431342660397
from;0.009815361256383454
transformers;0.013418842453183331
import;0.01057773699701629
AutoTokenizer,;0.02559036631227448
BertModel;0.015187688830847533
import;0.012399848497419038
torch;0.008817243281387152
tokenizer;0.00959851439436597
=;0.011264510488405778
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.24536639775579308
model;0.009119507078889635
=;0.00806168572983931
"BertModel.from_pretrained(""bert-base-uncased"")";0.06189691938378337
inputs;0.009777335472278038
=;0.007792360050758407
"tokenizer(""Hello,";0.04190226378304371
my;0.006776536994876824
dog;0.007189126190088177
is;0.006285933538543528
"cute"",";0.008075216150400953
"return_tensors=""pt"")";0.01860669535454545
outputs;0.00976955910813073
=;0.006754073422712429
model(**inputs);0.014338220282606177
last_hidden_states;0.010749602252142405
=;0.006436381441690342
outputs.last_hidden_state;0.008002735411108311
***;0.0059380910010222495
