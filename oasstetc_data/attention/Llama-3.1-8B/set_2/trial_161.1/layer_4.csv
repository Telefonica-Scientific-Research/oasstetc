text;attention
The;0.0138508985645371
easiest;0.01312855127532326
way;0.012483927687187151
to;0.012038459571806773
import;0.016561525521674475
the;0.012944216062002982
BERT;0.01921512043463761
language;0.014599113448963474
model;0.015079858088306976
into;0.016116220089560047
python;0.015998961603830127
for;0.013750547891836181
use;0.012110041213716083
with;0.012275149136347675
PyTorch;0.017688414568990212
is;0.015881916802559543
using;0.013072256542552816
the;0.011730166775684888
Hugging;0.013886771613937723
Face;0.015076658145132668
Transformer's;0.019674087923216628
library,;0.016334635150009456
which;0.012205878077142074
has;0.011676267333906134
built;0.011154361769487645
in;0.012740015023858773
methods;0.01249132697796824
for;0.011944195299517086
pre-training,;0.017662840054733387
inference,;0.013405835355504037
and;0.011125506527199937
deploying;0.011835843909023161
BERT.;0.017611433576783703
â€˜**;0.019854075099530882
from;0.013377543920079737
transformers;0.01613468381430158
import;0.013499022745056934
AutoTokenizer,;0.02124091244235139
BertModel;0.015150379031436486
import;0.013529378746796049
torch;0.011577278430944235
tokenizer;0.01406436513590331
=;0.013541915415869214
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08575155902056593
model;0.014209752832861784
=;0.012862805932082631
"BertModel.from_pretrained(""bert-base-uncased"")";0.05064058579100875
inputs;0.015109859364951924
=;0.012565597099070505
"tokenizer(""Hello,";0.026800868783062023
my;0.011390141985509295
dog;0.011634770700322476
is;0.010897302225112733
"cute"",";0.012906640610136796
"return_tensors=""pt"")";0.02375591595334546
outputs;0.013010322989917765
=;0.011403893786058683
model(**inputs);0.01615074465428902
last_hidden_states;0.016606469110078445
=;0.01120558877388811
outputs.last_hidden_state;0.013185150424729852
***;0.010591473163800132
