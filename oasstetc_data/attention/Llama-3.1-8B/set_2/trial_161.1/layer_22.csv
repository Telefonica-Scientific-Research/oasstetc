text;attention
The;0.015062265635540282
easiest;0.013052393496659964
way;0.012644596655346858
to;0.012070153846618819
import;0.015869896190316684
the;0.013212069329534767
BERT;0.029762401955934558
language;0.013921378495288196
model;0.01939676453689537
into;0.013522940017586292
python;0.014771428087600444
for;0.01317023338476192
use;0.01211186398789037
with;0.012766042732953392
PyTorch;0.019555999043861832
is;0.013937579388926181
using;0.0142307090713798
the;0.013400308427963458
Hugging;0.014557363160539507
Face;0.014189772495403945
Transformer's;0.019539846611971785
library,;0.015190713496806013
which;0.01269098630886327
has;0.012444020376971943
built;0.011606746013489487
in;0.012838723761920636
methods;0.01276705935646153
for;0.012740845188487716
pre-training,;0.01574376030983181
inference,;0.014102183637445496
and;0.011433359387680043
deploying;0.011987868584184321
BERT.;0.01919780779606253
â€˜**;0.025329029759503182
from;0.01335897441829604
transformers;0.013528459426792091
import;0.01308992182633663
AutoTokenizer,;0.023257440586704942
BertModel;0.019971089439777483
import;0.012825140538235625
torch;0.014703536258792735
tokenizer;0.015427363930053879
=;0.012701683875267824
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07078418144496965
model;0.013464032436290749
=;0.012279415304151149
"BertModel.from_pretrained(""bert-base-uncased"")";0.037359665916916224
inputs;0.01531552730397744
=;0.012337354079426056
"tokenizer(""Hello,";0.02148743540570607
my;0.011452413236882434
dog;0.01169427259790216
is;0.011147584718552192
"cute"",";0.01294835756533017
"return_tensors=""pt"")";0.022822469235231096
outputs;0.01495383284911675
=;0.012120199384341692
model(**inputs);0.015751300996079503
last_hidden_states;0.018539289716686294
=;0.011770729370679658
outputs.last_hidden_state;0.014498315807589219
***;0.01159090179923186
