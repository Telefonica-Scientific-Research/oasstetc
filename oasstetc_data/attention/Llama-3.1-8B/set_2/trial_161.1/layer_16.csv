text;attention
The;0.009043084830278464
easiest;0.007190343821334046
way;0.007398860291224395
to;0.006461823394579447
import;0.009648317177568538
the;0.007293891307002782
BERT;0.053506786068984284
language;0.008131737130210714
model;0.012449456116396867
into;0.00844874536691861
python;0.012232941409959076
for;0.008225842148768637
use;0.006658422438684942
with;0.0066398134235547196
PyTorch;0.02627288244394694
is;0.009020796794419745
using;0.008326787502138095
the;0.007445162064926206
Hugging;0.011418980582759112
Face;0.011542645455737714
Transformer's;0.021515959766292073
library,;0.010053488510969644
which;0.007131626067580716
has;0.006523752651868321
built;0.006208701332455739
in;0.006876758647460462
methods;0.007020012429083644
for;0.00688207344740485
pre-training,;0.012608562205938551
inference,;0.008436931485369243
and;0.006035966991714496
deploying;0.006455835955660141
BERT.;0.013109469371217116
â€˜**;0.015216745993016699
from;0.009906086151141535
transformers;0.011351650706482537
import;0.008111306768441844
AutoTokenizer,;0.02542533591821791
BertModel;0.01930115278479124
import;0.009506554759188098
torch;0.00887425971532212
tokenizer;0.010433660861150548
=;0.007754838446881938
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2952344610972596
model;0.008003092525196005
=;0.007211822518173959
"BertModel.from_pretrained(""bert-base-uncased"")";0.05800456939792755
inputs;0.008874750062828528
=;0.006562661243497258
"tokenizer(""Hello,";0.01756568922506048
my;0.006079137460609868
dog;0.006725753031525175
is;0.005717327981629288
"cute"",";0.007038738043729979
"return_tensors=""pt"")";0.02948143348920692
outputs;0.008316541923575158
=;0.0063261989269432824
model(**inputs);0.013586899010488101
last_hidden_states;0.012198083948212967
=;0.006247566496386779
outputs.last_hidden_state;0.008807733958856097
***;0.0059194889218498825
