text;attention
The;0.010811059637922537
easiest;0.005326012880495071
way;0.005031026802444628
to;0.004894207782956832
import;0.006750351311732034
the;0.005600195570328728
BERT;0.11717898887693651
language;0.007120101995505786
model;0.016161865376249196
into;0.009665894287165055
python;0.01338787195817948
for;0.008995286929460364
use;0.006383437791196506
with;0.005936197730858234
PyTorch;0.0471161452052155
is;0.008328929668928465
using;0.007481322058169419
the;0.007606986588598466
Hugging;0.008437871213111707
Face;0.008011448618012328
Transformer's;0.023045673875202854
library,;0.007135561499630685
which;0.0048956002028403564
has;0.004645238980806918
built;0.004327449329385293
in;0.0048460722921131304
methods;0.0048682039818759085
for;0.004898811717930029
pre-training,;0.010697079738421053
inference,;0.007745275880504852
and;0.004857726947667299
deploying;0.005033634495139231
BERT.;0.012472573781360517
â€˜**;0.010166009259766797
from;0.006882368015387677
transformers;0.008006955941713747
import;0.006789815067274877
AutoTokenizer,;0.03672222816147078
BertModel;0.044726507264209264
import;0.008036183557086842
torch;0.008093940756595013
tokenizer;0.007525955860681543
=;0.006605085401050599
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.26599193199797666
model;0.0065438988273239474
=;0.005471821822424541
"BertModel.from_pretrained(""bert-base-uncased"")";0.05181113672583249
inputs;0.006840513364162161
=;0.005096539038356871
"tokenizer(""Hello,";0.016421965864689584
my;0.004513034223846902
dog;0.005056865701792966
is;0.0043311162505388625
"cute"",";0.005469485281531077
"return_tensors=""pt"")";0.024349480858854102
outputs;0.0063619378165121225
=;0.004589056408491146
model(**inputs);0.009660670900168812
last_hidden_states;0.010031359997426065
=;0.004614883353785323
outputs.last_hidden_state;0.005406903962392638
***;0.004188243312311611
