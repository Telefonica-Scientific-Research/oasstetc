text;attention
The;0.013480713694747442
easiest;0.010984478765789174
way;0.010699220187597383
to;0.010663200727281726
import;0.01243801833938334
the;0.011403965216065359
BERT;0.01947179577356266
language;0.01202573159225504
model;0.012592132897824996
into;0.011547689091147206
python;0.014422506470063554
for;0.011262865067115935
use;0.010562592412199229
with;0.011166020440069828
PyTorch;0.017322933481498864
is;0.012078085736534275
using;0.011894625138882762
the;0.011475030341633946
Hugging;0.013874457525541804
Face;0.012513547043173097
Transformer's;0.015608419739487962
library,;0.013580326049758383
which;0.010922227383846626
has;0.010587402937716196
built;0.010206442865761698
in;0.011058444241204758
methods;0.01113371686396175
for;0.011262575900138048
pre-training,;0.014670880708667707
inference,;0.012666857328337937
and;0.01022248110256762
deploying;0.010678040230036847
BERT.;0.017012696020518112
â€˜**;0.01679040393955481
from;0.012345039558759595
transformers;0.01250396373212132
import;0.012069751875141527
AutoTokenizer,;0.025174350783918168
BertModel;0.016822071615387405
import;0.011622227243604394
torch;0.014366804650118033
tokenizer;0.015231843561394037
=;0.011336237197189606
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.16858989184034145
model;0.01114721064752026
=;0.010728491634444065
"BertModel.from_pretrained(""bert-base-uncased"")";0.06343896110670005
inputs;0.012566919068047131
=;0.011383021959057946
"tokenizer(""Hello,";0.01830302533858738
my;0.010377570315285956
dog;0.011663637079534196
is;0.010271279645831752
"cute"",";0.012297754285235414
"return_tensors=""pt"")";0.017756097589950486
outputs;0.011528638853374287
=;0.010330185600287312
model(**inputs);0.014931181699942982
last_hidden_states;0.013104740256792974
=;0.01009339146323129
outputs.last_hidden_state;0.011496565929563302
***;0.010238620214711773
