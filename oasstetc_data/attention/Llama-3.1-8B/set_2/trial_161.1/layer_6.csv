text;attention
The;0.011860447159646724
easiest;0.01284488166772667
way;0.010725734926824779
to;0.010768316617976309
import;0.016384267146028184
the;0.011680901234913997
BERT;0.022215826447357714
language;0.011780399071798205
model;0.017770397570025624
into;0.017830633290789804
python;0.01573586507292598
for;0.0128717932284512
use;0.010570848169946408
with;0.010299692484367386
PyTorch;0.015473184615791917
is;0.01734843940889595
using;0.01165192299787152
the;0.010452195505528524
Hugging;0.011501297950468094
Face;0.013959714910363014
Transformer's;0.01953588513183321
library,;0.015561150938425552
which;0.010372568913172122
has;0.010590239376978873
built;0.00877060977997128
in;0.0110282687814603
methods;0.010323032717868903
for;0.01096137654055772
pre-training,;0.017450172724791817
inference,;0.011575731915996762
and;0.009171066551009088
deploying;0.009658940128724352
BERT.;0.017546971911070568
â€˜**;0.01651138922590486
from;0.012021098266646223
transformers;0.015921623036432814
import;0.014850619789149293
AutoTokenizer,;0.027227415401435258
BertModel;0.014725297354970178
import;0.017177568385141225
torch;0.010069788800463468
tokenizer;0.01302333710473002
=;0.013925056782202797
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.12983792897056415
model;0.012369907757608442
=;0.01091616941657835
"BertModel.from_pretrained(""bert-base-uncased"")";0.05515757182810539
inputs;0.01251850094835752
=;0.009997430925086712
"tokenizer(""Hello,";0.036410460555177084
my;0.00977773142221173
dog;0.009666541152221143
is;0.00900704255101939
"cute"",";0.01163398957847666
"return_tensors=""pt"")";0.02197705033916125
outputs;0.011713342059504925
=;0.009191275448102632
model(**inputs);0.015792073406046575
last_hidden_states;0.01319902670117672
=;0.00911159242242014
outputs.last_hidden_state;0.011520456565550072
***;0.008475938915996396
