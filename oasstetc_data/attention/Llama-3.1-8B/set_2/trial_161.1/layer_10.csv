text;attention
The;0.010953253576525745
easiest;0.007737179148976994
way;0.007792172637678893
to;0.0072891529440707896
import;0.010880117488406497
the;0.008519620193710246
BERT;0.04196002423314002
language;0.00892815163348735
model;0.023204770390499136
into;0.016696039865682456
python;0.016991199974873764
for;0.012726943433160333
use;0.008430272786412564
with;0.007730668529640997
PyTorch;0.021587801997465166
is;0.013601060019867687
using;0.009781318521194913
the;0.00861387856956679
Hugging;0.010638567251083168
Face;0.01081339584330762
Transformer's;0.021851378266612688
library,;0.012722927359673245
which;0.008122876098086998
has;0.007094931418596767
built;0.006513515510715412
in;0.00792948767495972
methods;0.007644333934634083
for;0.007799224431123969
pre-training,;0.014683962959795585
inference,;0.012046350735832767
and;0.0071100298638141265
deploying;0.007351535696653041
BERT.;0.016873200371604367
â€˜**;0.013908754032425436
from;0.010846007149967389
transformers;0.012257175497172005
import;0.009843305219544797
AutoTokenizer,;0.05203608679847664
BertModel;0.02566585760196691
import;0.010564758127627027
torch;0.008698451777649673
tokenizer;0.01093151584482557
=;0.010094317741049784
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.20457248806719425
model;0.0093817001167088
=;0.008331725888361997
"BertModel.from_pretrained(""bert-base-uncased"")";0.04619831144427188
inputs;0.009839654527481796
=;0.008111325938637724
"tokenizer(""Hello,";0.027188737331562962
my;0.007046727036263091
dog;0.0074909744831912406
is;0.006688942853586204
"cute"",";0.008306334430923476
"return_tensors=""pt"")";0.021875486437579406
outputs;0.008926415366518583
=;0.007262522912835651
model(**inputs);0.016241445942485864
last_hidden_states;0.01272834765134974
=;0.0073409437324469355
outputs.last_hidden_state;0.008268768473161596
***;0.006733574213879383
