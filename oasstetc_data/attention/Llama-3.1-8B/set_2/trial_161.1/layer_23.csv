text;attention
The;0.01668409384559613
easiest;0.0148216714100635
way;0.01428590979835597
to;0.014079487122687806
import;0.0164458363566083
the;0.014570678970374671
BERT;0.023350234538965446
language;0.014645689215109505
model;0.015971045286330744
into;0.01581090052620032
python;0.015267602888805853
for;0.01465008649333634
use;0.013474353732389067
with;0.014462033387180892
PyTorch;0.017750997030326805
is;0.014354497583599827
using;0.015504248357457883
the;0.014640007405844773
Hugging;0.01447290680903951
Face;0.016617637222104673
Transformer's;0.016923401654801308
library,;0.015935140250922904
which;0.013879805357852881
has;0.01380538357287656
built;0.01324258725483164
in;0.014260077610284196
methods;0.013880109084007122
for;0.014547166275392567
pre-training,;0.016198442425689262
inference,;0.015418585324850438
and;0.013115777158019754
deploying;0.01443926338400037
BERT.;0.024560496921529926
â€˜**;0.017832215515140734
from;0.01493363604424771
transformers;0.014241851634958044
import;0.014831050771823567
AutoTokenizer,;0.022215701885956278
BertModel;0.01760086238957725
import;0.013987157464008792
torch;0.016570488151658243
tokenizer;0.015880834029607598
=;0.013958297816933774
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04371151148230293
model;0.014018186487160413
=;0.013844874415141943
"BertModel.from_pretrained(""bert-base-uncased"")";0.030885069016119593
inputs;0.015830720838809865
=;0.013936766798058008
"tokenizer(""Hello,";0.019476277040699944
my;0.013417471994678049
dog;0.014140198857227409
is;0.013117669449870304
"cute"",";0.01474462784424213
"return_tensors=""pt"")";0.01935547671998362
outputs;0.014331744409669661
=;0.013283032610849047
model(**inputs);0.01598852369296957
last_hidden_states;0.015542784540850424
=;0.013140200298724848
outputs.last_hidden_state;0.014041200973370022
***;0.01307541256992313
