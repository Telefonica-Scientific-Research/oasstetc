text;attention
The;0.008833089751662053
easiest;0.004749937125337006
way;0.004782755525352805
to;0.004460863637885831
import;0.007410846662946256
the;0.005025857184985703
BERT;0.07069671117075538
language;0.006356811570120022
model;0.012430314271568838
into;0.007238422539633402
python;0.01349388410241872
for;0.00685598345876839
use;0.0053501143161325355
with;0.00509107458599402
PyTorch;0.033220426124479244
is;0.006498053961194086
using;0.0063928710212655445
the;0.006619417406978749
Hugging;0.007974239350526291
Face;0.007388248293883562
Transformer's;0.017243181717024318
library,;0.006785110593645002
which;0.004848777608519903
has;0.00457343343969606
built;0.004067359939224013
in;0.004548258105880074
methods;0.0045721264515232365
for;0.004501830543244608
pre-training,;0.011550789783281761
inference,;0.007470289520415766
and;0.004265060051663939
deploying;0.004873541565692867
BERT.;0.01141802406817765
â€˜**;0.011015325890279589
from;0.006813877832430076
transformers;0.007056672165691418
import;0.007280510082124481
AutoTokenizer,;0.03922833921287807
BertModel;0.025091956502975594
import;0.006622146663603189
torch;0.006908076268897487
tokenizer;0.0074329935621032924
=;0.00573924723041135
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3889458977645527
model;0.005580157416341128
=;0.004916826794403967
"BertModel.from_pretrained(""bert-base-uncased"")";0.04642635184749909
inputs;0.006645027219504542
=;0.004640453539302437
"tokenizer(""Hello,";0.016712984307206823
my;0.004193387343385941
dog;0.0045664770628359
is;0.004046107445429305
"cute"",";0.005034375320456095
"return_tensors=""pt"")";0.01605186050542907
outputs;0.00619354518789762
=;0.004474053338761262
model(**inputs);0.010401417873424175
last_hidden_states;0.012387506085096278
=;0.004508238862796387
outputs.last_hidden_state;0.005631826148658637
***;0.0038666550757465133
