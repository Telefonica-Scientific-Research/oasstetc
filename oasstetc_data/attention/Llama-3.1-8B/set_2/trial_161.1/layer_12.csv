text;attention
The;0.00735698034334437
easiest;0.004319243923869088
way;0.0040634205800316615
to;0.003709346698262069
import;0.0057340118220051695
the;0.0044493895619604415
BERT;0.01991108683848606
language;0.00433479526344202
model;0.008004311281497131
into;0.006824388330789807
python;0.010635863494296276
for;0.005624299636866069
use;0.004292901636301036
with;0.0037343130687725323
PyTorch;0.013420911618779549
is;0.0072549497672114854
using;0.005393323619741772
the;0.004337375377954423
Hugging;0.005927213366208445
Face;0.0059260080651804175
Transformer's;0.012958217454032098
library,;0.00637438802317783
which;0.004289571795260588
has;0.003949583388244033
built;0.003327276223425037
in;0.004280323791485883
methods;0.0038995340140558585
for;0.003965741469945306
pre-training,;0.0106657735616015
inference,;0.0062612561518949235
and;0.003708850816913168
deploying;0.0037945083744594602
BERT.;0.009057043947332026
â€˜**;0.012793624992440022
from;0.006412564783835303
transformers;0.00721802857047453
import;0.0066222122422704074
AutoTokenizer,;0.024202129848958788
BertModel;0.010517470877181743
import;0.006651058264999019
torch;0.004712813304422902
tokenizer;0.006691956730946913
=;0.005869692254558303
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.48718680644459733
model;0.0050540831169513375
=;0.004835144036258835
"BertModel.from_pretrained(""bert-base-uncased"")";0.08949775994016278
inputs;0.0051875270262283
=;0.004311559024552543
"tokenizer(""Hello,";0.027666973727040103
my;0.0035309763714628357
dog;0.0041021622615144155
is;0.003432479143702839
"cute"",";0.004687368502445901
"return_tensors=""pt"")";0.01733101003230515
outputs;0.004899775575448283
=;0.0037702004313079278
model(**inputs);0.01155735727223129
last_hidden_states;0.007775147509051351
=;0.003910601403762971
outputs.last_hidden_state;0.004649941401432271
***;0.003135371602628261
