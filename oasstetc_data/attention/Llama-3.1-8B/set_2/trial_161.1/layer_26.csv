text;attention
The;0.012354923659447445
easiest;0.011314512646871331
way;0.011911548958323992
to;0.0106023459643294
import;0.012653995574249647
the;0.01151722603990308
BERT;0.023946553468886796
language;0.011566057345369227
model;0.01305697885969826
into;0.011984381635810795
python;0.013939791525011874
for;0.01146498087772528
use;0.01111236816631668
with;0.011400254065929795
PyTorch;0.019079013415404175
is;0.012710821417107035
using;0.012983335443600424
the;0.012190408169321348
Hugging;0.01218199217670567
Face;0.013823783503639392
Transformer's;0.016544511046483558
library,;0.013169756108689694
which;0.01158687601220079
has;0.011189747470795606
built;0.01062748701145636
in;0.011753425453954884
methods;0.012065702879441574
for;0.011597641396264556
pre-training,;0.014295997155071148
inference,;0.012130474113405421
and;0.01010473281454946
deploying;0.010963208264433316
BERT.;0.019835456398117908
â€˜**;0.022891754889591668
from;0.012260936230421267
transformers;0.012480762928884077
import;0.01290633776157943
AutoTokenizer,;0.02263321019169473
BertModel;0.01775053690793095
import;0.011409861769930637
torch;0.01340848992313955
tokenizer;0.012777603892098312
=;0.011238276431325162
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.14675158302431676
model;0.011576495119419933
=;0.010726618549698428
"BertModel.from_pretrained(""bert-base-uncased"")";0.06016641211151018
inputs;0.012587447611296553
=;0.010745238040995999
"tokenizer(""Hello,";0.019957700930748543
my;0.01065484537292995
dog;0.011107357634401807
is;0.010286236777191862
"cute"",";0.012196074915624827
"return_tensors=""pt"")";0.01908599516811683
outputs;0.012605755880153342
=;0.01019891584264137
model(**inputs);0.01484190610248209
last_hidden_states;0.015404413029350101
=;0.01021699476590556
outputs.last_hidden_state;0.01260960724354296
***;0.0108623419145613
