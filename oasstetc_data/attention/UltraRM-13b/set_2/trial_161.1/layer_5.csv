text;attention
The;0.20979155852355436
easiest;0.011168322027189054
way;0.010648376188382307
to;0.010388860350613656
import;0.01427300202122195
the;0.009440515786522016
BERT;0.015466276209760332
language;0.011748003833481306
model;0.010928859718593299
into;0.011619429839977147
python;0.011982961442082734
for;0.010283994255957996
use;0.009792045697111213
with;0.009703445845117316
PyTorch;0.012025376578127283
is;0.011381428375786186
using;0.010113321547209396
the;0.009315570388613477
Hugging;0.011304648301699447
Face;0.011703590476814267
Transformer's;0.013909206323328116
library,;0.011666349281714058
which;0.009900511167378773
has;0.009523266259566255
built;0.009343253931689456
in;0.01029896768380741
methods;0.01037009670631009
for;0.01045417999528253
pre-training,;0.015235856125917121
inference,;0.011706840468924173
and;0.009315400760896757
deploying;0.01009251940057755
BERT.;0.012186562470802471
â€˜**;0.012810129152218951
from;0.012617581445851227
transformers;0.01180151462028001
import;0.011906875321678785
AutoTokenizer,;0.017635765550508508
BertModel;0.010878236334813083
import;0.01089093604318416
torch;0.010348570592879823
tokenizer;0.010950983717591688
=;0.01134269617871106
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08836594538045398
model;0.010515369070416548
=;0.01064338343855379
"BertModel.from_pretrained(""bert-base-uncased"")";0.027320107460564493
inputs;0.010345032253787231
=;0.010564595504996336
"tokenizer(""Hello,";0.01890772011589172
my;0.009090252646593381
dog;0.009744509165544783
is;0.009162458761175326
"cute"",";0.010133567282370422
"return_tensors=""pt"")";0.015733749726634622
outputs;0.009852671163356398
=;0.009413838380133037
model(**inputs);0.016070308261067465
last_hidden_states;0.015632597853278554
=;0.009327124692317248
outputs.last_hidden_state;0.012006182041314884
***;0.008904699859823256
