text;attention
The;0.05651571233691272
easiest;0.01683191009127961
way;0.014510278550176791
to;0.013109540429110705
import;0.017787420538009913
the;0.012668245477331527
BERT;0.021984596518061096
language;0.011933088019814932
model;0.017419894980314515
into;0.012557010371674384
python;0.018026470619533042
for;0.011622789673496817
use;0.012989475990195239
with;0.010869256254171858
PyTorch;0.02510080391515653
is;0.016414679491440022
using;0.011485557199957445
the;0.010807980121911349
Hugging;0.017368350158431217
Face;0.011372325221860713
Transformer's;0.07744175207443627
library,;0.016281484102564265
which;0.011232188449259897
has;0.010362937673220352
built;0.010767689952979312
in;0.010269002224488682
methods;0.010939730190613414
for;0.0098727036789678
pre-training,;0.016537165404307063
inference,;0.012120602236991358
and;0.009797487300495985
deploying;0.011635273721060302
BERT.;0.021748122085648258
â€˜**;0.017791228700170157
from;0.011341945446402624
transformers;0.014109964397610007
import;0.01108188835716699
AutoTokenizer,;0.017628613543999878
BertModel;0.012944062978628013
import;0.011052538241347611
torch;0.011904900404364236
tokenizer;0.012938999477908856
=;0.010424490665024103
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07276864335208019
model;0.010391817127581445
=;0.009750473836173277
"BertModel.from_pretrained(""bert-base-uncased"")";0.04934231912877128
inputs;0.010045513054894026
=;0.009559455209406422
"tokenizer(""Hello,";0.013795670034362176
my;0.00956982748285259
dog;0.00969262011747446
is;0.009449669793444861
"cute"",";0.010842619985495324
"return_tensors=""pt"")";0.01333453107454906
outputs;0.00949059151885836
=;0.009321265280644588
model(**inputs);0.012134553794489251
last_hidden_states;0.0107035313864373
=;0.00914528300293763
outputs.last_hidden_state;0.010084343857316696
***;0.008977113695735067
