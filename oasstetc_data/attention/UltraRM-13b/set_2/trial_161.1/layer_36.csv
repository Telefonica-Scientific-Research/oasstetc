text;attention
The;0.0814418926280182
easiest;0.004683332871222751
way;0.0038012618378280598
to;0.0016716895129045982
import;0.002624321787626157
the;0.001414843750393837
BERT;0.010329579408280986
language;0.001456418851170582
model;0.00381222152922088
into;0.002443461434632848
python;0.006265114001760366
for;0.001815115625972417
use;0.0019880265368010237
with;0.0013114775843778204
PyTorch;0.028977019498616415
is;0.005092153828966707
using;0.0017473436697751745
the;0.0012967627562686186
Hugging;0.006693238642449964
Face;0.0016795200061446088
Transformer's;0.10423911814468406
library,;0.004196848497008894
which;0.0012177002640794278
has;0.0009514924370972297
built;0.0009344924972480632
in;0.0008761562592492007
methods;0.0010462390354576032
for;0.0008124887350974584
pre-training,;0.005712600822066303
inference,;0.0014681040923026898
and;0.0007515329801332832
deploying;0.0012542149068396342
BERT.;0.013616474318207483
â€˜**;0.004945274953202344
from;0.0013001452543069684
transformers;0.002108895994534545
import;0.0010627966815292525
AutoTokenizer,;0.006244867862084835
BertModel;0.002057422385233631
import;0.0010773952283778167
torch;0.0014091732412607437
tokenizer;0.001697232925929318
=;0.0008981673740363712
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5627466769837549
model;0.0008213039269602337
=;0.0007192255204854041
"BertModel.from_pretrained(""bert-base-uncased"")";0.0920162782402111
inputs;0.0007483321903769931
=;0.0006558558611968051
"tokenizer(""Hello,";0.001857541610539134
my;0.0006586760564505497
dog;0.0006797214726837587
is;0.0006340602809223497
"cute"",";0.000947700133835501
"return_tensors=""pt"")";0.0017297038403149927
outputs;0.000632775141812532
=;0.0006066060836317677
model(**inputs);0.0012849651883626997
last_hidden_states;0.0009305814786437555
=;0.0005766209733007264
outputs.last_hidden_state;0.0007878283046365639
***;0.0005419160594809923
