text;attention
The;0.010125709176832056
easiest;0.007941609857725983
way;0.006006707373664856
to;0.005466014811753367
import;0.011118906982132882
the;0.005645564583051829
BERT;0.02756511529828888
language;0.005468102691635288
model;0.011101517082139956
into;0.007898556366539173
python;0.012505744833056196
for;0.005683748465604578
use;0.005654530889761351
with;0.005169425746261213
PyTorch;0.021238053354488806
is;0.013656745880364921
using;0.006263464748518722
the;0.005470810733817035
Hugging;0.006752013071513053
Face;0.007327936076616793
Transformer's;0.457784399621648
library,;0.007304095359695222
which;0.004905766526189567
has;0.004680749693769607
built;0.004722126636016565
in;0.004802463296026665
methods;0.00531134545148608
for;0.004734586527851751
pre-training,;0.00792146426844071
inference,;0.006204846635843294
and;0.004539157177248438
deploying;0.005288555235372162
BERT.;0.020512842302173456
â€˜**;0.015595835182864275
from;0.00619239464638737
transformers;0.006315110589983991
import;0.0056275878908906025
AutoTokenizer,;0.009444457351219617
BertModel;0.006850570227402186
import;0.006056053090046667
torch;0.005526756960739304
tokenizer;0.007425800109793078
=;0.00524535050716495
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.05561689338287291
model;0.0051235676946841426
=;0.0047544883000482
"BertModel.from_pretrained(""bert-base-uncased"")";0.05515381682550268
inputs;0.0052624260354927075
=;0.004653026384663656
"tokenizer(""Hello,";0.00678517958773257
my;0.004518835181854263
dog;0.004683766979174451
is;0.004429257975416614
"cute"",";0.0053584247338096
"return_tensors=""pt"")";0.007292688712558476
outputs;0.004679480859643271
=;0.004480205167623706
model(**inputs);0.006083876116729746
last_hidden_states;0.005985747841334864
=;0.0044524783619123275
outputs.last_hidden_state;0.005294436219562392
***;0.00433881032736303
