text;attention
The;0.03575762619806386
easiest;0.018542585188539705
way;0.016253754612336482
to;0.013988116075249513
import;0.015895199812610332
the;0.013408776732489433
BERT;0.01897323092534597
language;0.01267742166535681
model;0.015541503445644714
into;0.013409723648184886
python;0.019556464120514146
for;0.01269648095747748
use;0.013946520224765824
with;0.012025643679935258
PyTorch;0.020695524926417592
is;0.017227762964059666
using;0.012596502212591265
the;0.01207230064934282
Hugging;0.017574016959665935
Face;0.012798288173919628
Transformer's;0.07536771585918924
library,;0.017117005097674838
which;0.012806300855473188
has;0.012006264921682112
built;0.01240769576085993
in;0.0119087977259696
methods;0.01283971765608471
for;0.011512088187480408
pre-training,;0.016643102522325013
inference,;0.013333872929727287
and;0.011430871731867163
deploying;0.013184641714893558
BERT.;0.0193095269640494
â€˜**;0.018827601628176576
from;0.012586739273238078
transformers;0.0145595342630788
import;0.012393640694665298
AutoTokenizer,;0.016894992961926474
BertModel;0.013890509100577465
import;0.012499022458796595
torch;0.01320357954253884
tokenizer;0.013864256299508331
=;0.011968443683079531
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04928108841995758
model;0.011909853097107697
=;0.01140824953172696
"BertModel.from_pretrained(""bert-base-uncased"")";0.04010524991745454
inputs;0.011735574242064586
=;0.011286830085728259
"tokenizer(""Hello,";0.014817786968793637
my;0.011347727237630424
dog;0.011480144018808548
is;0.011234144731292677
"cute"",";0.01249073638544415
"return_tensors=""pt"")";0.014649476163887665
outputs;0.011262131911340926
=;0.011101325381778956
model(**inputs);0.013583579754400082
last_hidden_states;0.012455307112193873
=;0.01097038992770511
outputs.last_hidden_state;0.011873062420273821
***;0.010813977687036786
