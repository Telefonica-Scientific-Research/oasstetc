text;attention
The;0.03682242253655215
easiest;0.00035869267129020303
way;0.000246917094050388
to;9.382749023021669e-05
import;0.00017175080297704252
the;9.13338934961878e-05
BERT;0.00389260785247167
language;0.00012115644653237712
model;0.00021128263395731685
into;0.0001838307464963073
python;0.000612924262482705
for;0.00012967529834961167
use;0.00018026531711929573
with;0.00010863608792940064
PyTorch;0.07545655872903026
is;0.0004238898516891136
using;0.00016758941950573277
the;0.00011699790212733197
Hugging;0.006738569176554905
Face;0.00015541994421224098
Transformer's;0.0868979469208959
library,;0.0004564907377245176
which;9.423408611178282e-05
has;6.807800121035812e-05
built;6.735703180704482e-05
in;6.0112388863341616e-05
methods;7.46537028591742e-05
for;5.347509830020091e-05
pre-training,;0.0011813596981027567
inference,;0.00012779059300664248
and;5.0803298537142294e-05
deploying;0.00010681344438654623
BERT.;0.0019025777305210518
â€˜**;0.0007094813298553932
from;8.951055107608325e-05
transformers;0.00018911753357539403
import;8.138457600921047e-05
AutoTokenizer,;0.0013188833141569616
BertModel;0.00030664803106550517
import;7.514382029560337e-05
torch;0.00014485232818609012
tokenizer;0.0001351605162022045
=;6.024571577883317e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7358516476629373
model;4.909826399652549e-05
=;4.3961784366284286e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.042567524642363776
inputs;4.369712289260054e-05
=;3.873103587825868e-05
"tokenizer(""Hello,";0.00015270117521024113
my;3.937878365673684e-05
dog;4.070153842955199e-05
is;3.751721639312491e-05
"cute"",";6.406636089677067e-05
"return_tensors=""pt"")";0.00017620636614128029
outputs;3.6649335727190705e-05
=;3.515744271426969e-05
model(**inputs);9.751929339035896e-05
last_hidden_states;6.747492282940049e-05
=;3.290879139719693e-05
outputs.last_hidden_state;5.804362114750575e-05
***;3.0546034049424937e-05
