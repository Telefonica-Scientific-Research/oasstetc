text;attention
The;0.006121620687748041
easiest;0.007571671274157204
way;0.005951657219950768
to;0.005179878186025429
import;0.007861836083638643
the;0.005277726492193737
BERT;0.03547231197180077
language;0.0054557594937267445
model;0.012071510576580046
into;0.009414862316251434
python;0.012672927749939134
for;0.006157571543287788
use;0.005284502974185641
with;0.0049812410158838975
PyTorch;0.054280250690472716
is;0.010777279299454122
using;0.0069073706325174875
the;0.006006649222989207
Hugging;0.007116646988391963
Face;0.017295237919128452
Transformer's;0.11274977298478817
library,;0.007038921079235828
which;0.004932106367562719
has;0.005001819515485021
built;0.004804937004979585
in;0.005222698813303036
methods;0.0049235771893128945
for;0.005177222812551568
pre-training,;0.009166746024523045
inference,;0.006542826357916975
and;0.00456919468242425
deploying;0.005159389407892559
BERT.;0.01701922432135882
â€˜**;0.021202709910108822
from;0.006870314874963372
transformers;0.008479261676593965
import;0.008231190477719896
AutoTokenizer,;0.021052494557580254
BertModel;0.009654276472666227
import;0.007970150987290661
torch;0.007000106511287331
tokenizer;0.009867130901720986
=;0.008813975162075957
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.24626636486622175
model;0.005876395403014525
=;0.005659734116352504
"BertModel.from_pretrained(""bert-base-uncased"")";0.11027566327641794
inputs;0.005657214530758821
=;0.005317977946999006
"tokenizer(""Hello,";0.010962610589176365
my;0.00461570166781676
dog;0.005583225918886849
is;0.004631568478165478
"cute"",";0.005413020339964487
"return_tensors=""pt"")";0.013423705107745602
outputs;0.005257415531165468
=;0.005021012173046544
model(**inputs);0.009820341171968587
last_hidden_states;0.007899544272889964
=;0.0049380390703373245
outputs.last_hidden_state;0.005678945119002197
***;0.004392959988404564
