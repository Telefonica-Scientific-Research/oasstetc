text;attention
The;0.02116841456081203
easiest;0.011072917500436628
way;0.010170256336577858
to;0.009881177660361827
import;0.017087014931793768
the;0.010230075199469824
BERT;0.036237009235368094
language;0.011502459714835453
model;0.013839064279205386
into;0.019991197608423515
python;0.018992832783812124
for;0.011803115393805642
use;0.011065239115168145
with;0.010173219695465065
PyTorch;0.022281992224486895
is;0.017268645182621663
using;0.0118405382169021
the;0.009937931078876767
Hugging;0.0111923881972083
Face;0.017006721980000705
Transformer's;0.06417747747839846
library,;0.013033952868962017
which;0.009511761960089397
has;0.009382343855124303
built;0.008928411412184346
in;0.01023120347012033
methods;0.010077794895210072
for;0.009689780864240858
pre-training,;0.021553197933781444
inference,;0.012591973305057793
and;0.009205436154302105
deploying;0.010805686905891102
BERT.;0.020312260347930808
â€˜**;0.012420501411937844
from;0.013009747852021688
transformers;0.012688259345307236
import;0.011360187116403098
AutoTokenizer,;0.02497945577193858
BertModel;0.01374491397223876
import;0.015386192080717452
torch;0.011604120522526792
tokenizer;0.014554060252401049
=;0.01236308007540513
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1001057536777625
model;0.010952347455661784
=;0.010649632338397177
"BertModel.from_pretrained(""bert-base-uncased"")";0.04418129558258381
inputs;0.012386874504075416
=;0.010346173067404953
"tokenizer(""Hello,";0.021404600100450005
my;0.009223514662123136
dog;0.009617129921913234
is;0.009241024693528459
"cute"",";0.010885208289944444
"return_tensors=""pt"")";0.016558912277278775
outputs;0.010328404679680943
=;0.00928114342629447
model(**inputs);0.01646995347522341
last_hidden_states;0.014888686188299525
=;0.009501244709170239
outputs.last_hidden_state;0.01099061488581122
***;0.00863347531657387
