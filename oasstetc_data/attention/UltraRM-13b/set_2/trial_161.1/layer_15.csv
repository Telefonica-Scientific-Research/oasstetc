text;attention
The;0.004412922093612197
easiest;0.00498987366188846
way;0.003909530340491882
to;0.003646952883550948
import;0.0055048963578636975
the;0.003804037183344947
BERT;0.0321642821996938
language;0.004429994949532021
model;0.013340844406797829
into;0.008451275775614964
python;0.009000708589464902
for;0.0050595035865971675
use;0.004307075180732934
with;0.0036571940933450725
PyTorch;0.023243681764000997
is;0.008724157171052222
using;0.0046402789989497826
the;0.0044597944834810326
Hugging;0.00580192667121956
Face;0.013372909399929075
Transformer's;0.0627436370130643
library,;0.0054776629368613
which;0.0036924802487616768
has;0.003628863304988396
built;0.0034692906425058713
in;0.0038185659615233387
methods;0.0035862169133700195
for;0.003607017670964996
pre-training,;0.008071831074382382
inference,;0.005871489041769106
and;0.0034111991899636175
deploying;0.0037494977815767092
BERT.;0.016033350670821768
â€˜**;0.014367556079128407
from;0.0056371968917974975
transformers;0.006589105129873276
import;0.0063870975432948306
AutoTokenizer,;0.027550793794425134
BertModel;0.009756235972941429
import;0.00816095204943827
torch;0.005794113425182622
tokenizer;0.008357203285171843
=;0.0061639932497057305
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.39539004944228073
model;0.004704609669595152
=;0.004290745470477707
"BertModel.from_pretrained(""bert-base-uncased"")";0.11944760656765949
inputs;0.004908679277394713
=;0.003970565966819875
"tokenizer(""Hello,";0.011982485334870672
my;0.0034507431408520328
dog;0.00392420671802214
is;0.0035167621202480672
"cute"",";0.004850925393496819
"return_tensors=""pt"")";0.01337931792441609
outputs;0.004080754264763761
=;0.0035468479427159714
model(**inputs);0.008461619756369294
last_hidden_states;0.0061278247861794475
=;0.0035509349742898753
outputs.last_hidden_state;0.004361111638243168
***;0.0032070199486289165
