text;attention
The;0.02593189495316528
easiest;0.009439096490098208
way;0.008213347276667386
to;0.008096037127202898
import;0.011847916832637544
the;0.007766104979114825
BERT;0.025103059375741876
language;0.0073082354663977465
model;0.011547457200549964
into;0.008169900885971056
python;0.01215839861745892
for;0.00722753767125407
use;0.007532157286771946
with;0.006724614022395902
PyTorch;0.016201739858138293
is;0.01432044549863018
using;0.007132053031184563
the;0.006851578648313549
Hugging;0.009042858532916712
Face;0.008191465524322736
Transformer's;0.30790249957258786
library,;0.012625538942483019
which;0.00768648951127126
has;0.006794817466091076
built;0.006661548608264358
in;0.006779040806816971
methods;0.007393776531228143
for;0.006589058585517303
pre-training,;0.010248087709856045
inference,;0.008294078931401579
and;0.006469015547909365
deploying;0.007564027633847757
BERT.;0.020338195334530504
â€˜**;0.015706015484059834
from;0.007785673288536033
transformers;0.008442763029467575
import;0.007042831506514351
AutoTokenizer,;0.009715198652597543
BertModel;0.008263417965010988
import;0.008014182080990962
torch;0.007441813672463963
tokenizer;0.009276692541043094
=;0.006872579341813832
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06945272801096215
model;0.0070664366834405305
=;0.006414851067383908
"BertModel.from_pretrained(""bert-base-uncased"")";0.08283802253582473
inputs;0.0071874390383273425
=;0.006581080959796008
"tokenizer(""Hello,";0.010175978665369137
my;0.006615291893801802
dog;0.006540299225462231
is;0.0064424902262820525
"cute"",";0.007997904079588083
"return_tensors=""pt"")";0.010283006251515082
outputs;0.006513597503582555
=;0.006289541128824576
model(**inputs);0.009254011901937783
last_hidden_states;0.007982055178174475
=;0.00621258986503494
outputs.last_hidden_state;0.0073912100927695235
***;0.006048223668686005
