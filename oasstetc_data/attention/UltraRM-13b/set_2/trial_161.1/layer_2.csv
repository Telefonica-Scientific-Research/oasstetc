text;attention
The;2.5409621484271756e-15
easiest;6.491645309894255e-20
way;6.180831672735677e-20
to;5.566242481963314e-20
import;6.301331105703241e-20
the;0.9978869956983166
BERT;9.435982869460394e-20
language;6.568295083751795e-20
model;5.966384712538783e-20
into;6.160452871453114e-20
python;9.603447803239438e-20
for;6.049028957581366e-20
use;6.313292358606211e-20
with;5.890135441010145e-20
PyTorch;1.3730943964408203e-19
is;6.368238023159218e-20
using;5.973179935341603e-20
the;5.730132355241494e-20
Hugging;9.525878934895079e-20
Face;6.445552559600949e-20
Transformer's;9.336859282309423e-20
library,;0.002113004301680958
which;6.156788838502073e-20
has;5.812693260758227e-20
built;6.257597083025639e-20
in;5.729278263791553e-20
methods;6.378539038484242e-20
for;5.646963038943111e-20
pre-training,;9.383762164490161e-20
inference,;6.91725002290044e-20
and;5.872598850107589e-20
deploying;6.402477372907018e-20
BERT.;7.85475226729562e-20
â€˜**;8.441198139790436e-20
from;5.983046882326359e-20
transformers;7.083577124244634e-20
import;6.612974693573104e-20
AutoTokenizer,;1.0057256852958864e-19
BertModel;7.195421384973233e-20
import;6.021424592849182e-20
torch;7.204280644196667e-20
tokenizer;6.891476978439261e-20
=;6.207551098719172e-20
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";5.43091596729838e-19
model;5.667505252507876e-20
=;5.97408040122742e-20
"BertModel.from_pretrained(""bert-base-uncased"")";3.141307334784196e-19
inputs;6.072013875929955e-20
=;5.811602349525716e-20
"tokenizer(""Hello,";1.049052210718732e-19
my;5.749226310099488e-20
dog;5.982729322973595e-20
is;5.553392720627358e-20
"cute"",";6.772468868342341e-20
"return_tensors=""pt"")";1.11470013383128e-19
outputs;5.987684972590417e-20
=;5.694997256829145e-20
model(**inputs);9.213630415412593e-20
last_hidden_states;8.291418006717485e-20
=;5.485369920858077e-20
outputs.last_hidden_state;8.3785763380598e-20
***;5.3232147080541236e-20
