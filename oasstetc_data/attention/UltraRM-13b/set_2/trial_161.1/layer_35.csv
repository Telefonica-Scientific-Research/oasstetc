text;attention
The;0.218444048340612
easiest;0.016492396680448346
way;0.012167889865037054
to;0.007750975049783346
import;0.00943077304724943
the;0.006225108039166656
BERT;0.021488770586809404
language;0.0064709799556604865
model;0.010647964120210174
into;0.0075273131940841375
python;0.016762226040508275
for;0.006586715085898027
use;0.007961027913954612
with;0.005535377593712502
PyTorch;0.038372060977090526
is;0.013798886231954032
using;0.006313602855190024
the;0.0054307588524124045
Hugging;0.015153913952930838
Face;0.006677826049270618
Transformer's;0.07880415536325913
library,;0.012050039039119997
which;0.005811907571592425
has;0.00489288350886543
built;0.004954957270878203
in;0.00466398702440152
methods;0.00524981715313089
for;0.004439868099601114
pre-training,;0.012969619102715841
inference,;0.006283153574234333
and;0.004331789391736538
deploying;0.005835372494360945
BERT.;0.022363118373560555
â€˜**;0.01600762922886356
from;0.005594262143920064
transformers;0.007743061898754559
import;0.005231452851381496
AutoTokenizer,;0.012839307728911896
BertModel;0.007872970152577622
import;0.005315185526190868
torch;0.0062895040816772696
tokenizer;0.006953371257167091
=;0.0048066879232118226
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.16320297056116656
model;0.004534616466870978
=;0.004244872732339321
"BertModel.from_pretrained(""bert-base-uncased"")";0.06630340728592883
inputs;0.004400754032448538
=;0.004052700083430576
"tokenizer(""Hello,";0.0074733734107376125
my;0.004050567068469509
dog;0.004180663547533292
is;0.003980535539852275
"cute"",";0.005029655314982964
"return_tensors=""pt"")";0.007246270566601714
outputs;0.003989548944422185
=;0.00389126282440111
model(**inputs);0.005972535150140438
last_hidden_states;0.004994499167666138
=;0.0037747515321884906
outputs.last_hidden_state;0.004494588593902388
***;0.003639711984820843
