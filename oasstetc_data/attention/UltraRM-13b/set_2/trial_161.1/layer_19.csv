text;attention
The;0.010494364780537311
easiest;0.010302646782010812
way;0.008875328199803714
to;0.008059759453438642
import;0.011539113759460439
the;0.007958494768965726
BERT;0.04212289844917553
language;0.008053177792166421
model;0.013258828102806272
into;0.009768662382726768
python;0.012852415095893938
for;0.008122049071316152
use;0.007863023203097813
with;0.007459769306762169
PyTorch;0.018900320635848438
is;0.01298865258976388
using;0.0083285766127881
the;0.00784573222638541
Hugging;0.010212126121262696
Face;0.011108293131478587
Transformer's;0.2217232007012428
library,;0.009687950890035693
which;0.00757473869655261
has;0.007359259678731982
built;0.007349871710040191
in;0.0073309617606151485
methods;0.007758984910409966
for;0.007362973096215799
pre-training,;0.010332201258758805
inference,;0.00885002478785193
and;0.0071515066613922216
deploying;0.007690104994064199
BERT.;0.024100460774903715
â€˜**;0.01811804558041893
from;0.008582491941708794
transformers;0.010306399451552786
import;0.008137585627379944
AutoTokenizer,;0.013645737860935505
BertModel;0.010923336826128173
import;0.008875838974413219
torch;0.008535670216412791
tokenizer;0.009311153580079804
=;0.008373157527333001
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07937550439034853
model;0.008174903412979507
=;0.007578216924294968
"BertModel.from_pretrained(""bert-base-uncased"")";0.09916891919298886
inputs;0.008306078191079789
=;0.007372356871541259
"tokenizer(""Hello,";0.012676210241536166
my;0.007273084255054593
dog;0.007806517639257774
is;0.007228369177459999
"cute"",";0.008721187068332437
"return_tensors=""pt"")";0.012525149712534238
outputs;0.007557971293484608
=;0.007163081012669312
model(**inputs);0.010806085855289936
last_hidden_states;0.010095675533564028
=;0.007157748022278998
outputs.last_hidden_state;0.008756818689380585
***;0.0070602325430577486
