text;attention
The;0.11569290253500251
easiest;0.010291441274952316
way;0.01020251849905996
to;0.009883639438256034
import;0.0150307315423357
the;0.009628345495598498
BERT;0.022025875386045207
language;0.012285382423634585
model;0.01175755339664856
into;0.012237617551334668
python;0.01333294385509094
for;0.011117257258211988
use;0.010082150534386852
with;0.010077284821746564
PyTorch;0.01861693011282408
is;0.012135153451028896
using;0.010679359177498599
the;0.009727945031378298
Hugging;0.01177174931478487
Face;0.01482095385196987
Transformer's;0.017063951394571324
library,;0.01225230828699154
which;0.010274957387947396
has;0.009745287843223639
built;0.009341574734582407
in;0.009986061080293844
methods;0.010487261423101359
for;0.009980076046997927
pre-training,;0.015220216756133971
inference,;0.010885458293989264
and;0.009664622006133902
deploying;0.010680083902206707
BERT.;0.014163183560634464
â€˜**;0.01302662353182977
from;0.014254609546246027
transformers;0.0164340281154447
import;0.012706758984359209
AutoTokenizer,;0.021799277903415574
BertModel;0.013146422471377593
import;0.012513143116784631
torch;0.011457704363642343
tokenizer;0.013861407324826368
=;0.012354769068003025
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.11079542338138733
model;0.011831149328380537
=;0.011403332363069377
"BertModel.from_pretrained(""bert-base-uncased"")";0.03428932613540577
inputs;0.012078516205291804
=;0.011315828353628883
"tokenizer(""Hello,";0.02187337617704011
my;0.009674058308626794
dog;0.010976722307301965
is;0.009576580515158728
"cute"",";0.011685148345402395
"return_tensors=""pt"")";0.019948232415304207
outputs;0.010845626772778443
=;0.01005906616152161
model(**inputs);0.015770231680954323
last_hidden_states;0.014698787773324364
=;0.00973069541220278
outputs.last_hidden_state;0.011499718152354763
***;0.009250658116340155
