text;attention
The;0.024706604470488114
easiest;0.012746828801269853
way;0.011724132621670404
to;0.011668241053798701
import;0.014341601402023387
the;0.011106589868573864
BERT;0.020825911709711993
language;0.010251711466414888
model;0.014304801845721471
into;0.011011461449443549
python;0.015643040367605974
for;0.010778694507852673
use;0.010838189629625725
with;0.009535980766120682
PyTorch;0.019953592906805574
is;0.025425939020006883
using;0.010106984214205285
the;0.00949676599221996
Hugging;0.011775979245726013
Face;0.010312026330392332
Transformer's;0.18157644868335865
library,;0.016810251636017823
which;0.010718428506500073
has;0.009658759290762072
built;0.009726805923492711
in;0.00969943534878624
methods;0.010865653801876528
for;0.009223461455082835
pre-training,;0.013850561768834313
inference,;0.011291118096225856
and;0.00915840481501071
deploying;0.010860415144017045
BERT.;0.02570889596155343
â€˜**;0.026143801540653072
from;0.011843728133708648
transformers;0.011378583282631841
import;0.009787173091302746
AutoTokenizer,;0.013291464558010902
BertModel;0.011580109388305978
import;0.010821074297037771
torch;0.010275844589101885
tokenizer;0.012384753159507598
=;0.01014913637182045
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04621739504435393
model;0.009733412679248567
=;0.009190133171616405
"BertModel.from_pretrained(""bert-base-uncased"")";0.05167486014648055
inputs;0.009851325499885143
=;0.00922286515531867
"tokenizer(""Hello,";0.012439070115552849
my;0.009311899061178122
dog;0.009452686318686574
is;0.009130236453259014
"cute"",";0.01054717340281701
"return_tensors=""pt"")";0.011951818967994353
outputs;0.009286831797503172
=;0.00905758396012083
model(**inputs);0.01172281252950109
last_hidden_states;0.010300860713482187
=;0.008923110994899947
outputs.last_hidden_state;0.009807135525675532
***;0.008819401949149383
