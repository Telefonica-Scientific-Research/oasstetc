text;attention
The;0.06018282560404476
easiest;0.022757308368281753
way;0.017583370865157787
to;0.013638929390572985
import;0.018581393888666087
the;0.013356592161455436
BERT;0.03085171686538846
language;0.012988521806201941
model;0.01741279184681565
into;0.015075849495132852
python;0.02193447393412208
for;0.012479739378046407
use;0.013994352861714528
with;0.01160481110594605
PyTorch;0.03467000178150567
is;0.0174674920400348
using;0.012634325324869326
the;0.011523781414770423
Hugging;0.018319277302278097
Face;0.01212963581458122
Transformer's;0.05516124062660521
library,;0.015488086642700365
which;0.01118489490820464
has;0.010521379155378758
built;0.010964264492784917
in;0.010410541130399315
methods;0.011085773151168052
for;0.01011429350170339
pre-training,;0.01645179437000013
inference,;0.011829021404482712
and;0.01000103480562838
deploying;0.011573694636886315
BERT.;0.02197993274009399
â€˜**;0.016878635979860876
from;0.0115862218570879
transformers;0.014131655831449997
import;0.01121197033000699
AutoTokenizer,;0.01799960028765082
BertModel;0.013911043839992199
import;0.011157918621891713
torch;0.011960614852460846
tokenizer;0.012837759329010358
=;0.010503413169025906
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.05737152670722737
model;0.010370636678942604
=;0.009919329516542423
"BertModel.from_pretrained(""bert-base-uncased"")";0.03774803024178287
inputs;0.01025051281530839
=;0.00972166986188464
"tokenizer(""Hello,";0.013062033526309889
my;0.009752818771192169
dog;0.009880868924041202
is;0.009662429476909767
"cute"",";0.010775324172064737
"return_tensors=""pt"")";0.01288323792024711
outputs;0.00969316526997318
=;0.009539965724464063
model(**inputs);0.011658973671320157
last_hidden_states;0.010763326787582248
=;0.009403352919508048
outputs.last_hidden_state;0.010148147511491998
***;0.009262672589147033
