text;attention
The;0.012157326808447031
easiest;0.011222126616554558
way;0.00977216046489523
to;0.009355242494801975
import;0.0134503616998789
the;0.009347291763171303
BERT;0.02973134174670286
language;0.009713116730214593
model;0.013096891875031049
into;0.01061902281221546
python;0.01321972196539402
for;0.009684386513294094
use;0.009165610836492732
with;0.008857939196654391
PyTorch;0.018753486401687686
is;0.011957495499655357
using;0.009863964109302476
the;0.009142394671879316
Hugging;0.01250887491944678
Face;0.011164045805065418
Transformer's;0.2167804081836907
library,;0.011590359519864668
which;0.00909796659222634
has;0.008622710429123728
built;0.008436964544909149
in;0.008497277619859892
methods;0.0088951174923058
for;0.008727354431303456
pre-training,;0.01129019155601052
inference,;0.009942219113531776
and;0.008152955315528532
deploying;0.008949739902978216
BERT.;0.02041965334220067
â€˜**;0.013952324225025839
from;0.009602849267834446
transformers;0.010298483104068558
import;0.009563807771364393
AutoTokenizer,;0.014343690134326278
BertModel;0.011521005174673147
import;0.009757271170291999
torch;0.009569025198646138
tokenizer;0.010515907767743443
=;0.009138055749541582
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06757956732868559
model;0.00925057295623827
=;0.008660683484841517
"BertModel.from_pretrained(""bert-base-uncased"")";0.07836051714695767
inputs;0.00943214469761379
=;0.008449826249302819
"tokenizer(""Hello,";0.012363278624033663
my;0.00827507761352375
dog;0.008369075726499711
is;0.008163319460398204
"cute"",";0.009646374121110673
"return_tensors=""pt"")";0.013513581720885896
outputs;0.008741681896467365
=;0.008129872830353078
model(**inputs);0.013220142642540823
last_hidden_states;0.010750972281834355
=;0.008148912694841831
outputs.last_hidden_state;0.01051569289611032
***;0.007980565089926131
