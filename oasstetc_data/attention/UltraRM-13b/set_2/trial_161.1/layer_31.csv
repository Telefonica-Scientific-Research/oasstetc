text;attention
The;0.05261975712475333
easiest;0.016559497368658183
way;0.013799943991234711
to;0.012315139359914922
import;0.013862060172681207
the;0.011508481189026476
BERT;0.019357973185725977
language;0.010435680759372758
model;0.01454093085187134
into;0.01137502062794001
python;0.01605308252637644
for;0.010887062286083774
use;0.011544592413038315
with;0.009831245967922168
PyTorch;0.022460490847628058
is;0.014864397667140625
using;0.010230333729959955
the;0.00976289915669746
Hugging;0.015283139514157256
Face;0.010217829853269467
Transformer's;0.10645249547457632
library,;0.015307786567038998
which;0.010516144101252328
has;0.00965114005382315
built;0.010053061511783382
in;0.00944475682018353
methods;0.010290726305450882
for;0.00913384160879429
pre-training,;0.01561634701038085
inference,;0.01126141735724071
and;0.009004172323486532
deploying;0.010995362324959826
BERT.;0.018915428889280782
â€˜**;0.018578781051036365
from;0.010490276137059027
transformers;0.013661802987799804
import;0.009899213740962132
AutoTokenizer,;0.016763077440847988
BertModel;0.012066068391053784
import;0.010039990030288045
torch;0.011055317647076075
tokenizer;0.011977849296160574
=;0.00964854630003561
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09323737783752085
model;0.009462029024304208
=;0.008974794218328808
"BertModel.from_pretrained(""bert-base-uncased"")";0.06351194883004212
inputs;0.00918266210673405
=;0.00876366438906281
"tokenizer(""Hello,";0.013347980233002512
my;0.008856947212383842
dog;0.00891532205638246
is;0.00869278184919623
"cute"",";0.01034993471689635
"return_tensors=""pt"")";0.013201730524821586
outputs;0.008693128161349623
=;0.008529649261370007
model(**inputs);0.011626546992315447
last_hidden_states;0.010226741144987487
=;0.008367834488306941
outputs.last_hidden_state;0.009565649013634818
***;0.008160115975336295
