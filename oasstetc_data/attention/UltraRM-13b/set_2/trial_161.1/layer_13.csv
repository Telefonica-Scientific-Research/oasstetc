text;attention
The;0.009228344851371324
easiest;0.008209745270288086
way;0.006887089009978998
to;0.006583829689115833
import;0.010316271692287224
the;0.006818575517409984
BERT;0.028632722148857627
language;0.007850285940759885
model;0.015767539391776213
into;0.013190331738569828
python;0.015285383927254344
for;0.00852111298061291
use;0.007341341876139823
with;0.006614086829137827
PyTorch;0.03863439101969464
is;0.012025420324441333
using;0.0075122560789575845
the;0.007979759258092214
Hugging;0.008940352613122697
Face;0.014087271665983477
Transformer's;0.07529519119980224
library,;0.009670232510029495
which;0.006294092516340026
has;0.0063925772287044295
built;0.006085101883263539
in;0.006896437975938743
methods;0.006625016620495654
for;0.0064324060269522814
pre-training,;0.013970909082483824
inference,;0.010271516571005373
and;0.006118581938741276
deploying;0.007577124305861286
BERT.;0.017189650615323168
â€˜**;0.013732584411015245
from;0.008402567711821284
transformers;0.010154580042272801
import;0.010052912696624084
AutoTokenizer,;0.032974170932921903
BertModel;0.01635440473113534
import;0.010345108194518006
torch;0.010913776562314155
tokenizer;0.013952044129505395
=;0.011159666604899565
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.23445444187456618
model;0.008011087368047643
=;0.00716113702640574
"BertModel.from_pretrained(""bert-base-uncased"")";0.06089828912201196
inputs;0.007884240309074218
=;0.006875208448558535
"tokenizer(""Hello,";0.01659257727166444
my;0.006115079171455667
dog;0.006974353944080706
is;0.006242481543273483
"cute"",";0.00781625178506929
"return_tensors=""pt"")";0.017303437816767773
outputs;0.007241725115130401
=;0.006765476850440967
model(**inputs);0.013682596056363542
last_hidden_states;0.009164478606334462
=;0.0063319660456932336
outputs.last_hidden_state;0.007477647522539289
***;0.005720757806701225
