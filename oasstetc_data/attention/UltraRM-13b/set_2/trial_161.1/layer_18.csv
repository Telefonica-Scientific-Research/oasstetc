text;attention
The;0.00785470462044545
easiest;0.008062744061678846
way;0.007329733895385255
to;0.006924538990907566
import;0.009454428830665076
the;0.006887320624257007
BERT;0.025659578702214973
language;0.007075166172132994
model;0.010348053704816349
into;0.008354043620924562
python;0.010165827588875756
for;0.007031302357364286
use;0.00675235428975555
with;0.00660080878420745
PyTorch;0.01711542349931032
is;0.00880126263151696
using;0.007405405964422838
the;0.007100164239658959
Hugging;0.009502223903811398
Face;0.011045776477373068
Transformer's;0.36425621935672803
library,;0.008207192216877317
which;0.006656333535377408
has;0.0064134199243297005
built;0.006322772264608945
in;0.006434571012275361
methods;0.006777027355149671
for;0.0064384466022425185
pre-training,;0.009257097028482944
inference,;0.007749421287183846
and;0.006306488747952014
deploying;0.006743975881235615
BERT.;0.013769133247536699
â€˜**;0.011743975452353147
from;0.0075538730537703805
transformers;0.00887818538033963
import;0.007811511370517352
AutoTokenizer,;0.011692102648974521
BertModel;0.009053893725925476
import;0.0074418998855771525
torch;0.007498066446826061
tokenizer;0.008202065245916082
=;0.0074542218542729384
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.045797061834894695
model;0.007155805880265187
=;0.006668213745613422
"BertModel.from_pretrained(""bert-base-uncased"")";0.09000073454630513
inputs;0.007248538725136932
=;0.006647322838183851
"tokenizer(""Hello,";0.012338730454445506
my;0.006463821604057645
dog;0.007055542780669382
is;0.006377994668482282
"cute"",";0.007291185988239566
"return_tensors=""pt"")";0.011737387565189548
outputs;0.006705775792925749
=;0.006504732431753248
model(**inputs);0.01011041593338867
last_hidden_states;0.008928702730409384
=;0.006425956652138854
outputs.last_hidden_state;0.008204624449217985
***;0.006204694894505451
