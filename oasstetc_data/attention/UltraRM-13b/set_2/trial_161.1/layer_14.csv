text;attention
The;0.00711535178861089
easiest;0.006341538961742938
way;0.005355950988066141
to;0.004894781230288385
import;0.007402677683163375
the;0.005059776534706434
BERT;0.03182330678682458
language;0.006098541167486739
model;0.01570174857341853
into;0.010930924045702216
python;0.01323753547216691
for;0.006639067889660687
use;0.005602597516587234
with;0.004907497013332628
PyTorch;0.023962884688175205
is;0.013025037817908886
using;0.0058159578011594895
the;0.005797911450633683
Hugging;0.0071054358492071805
Face;0.025424173366913657
Transformer's;0.08041829073042513
library,;0.008650815039148243
which;0.005174115997808891
has;0.005036357992405964
built;0.004900971809359749
in;0.0052356340524124445
methods;0.005092622581698431
for;0.00491061081497525
pre-training,;0.011890280258096601
inference,;0.008601202510515018
and;0.004702659619795158
deploying;0.005704120889175953
BERT.;0.019373182185795226
â€˜**;0.015447159248018205
from;0.0067704062269955245
transformers;0.008068457546081228
import;0.008328492697588856
AutoTokenizer,;0.029080299038804724
BertModel;0.012394600192143931
import;0.008805085454309212
torch;0.008415073922887343
tokenizer;0.010444004273635737
=;0.007460912815555945
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.29637863432662137
model;0.005868051329714672
=;0.005384683443235927
"BertModel.from_pretrained(""bert-base-uncased"")";0.07343214482188251
inputs;0.007693457908295568
=;0.005487235142454582
"tokenizer(""Hello,";0.019554534258957652
my;0.0046610962034870015
dog;0.005431525979152172
is;0.004746158614932538
"cute"",";0.006412706170020197
"return_tensors=""pt"")";0.018526229566913587
outputs;0.005690571534749306
=;0.005025723885569991
model(**inputs);0.01549382332428766
last_hidden_states;0.007658946099943824
=;0.004877418881405676
outputs.last_hidden_state;0.006043750961233438
***;0.00448525502375376
