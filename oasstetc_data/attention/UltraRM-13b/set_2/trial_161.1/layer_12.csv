text;attention
The;0.011432961919117158
easiest;0.008403931492258904
way;0.006980139847397346
to;0.006238040034845936
import;0.009408552169821068
the;0.0062743381903962855
BERT;0.020278496374516085
language;0.0068601802434277905
model;0.012401272028147545
into;0.015454171224896628
python;0.01564436107659549
for;0.009127276273142151
use;0.0070656152832671
with;0.006326070296025939
PyTorch;0.026236783285348064
is;0.01808671180104629
using;0.008370747554194247
the;0.007536381515214167
Hugging;0.008536556037115624
Face;0.013816224845409153
Transformer's;0.06274915561755069
library,;0.010064624776764201
which;0.0064928963148730805
has;0.006108561856942269
built;0.005763233134644942
in;0.007179510839988651
methods;0.006591356146381349
for;0.0062615956347874665
pre-training,;0.014700607841990027
inference,;0.01150559534714644
and;0.006139066862782072
deploying;0.008333367450672424
BERT.;0.024286629420266954
â€˜**;0.012323846459727844
from;0.009194771921177921
transformers;0.010148405956591602
import;0.010295565103807556
AutoTokenizer,;0.02579759127038373
BertModel;0.012669402644354786
import;0.012047064979641413
torch;0.008936228254723887
tokenizer;0.01168719397381278
=;0.009497064889172702
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2635192369712596
model;0.00755852106455508
=;0.007028498141938517
"BertModel.from_pretrained(""bert-base-uncased"")";0.06488195802156124
inputs;0.007591390624538789
=;0.0067235359413814234
"tokenizer(""Hello,";0.015497835203186448
my;0.005679238853069137
dog;0.006094061933788139
is;0.0056423132458312385
"cute"",";0.007055852768371666
"return_tensors=""pt"")";0.02048069219855459
outputs;0.006959715789485667
=;0.006433383988652972
model(**inputs);0.01664269984493165
last_hidden_states;0.00916469164234038
=;0.006579215409182882
outputs.last_hidden_state;0.007890412661537954
***;0.005324597505464709
