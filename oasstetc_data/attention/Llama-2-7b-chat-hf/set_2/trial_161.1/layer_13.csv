text;attention
The;6.152200397837379e-12
easiest;4.004443038406079e-12
way;3.8911069960012795e-12
to;3.5086745397294877e-12
import;4.31922259865533e-12
the;3.629511791107111e-12
BERT;9.814738200789193e-12
language;4.008277412430296e-12
model;5.468118812429327e-12
into;4.892794197098554e-12
python;6.411697460872068e-12
for;4.46851629050373e-12
use;3.604761651321655e-12
with;3.5515260853237763e-12
PyTorch;9.477653883499931e-12
is;6.084647289944049e-12
using;4.751373972481946e-12
the;3.951170904291621e-12
Hugging;4.9453069732283245e-12
Face;7.109971487014946e-12
Transformer's;6.451105037793574e-11
library,;6.554526203409122e-12
which;3.761914724482481e-12
has;3.6827086196348785e-12
built;3.310082194296724e-12
in;4.494367450842596e-12
methods;4.007265031451934e-12
for;3.7395228276472605e-12
pre-training,;9.824143791810719e-12
inference,;5.500337204704318e-12
and;3.525571816068601e-12
deploying;4.143055733897057e-12
BERT.;0.9999999991213064
â€˜**;6.258463528635286e-12
from;5.004881334965116e-12
transformers;4.619576223852848e-12
import;6.812668787100093e-12
AutoTokenizer,;1.8620467780498876e-11
BertModel;7.943932112674307e-12
import;7.540125672993313e-12
torch;5.119091576090678e-12
tokenizer;8.394742273768975e-12
=;8.235300732594437e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.0915480802568775e-10
model;5.648214891923878e-12
=;4.577160773501959e-12
"BertModel.from_pretrained(""bert-base-uncased"")";6.665184864146364e-11
inputs;4.339684817373703e-12
=;4.1254693534386656e-12
"tokenizer(""Hello,";1.7290144213703253e-11
my;3.600392909426018e-12
dog;3.6327955218238998e-12
is;3.4285922453702884e-12
"cute"",";4.683969932570666e-12
"return_tensors=""pt"")";1.3556292162058411e-11
outputs;4.14618355098884e-12
=;3.80038711347661e-12
model(**inputs);1.054836883216036e-11
last_hidden_states;7.097443967426909e-12
=;3.679049892368149e-12
outputs.last_hidden_state;5.9087471065605506e-12
***;3.1745990589328075e-12
