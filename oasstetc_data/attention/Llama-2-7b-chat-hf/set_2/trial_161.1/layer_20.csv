text;attention
The;1.202113826559826e-15
easiest;1.1726955770962266e-15
way;1.0663932113702484e-15
to;9.723696970463639e-16
import;1.4421232382018447e-15
the;9.922503008168265e-16
BERT;4.04202575938897e-15
language;1.2004575603702198e-15
model;1.2748035324846143e-15
into;1.112368698720479e-15
python;1.1944047249076332e-15
for;1.024311954751937e-15
use;9.86789336462745e-16
with;1.006168047477363e-15
PyTorch;1.7502404339121176e-15
is;1.049014685392982e-15
using;1.0543669633780024e-15
the;1.0131498293515442e-15
Hugging;1.9761487941325072e-15
Face;1.1565513233509486e-15
Transformer's;1.9160544821065944e-14
library,;1.1445988433274136e-15
which;9.74441311139434e-16
has;1.0055028748983625e-15
built;9.96855642819522e-16
in;1.0381864386699427e-15
methods;1.0043551760351067e-15
for;1.0406286451988994e-15
pre-training,;1.4792975057854787e-15
inference,;1.161353879932519e-15
and;9.471609623615723e-16
deploying;1.102695670131203e-15
BERT.;0.999999999999897
â€˜**;1.224002497494511e-15
from;1.0419273086890085e-15
transformers;1.2285303627298001e-15
import;1.109987400291928e-15
AutoTokenizer,;2.1724220098995996e-15
BertModel;1.4575112708253268e-15
import;1.1234163089332083e-15
torch;1.1877376833795707e-15
tokenizer;1.459860161415478e-15
=;1.111180400296959e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";7.469599266079492e-15
model;1.0415629944202903e-15
=;9.796306328738303e-16
"BertModel.from_pretrained(""bert-base-uncased"")";4.870685066440463e-15
inputs;1.1396718261674935e-15
=;1.0176713124345709e-15
"tokenizer(""Hello,";1.5459197063807043e-15
my;9.422477519169652e-16
dog;1.009200203184041e-15
is;9.554743274586299e-16
"cute"",";1.0484729815271125e-15
"return_tensors=""pt"")";2.26131012909676e-15
outputs;1.0353937647268888e-15
=;9.753912858185738e-16
model(**inputs);1.3167313896991132e-15
last_hidden_states;1.582733846620459e-15
=;9.41186166041821e-16
outputs.last_hidden_state;1.3438602687270176e-15
***;9.301596008235637e-16
