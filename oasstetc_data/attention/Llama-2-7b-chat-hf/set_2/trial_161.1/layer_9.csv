text;attention
The;7.671282511719647e-13
easiest;6.088248785537078e-13
way;5.996865813355712e-13
to;5.338950273987798e-13
import;7.035544538220581e-13
the;5.494139163598576e-13
BERT;1.4691501500316816e-12
language;5.835630225166966e-13
model;1.056839577906642e-12
into;8.70257261892931e-13
python;8.543717866937806e-13
for;6.405970373299571e-13
use;5.739657845857598e-13
with;5.498534177686497e-13
PyTorch;9.902204049123e-13
is;7.550443683511889e-13
using;6.755024154575219e-13
the;5.121859541447169e-13
Hugging;6.719056759622539e-13
Face;8.748946331145566e-13
Transformer's;5.953488754199105e-12
library,;9.44960970395218e-13
which;5.403528880192455e-13
has;5.020603336876321e-13
built;4.676044430736523e-13
in;6.162747269201185e-13
methods;5.776952188136146e-13
for;5.745203222460555e-13
pre-training,;1.4549052077155576e-12
inference,;8.141450346205222e-13
and;5.015902206900551e-13
deploying;6.277219996685673e-13
BERT.;0.9999999999072964
â€˜**;9.752261727649668e-13
from;6.665336725718365e-13
transformers;8.468206170466504e-13
import;6.981156126114759e-13
AutoTokenizer,;1.882460313159667e-12
BertModel;1.0080893666106497e-12
import;7.626163189895532e-13
torch;6.878644540088987e-13
tokenizer;9.199448130336452e-13
=;7.373609059828072e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";3.308376006544851e-11
model;6.969024816539749e-13
=;6.517128576417908e-13
"BertModel.from_pretrained(""bert-base-uncased"")";7.036540425671547e-12
inputs;7.331658898339616e-13
=;6.215541400708733e-13
"tokenizer(""Hello,";1.9448158146010365e-12
my;5.518613168400856e-13
dog;6.381381915352981e-13
is;4.946737217133113e-13
"cute"",";6.710435605178739e-13
"return_tensors=""pt"")";1.9277999015825797e-12
outputs;6.941582496653099e-13
=;5.296617732163162e-13
model(**inputs);1.664496929644393e-12
last_hidden_states;1.3043001814053459e-12
=;5.425072663552304e-13
outputs.last_hidden_state;8.710792811086021e-13
***;4.444085611637786e-13
