text;attention
The;6.0283321263190235e-12
easiest;4.4040179939849115e-12
way;4.361931642006478e-12
to;3.80466220597521e-12
import;5.2973555877794936e-12
the;3.931960021815278e-12
BERT;2.1950581976664313e-11
language;4.786883703134508e-12
model;7.848523902307976e-12
into;5.72893622211985e-12
python;8.996316048017111e-12
for;4.658326378500874e-12
use;4.198706691250751e-12
with;3.969823468176862e-12
PyTorch;1.2500195222682393e-11
is;6.193108779655821e-12
using;5.435489418430925e-12
the;4.538862321483448e-12
Hugging;6.351384999420804e-12
Face;6.7884085860978636e-12
Transformer's;1.28343005361981e-10
library,;5.9168637585730974e-12
which;4.0461152968110005e-12
has;3.988461029676122e-12
built;3.5399166329456803e-12
in;4.215102906584148e-12
methods;4.284318180911653e-12
for;4.000682795681855e-12
pre-training,;9.614954615918278e-12
inference,;5.5711351390455915e-12
and;3.88482965509073e-12
deploying;4.252305260369875e-12
BERT.;0.9999999989184876
â€˜**;5.539809642082082e-12
from;4.894783247497958e-12
transformers;5.3393625583155e-12
import;6.219470241423815e-12
AutoTokenizer,;1.927508122597728e-11
BertModel;9.004103486465483e-12
import;6.376936873152219e-12
torch;6.0610585322280245e-12
tokenizer;7.645296870482375e-12
=;5.340777290989787e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.659586164076497e-10
model;4.4242501764692945e-12
=;4.466545243733895e-12
"BertModel.from_pretrained(""bert-base-uncased"")";1.0939746571235942e-10
inputs;4.833900816944791e-12
=;4.129917323030003e-12
"tokenizer(""Hello,";1.8632583688453573e-11
my;3.7735309618044454e-12
dog;4.088542031952834e-12
is;3.735278625387663e-12
"cute"",";4.791523978192154e-12
"return_tensors=""pt"")";1.813467411318763e-11
outputs;4.7519013265510456e-12
=;4.168619347221605e-12
model(**inputs);1.202773865087453e-11
last_hidden_states;9.473573516734224e-12
=;4.281476014899745e-12
outputs.last_hidden_state;7.876185528767472e-12
***;3.4381213078684177e-12
