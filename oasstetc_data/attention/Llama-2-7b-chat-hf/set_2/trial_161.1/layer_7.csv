text;attention
The;3.301109019300475e-15
easiest;2.792324631126973e-15
way;2.729291024746604e-15
to;2.705001246065721e-15
import;3.4193764505739036e-15
the;2.5932269569454717e-15
BERT;5.501712248198689e-15
language;3.74680310334817e-15
model;3.652112784975733e-15
into;3.704388895480435e-15
python;3.6947269981273986e-15
for;3.1475596029350228e-15
use;2.7057772798092586e-15
with;2.626605723025857e-15
PyTorch;4.582262971013315e-15
is;3.4939936806111896e-15
using;2.8780798706683333e-15
the;2.52251069902713e-15
Hugging;3.589472334811848e-15
Face;4.365764404263916e-15
Transformer's;1.1326380509282434e-14
library,;3.891974365654496e-15
which;2.7330082749649688e-15
has;2.5348034116817815e-15
built;2.468857420606541e-15
in;2.824032939645142e-15
methods;2.7078305488596155e-15
for;2.6842654104687173e-15
pre-training,;5.3328071689043216e-15
inference,;3.2220172652440195e-15
and;2.4384390404936633e-15
deploying;2.9639233206329513e-15
BERT.;0.9999999999997202
â€˜**;3.6619172983135975e-15
from;3.515655056777038e-15
transformers;3.2208314000531693e-15
import;3.523619396668702e-15
AutoTokenizer,;7.671036942952998e-15
BertModel;3.676418803116899e-15
import;3.694530784742443e-15
torch;2.856991279539177e-15
tokenizer;3.4008414197498522e-15
=;3.4442561268494714e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";5.844642217847585e-14
model;3.1827119449168892e-15
=;3.2012144631152596e-15
"BertModel.from_pretrained(""bert-base-uncased"")";1.2594212930588981e-14
inputs;3.3003650497978293e-15
=;2.887586198194504e-15
"tokenizer(""Hello,";6.01623545701153e-15
my;2.49231307154069e-15
dog;2.5276430753412975e-15
is;2.4331553855778752e-15
"cute"",";2.983848030245546e-15
"return_tensors=""pt"")";5.378014142608708e-15
outputs;2.9979619966026687e-15
=;2.7079251708395183e-15
model(**inputs);4.342431071065916e-15
last_hidden_states;4.6746159325820555e-15
=;2.674658721860585e-15
outputs.last_hidden_state;3.3258753567565763e-15
***;2.3114441673112173e-15
