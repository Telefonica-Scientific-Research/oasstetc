text;attention
The;1.690667247095213e-19
easiest;1.7472274830309162e-19
way;1.5883511037752897e-19
to;1.5145580867698587e-19
import;1.9151617374231641e-19
the;1.5636664220064063e-19
BERT;1.9431154311283264e-19
language;1.6556858490713357e-19
model;1.5351738506358303e-19
into;1.5989789072597993e-19
python;1.8923032754513051e-19
for;1.5517671420039772e-19
use;1.627565582124587e-19
with;1.5158626211220197e-19
PyTorch;2.444517366551061e-19
is;1.5464881127795872e-19
using;1.5822159296613903e-19
the;1.5986090414200061e-19
Hugging;2.1330036265284783e-19
Face;1.641387252094627e-19
Transformer's;2.3058107078865523e-19
library,;1.7939705874497202e-19
which;1.50603780079176e-19
has;1.51622441292362e-19
built;1.6102274130055994e-19
in;1.4794263303139708e-19
methods;1.6662130108122866e-19
for;1.4871542512379957e-19
pre-training,;2.2949966764030073e-19
inference,;1.7227014616599015e-19
and;1.4617474492374856e-19
deploying;1.6511282805589803e-19
BERT.;1.0
â€˜**;2.241507986040807e-19
from;1.5454565717454032e-19
transformers;1.7647140787625975e-19
import;1.7057384411125924e-19
AutoTokenizer,;2.335502832489506e-19
BertModel;1.738968862912198e-19
import;1.5525642623006833e-19
torch;2.0059929202225651e-19
tokenizer;1.6331060733024314e-19
=;1.5596271335719256e-19
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.0891527483885728e-18
model;1.4253685164868483e-19
=;1.5243622802677057e-19
"BertModel.from_pretrained(""bert-base-uncased"")";7.145324183786026e-19
inputs;1.4879501971206652e-19
=;1.4925185242391066e-19
"tokenizer(""Hello,";2.487631056513477e-19
my;1.451371355637326e-19
dog;1.5209724912405498e-19
is;1.4085931573940505e-19
"cute"",";1.73618986754273e-19
"return_tensors=""pt"")";2.8167922351110533e-19
outputs;1.46102598758838e-19
=;1.4775998589816417e-19
model(**inputs);2.4221443253456075e-19
last_hidden_states;2.0719648106256413e-19
=;1.419853073336171e-19
outputs.last_hidden_state;2.2132026442118476e-19
***;1.3485752033732816e-19
