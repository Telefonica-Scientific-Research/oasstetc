text;attention
The;2.5244059137917125e-16
easiest;2.2373244005988524e-16
way;2.1175865703082055e-16
to;2.0362863772671588e-16
import;2.573348312654789e-16
the;2.1428678737913388e-16
BERT;4.0100558478652343e-16
language;2.290902507142661e-16
model;2.2455473370838897e-16
into;2.2065632130023769e-16
python;2.283702585830262e-16
for;2.1053004135519572e-16
use;2.121764462950939e-16
with;2.1157759588130827e-16
PyTorch;3.2500513985529473e-16
is;2.475821192122106e-16
using;2.1937417918935666e-16
the;1.980027495731205e-16
Hugging;2.662999857308634e-16
Face;2.7879582987058567e-16
Transformer's;3.991466424372668e-16
library,;2.3929462962202523e-16
which;1.972990646552803e-16
has;1.9332390560270782e-16
built;1.9603271385305305e-16
in;2.1095619481163338e-16
methods;2.083496681795063e-16
for;2.1314502593922759e-16
pre-training,;3.2235791964527854e-16
inference,;2.3629331749012774e-16
and;1.8620168071764253e-16
deploying;2.1722680350701747e-16
BERT.;0.9999999999999811
â€˜**;3.571311586022527e-16
from;2.591605977963095e-16
transformers;2.4866508757837585e-16
import;2.429480029377961e-16
AutoTokenizer,;3.7952304716270413e-16
BertModel;2.560298017586244e-16
import;2.244652493569028e-16
torch;2.2158723662630888e-16
tokenizer;2.4498235929471034e-16
=;2.5454368936055395e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.9317100395893096e-15
model;2.195425740692294e-16
=;2.4113166130308583e-16
"BertModel.from_pretrained(""bert-base-uncased"")";1.1329080171051207e-15
inputs;2.1938020064106532e-16
=;2.2871567039485263e-16
"tokenizer(""Hello,";4.606942861714521e-16
my;1.9939154234357688e-16
dog;2.0637814450222165e-16
is;1.8965073647312923e-16
"cute"",";2.4286471986486655e-16
"return_tensors=""pt"")";4.345632597876733e-16
outputs;2.075388680455068e-16
=;2.0958610217620426e-16
model(**inputs);3.5653353553055856e-16
last_hidden_states;3.2568854073165186e-16
=;1.99309519396791e-16
outputs.last_hidden_state;2.9425282537734667e-16
***;1.7729660806578205e-16
