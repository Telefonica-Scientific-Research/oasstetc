text;attention
The;2.366820232064276e-14
easiest;2.3449306328911e-14
way;2.1742214232260845e-14
to;1.9896219435555076e-14
import;2.96581372223969e-14
the;2.0097772145868735e-14
BERT;1.203225096668316e-13
language;2.8245040439219554e-14
model;3.274266472707231e-14
into;2.31725250254588e-14
python;3.0929495335803594e-14
for;2.1764589397703917e-14
use;2.0515559530060626e-14
with;2.0354385659521372e-14
PyTorch;5.837951661707175e-14
is;2.1891896914652373e-14
using;2.373852732700899e-14
the;2.2210084591819085e-14
Hugging;3.189367661720414e-14
Face;3.187720575756296e-14
Transformer's;5.954342763002836e-13
library,;2.6795280391119062e-14
which;2.0361290483999227e-14
has;2.0012988218928925e-14
built;1.936429071218318e-14
in;2.2114369445170048e-14
methods;2.0964099053078266e-14
for;2.043420850402436e-14
pre-training,;3.958173269527859e-14
inference,;2.6002776024810103e-14
and;1.9795779331169135e-14
deploying;2.2110656944753078e-14
BERT.;0.9999999999972979
â€˜**;2.6718294469201557e-14
from;2.423155945234473e-14
transformers;3.6876195506604965e-14
import;2.3277384299101955e-14
AutoTokenizer,;5.682639678956291e-14
BertModel;3.9293807521551744e-14
import;2.4221142414563183e-14
torch;3.691696661229132e-14
tokenizer;4.3349001924743366e-14
=;2.720132588289955e-14
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";3.051543084528872e-13
model;2.480836569181717e-14
=;2.1820106077870197e-14
"BertModel.from_pretrained(""bert-base-uncased"")";1.343724949394117e-13
inputs;2.446134337945026e-14
=;2.237640967297285e-14
"tokenizer(""Hello,";4.221256094721588e-14
my;2.030049510179027e-14
dog;2.343942955375602e-14
is;1.9914215408026826e-14
"cute"",";2.2882541049410268e-14
"return_tensors=""pt"")";4.794826781123093e-14
outputs;2.199931491809996e-14
=;2.0959600191065824e-14
model(**inputs);3.3503405903352196e-14
last_hidden_states;3.0926990870745266e-14
=;2.0322563491230935e-14
outputs.last_hidden_state;2.732328700837951e-14
***;1.9187431497351267e-14
