text;attention
The;2.2026641222197306e-15
easiest;2.031538227484396e-15
way;2.1087135926026892e-15
to;1.935920989183653e-15
import;2.1028520094769524e-15
the;1.892116118994116e-15
BERT;3.56326777030205e-15
language;2.210960202706342e-15
model;2.3554905658622686e-15
into;2.115189656460096e-15
python;2.126418072056752e-15
for;2.0164925894422447e-15
use;1.9978219870499046e-15
with;1.9444175198459617e-15
PyTorch;2.7602095489204182e-15
is;1.9411145598203267e-15
using;2.003645928591356e-15
the;1.9493580425215242e-15
Hugging;3.089422735549773e-15
Face;2.0779376531296846e-15
Transformer's;3.635961942657619e-14
library,;2.2375095180905595e-15
which;1.981661517705904e-15
has;1.9504289330603115e-15
built;2.03412448919094e-15
in;2.0268944174037543e-15
methods;2.058539724120097e-15
for;1.968270771699265e-15
pre-training,;2.9464532826456677e-15
inference,;2.1901859364692524e-15
and;1.8944363170621076e-15
deploying;2.1191791373770324e-15
BERT.;0.9999999999998168
â€˜**;2.18004526420171e-15
from;2.0802466274284713e-15
transformers;2.3308509107430875e-15
import;2.063287119981382e-15
AutoTokenizer,;2.6849778013280632e-15
BertModel;2.2773959164151177e-15
import;2.00433658715099e-15
torch;2.2199244490449592e-15
tokenizer;2.2631861732575434e-15
=;2.0443706522505697e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.0873903682065958e-14
model;2.0099305274436736e-15
=;1.9005850884095106e-15
"BertModel.from_pretrained(""bert-base-uncased"")";7.204076739434675e-15
inputs;2.1540167241504024e-15
=;2.0228230592325865e-15
"tokenizer(""Hello,";3.0757777375043956e-15
my;1.910006691164171e-15
dog;1.930106904207137e-15
is;1.9400462827490137e-15
"cute"",";2.0643868213944503e-15
"return_tensors=""pt"")";3.893299398511295e-15
outputs;2.1016077853628597e-15
=;1.8892226716693502e-15
model(**inputs);2.5364982854317325e-15
last_hidden_states;3.0687352596155273e-15
=;1.8464998387960957e-15
outputs.last_hidden_state;2.606233277066195e-15
***;1.8433955957805904e-15
