text;attention
The;1.9072176868666574e-11
easiest;1.405287839253253e-11
way;1.2931048796763708e-11
to;1.2018241890746058e-11
import;1.7537395334826052e-11
the;1.3016774996015217e-11
BERT;6.71303158748639e-11
language;1.4630529628515976e-11
model;2.5810793102517516e-11
into;2.035002148803093e-11
python;2.4526877486392206e-11
for;1.6395529775344984e-11
use;1.3031837915039753e-11
with;1.2645901562450083e-11
PyTorch;3.6047756685014074e-11
is;1.7741452396569213e-11
using;1.6624643036389334e-11
the;1.3933305074225348e-11
Hugging;1.9092045232212132e-11
Face;4.458694596859077e-11
Transformer's;3.303333589711954e-10
library,;2.0025129079699327e-11
which;1.2437412132809796e-11
has;1.2342610327085364e-11
built;1.1091160763494397e-11
in;1.3187902232805218e-11
methods;1.3317859033649391e-11
for;1.254959268001703e-11
pre-training,;4.109823796250467e-11
inference,;2.1633920360614272e-11
and;1.1921104500486326e-11
deploying;1.4854417443776566e-11
BERT.;0.9999999962553343
â€˜**;1.644425392039181e-11
from;1.5472356927151134e-11
transformers;1.795387365209621e-11
import;1.9706766186542544e-11
AutoTokenizer,;1.9726056379028706e-10
BertModel;5.228674423882813e-11
import;2.0204697064517748e-11
torch;1.7907180043460842e-11
tokenizer;3.2754576714241206e-11
=;1.942699047143463e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.7875687633484992e-09
model;1.7016938046509383e-11
=;1.484591966683164e-11
"BertModel.from_pretrained(""bert-base-uncased"")";2.0423364818241446e-10
inputs;1.8304587272194103e-11
=;1.690285968150156e-11
"tokenizer(""Hello,";6.434895965320561e-11
my;1.1992969234276879e-11
dog;1.3343910013035485e-11
is;1.1687898783473843e-11
"cute"",";1.557737670667004e-11
"return_tensors=""pt"")";7.137276907830586e-11
outputs;1.624124260376186e-11
=;1.3769327415520961e-11
model(**inputs);3.4303167598807406e-11
last_hidden_states;3.132809045653085e-11
=;1.3772177646333567e-11
outputs.last_hidden_state;2.1732698771478796e-11
***;1.093501668492645e-11
