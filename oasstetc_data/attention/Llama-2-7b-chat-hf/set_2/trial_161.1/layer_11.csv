text;attention
The;1.179403903819516e-12
easiest;7.341152329284634e-13
way;6.855576369386946e-13
to;6.358238758966639e-13
import;8.115811739253204e-13
the;6.864608618316201e-13
BERT;2.55138142174825e-12
language;7.919423519034497e-13
model;1.646217061976332e-12
into;1.1628426007437121e-12
python;1.4025029318778953e-12
for;1.0654415401319411e-12
use;7.127048332332146e-13
with;6.984246213408451e-13
PyTorch;2.081884069284465e-12
is;1.2395305387667007e-12
using;9.369662119413861e-13
the;8.28122870223464e-13
Hugging;8.459406425600482e-13
Face;2.1082592496089525e-12
Transformer's;6.758036116300117e-12
library,;1.2305471244873217e-12
which;6.875110025573721e-13
has;6.523767672176659e-13
built;5.816056692800291e-13
in;7.419370716543707e-13
methods;6.995294005264231e-13
for;6.740546089993824e-13
pre-training,;1.841537193192853e-12
inference,;1.089316560263979e-12
and;6.392340166206729e-13
deploying;8.178586959723244e-13
BERT.;0.9999999998901927
â€˜**;1.1626559270348226e-12
from;7.945047024319587e-13
transformers;1.0674842374694533e-12
import;1.073398019288364e-12
AutoTokenizer,;4.1893033695814725e-12
BertModel;1.619644637238567e-12
import;1.0196614855448671e-12
torch;8.743334813541215e-13
tokenizer;1.1496735830263307e-12
=;9.522650530133543e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";3.5876161676060554e-11
model;9.173142812179408e-13
=;7.441532000601271e-13
"BertModel.from_pretrained(""bert-base-uncased"")";4.716425902599479e-12
inputs;7.833912881237124e-13
=;6.937419552815441e-13
"tokenizer(""Hello,";2.1657531112480027e-12
my;6.099793656110324e-13
dog;6.561741044364725e-13
is;6.101980048763771e-13
"cute"",";8.043484902105115e-13
"return_tensors=""pt"")";1.9432013274641723e-12
outputs;7.149998346221845e-13
=;6.449795775081974e-13
model(**inputs);1.596035217944276e-12
last_hidden_states;1.173289292074476e-12
=;6.447701601964486e-13
outputs.last_hidden_state;8.237695893782052e-13
***;5.666518647667287e-13
