text;attention
The;0.0011011497845018576
easiest;0.0002634582240185074
way;0.0002727984518242088
to;0.001348612772631875
import;0.00024303219844688605
the;0.0006588356163531145
BERT;0.0014648151501284032
language;8.202172608625483e-05
model;8.042918415582684e-05
into;0.00017165385723502967
python;0.00011475614162457944
for;0.0003485264286959884
use;8.377331230500594e-05
with;0.00022469372482305318
PyTorch;0.0013051340233641725
is;0.00014001938526813566
using;5.887076608778832e-05
the;0.000169544731683903
Hugging;0.0012135123549477178
Face;4.4253347278784856e-05
Transformer's;0.005535118783855642
library,;0.00045242503029902013
which;5.443403934584462e-05
has;7.231044747861977e-05
built;4.0486586566097165e-05
in;0.00010492542022734222
methods;3.474843447028169e-05
for;9.381413240859827e-05
pre-training,;0.003454183707646813
inference,;0.00025749349814087655
and;7.969065485294503e-05
deploying;0.00013516544127662884
BERT.;0.00036511443018993623
â€˜**;6.872291974644996e-05
from;5.256651711005428e-05
transformers;7.781760008908658e-05
import;4.014231627816214e-05
AutoTokenizer,;0.0005494819303007781
BertModel;5.431055038123272e-05
import;3.618995770446037e-05
torch;5.2788703728938916e-05
tokenizer;4.972513234773555e-05
=;4.321692524278591e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.9442629728889926
model;2.2094240430176313e-05
=;3.2494460818511745e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.033969834151695175
inputs;1.985122478450754e-05
=;2.5477292334735277e-05
"tokenizer(""Hello,";0.0001619777358540431
my;2.0169072061901364e-05
dog;1.818428248871392e-05
is;2.119078358644195e-05
"cute"",";3.464283932576137e-05
"return_tensors=""pt"")";0.00014229698729994455
outputs;1.760344343834295e-05
=;1.9631436519746533e-05
model(**inputs);5.3116780097734095e-05
last_hidden_states;2.8885734284457723e-05
=;1.6574905968765154e-05
outputs.last_hidden_state;2.4027480703207158e-05
***;1.4209918165754907e-05
