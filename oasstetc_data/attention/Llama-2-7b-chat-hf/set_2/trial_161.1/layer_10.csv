text;attention
The;2.0733386099262458e-11
easiest;1.4154124796955417e-11
way;1.2660068972349702e-11
to;1.1939790704983963e-11
import;1.867967204183851e-11
the;1.2800214544204381e-11
BERT;3.940936517775355e-11
language;1.3084078944240686e-11
model;2.341162222923454e-11
into;2.4451014927347416e-11
python;2.5247071672292893e-11
for;1.6602293838633867e-11
use;1.3771303035214436e-11
with;1.3827676811695436e-11
PyTorch;3.526250531359341e-11
is;1.9440873347851545e-11
using;2.105294015372725e-11
the;1.32777539509486e-11
Hugging;1.6012277085408095e-11
Face;2.315120865922908e-11
Transformer's;1.259708437877792e-10
library,;2.5394668277600116e-11
which;1.287298462109743e-11
has;1.2984343013353094e-11
built;1.0591968166327403e-11
in;1.4355717173210445e-11
methods;1.3302494073995962e-11
for;1.3501792894776021e-11
pre-training,;3.9577876243559863e-11
inference,;2.0152046640320772e-11
and;1.1502443221261738e-11
deploying;1.553571401056628e-11
BERT.;0.9999999973359011
â€˜**;3.2064990485884084e-11
from;1.868416847280983e-11
transformers;2.0469808196447696e-11
import;2.3227911318312702e-11
AutoTokenizer,;7.005518892396222e-11
BertModel;2.987407582801532e-11
import;2.4493933455879705e-11
torch;1.821867713932098e-11
tokenizer;2.452718013133708e-11
=;2.5148584207888507e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.0620904847971848e-09
model;1.9579486639984863e-11
=;1.706826732376676e-11
"BertModel.from_pretrained(""bert-base-uncased"")";2.2003656655724784e-10
inputs;1.8635018228063065e-11
=;1.7345664557879277e-11
"tokenizer(""Hello,";5.190449849451695e-11
my;1.1951233740885153e-11
dog;1.1668541096781036e-11
is;1.0901160396255412e-11
"cute"",";1.515265728916222e-11
"return_tensors=""pt"")";5.900268398071125e-11
outputs;1.7361665056524877e-11
=;1.3485529050266731e-11
model(**inputs);4.97121221085938e-11
last_hidden_states;3.062546393033282e-11
=;1.6338362367388905e-11
outputs.last_hidden_state;1.9553102750201505e-11
***;1.0210089173408837e-11
