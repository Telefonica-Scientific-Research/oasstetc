text;attention
The;1.1855039878099746e-17
easiest;1.4126603475140473e-17
way;1.1998975790637182e-17
to;1.1311447937946532e-17
import;1.6954017229525416e-17
the;1.115778659372431e-17
BERT;1.6171864608615034e-17
language;1.206710194319901e-17
model;1.2658225678713606e-17
into;1.1258321075275921e-17
python;1.1908605565634358e-17
for;1.0979796269006306e-17
use;1.1676507334059554e-17
with;1.1083788381397078e-17
PyTorch;1.6103541626643503e-17
is;1.1777920598038277e-17
using;1.153088050812188e-17
the;1.0498976400541758e-17
Hugging;1.5413049750847623e-17
Face;1.4031746226546848e-17
Transformer's;1.7804450147495907e-17
library,;1.2991119314321528e-17
which;1.0298216522326406e-17
has;1.0461937212483133e-17
built;1.0717492278562299e-17
in;1.249440405702976e-17
methods;1.1932601707209036e-17
for;1.1114275741243955e-17
pre-training,;1.5639519552267538e-17
inference,;1.1373726543912284e-17
and;1.0169697527750613e-17
deploying;1.130834823863875e-17
BERT.;0.9999999999999993
â€˜**;1.410244643387434e-17
from;1.2194013403062967e-17
transformers;1.3054707729942915e-17
import;1.2257357995880365e-17
AutoTokenizer,;1.6569456252296435e-17
BertModel;1.2440865414846828e-17
import;1.1432918703100036e-17
torch;1.1826816505788091e-17
tokenizer;1.2370530519693696e-17
=;1.1887628645821817e-17
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.0429701610603531e-16
model;1.0717771602889161e-17
=;1.1134442313808465e-17
"BertModel.from_pretrained(""bert-base-uncased"")";5.2359878520729884e-17
inputs;1.1712385694476834e-17
=;1.1144218232744267e-17
"tokenizer(""Hello,";2.091339611176821e-17
my;1.0206009657523685e-17
dog;1.1414530812017809e-17
is;1.0143910310268736e-17
"cute"",";1.1580395849961985e-17
"return_tensors=""pt"")";1.775029127168165e-17
outputs;1.073076542872133e-17
=;1.0464175870505685e-17
model(**inputs);1.7141473346601043e-17
last_hidden_states;1.5096107142606973e-17
=;1.0484033611222589e-17
outputs.last_hidden_state;1.4629123288126738e-17
***;9.57411442654167e-18
