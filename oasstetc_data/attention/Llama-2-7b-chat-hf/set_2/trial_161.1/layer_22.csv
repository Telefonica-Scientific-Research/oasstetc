text;attention
The;4.328906793072104e-15
easiest;3.90047878564219e-15
way;3.616469917119427e-15
to;3.5042070768468953e-15
import;4.555889289201486e-15
the;3.6562341995289766e-15
BERT;1.3030218487476785e-14
language;3.730948140213485e-15
model;4.017537591093139e-15
into;3.8906554218473056e-15
python;3.802258543054314e-15
for;3.6667278876660855e-15
use;3.539826902194288e-15
with;3.67187976189225e-15
PyTorch;5.431643057766192e-15
is;3.4760836235453665e-15
using;3.895072281110207e-15
the;3.673840405072056e-15
Hugging;6.760068934466923e-15
Face;4.538139056115351e-15
Transformer's;9.43350583362454e-14
library,;3.852729085416112e-15
which;3.3119684089657043e-15
has;3.361326638847485e-15
built;3.4685621766442237e-15
in;3.434930696759013e-15
methods;3.496978110761085e-15
for;3.434266540290298e-15
pre-training,;4.645551197387937e-15
inference,;4.368215354498529e-15
and;3.2774788616386743e-15
deploying;3.843341067308956e-15
BERT.;0.9999999999996214
â€˜**;4.1074980208713996e-15
from;4.314617508678669e-15
transformers;4.129554581034024e-15
import;4.014082236856235e-15
AutoTokenizer,;6.057918117039889e-15
BertModel;4.685797380254151e-15
import;3.925111773825974e-15
torch;3.9786797159266665e-15
tokenizer;4.62146077261072e-15
=;3.7883605681977185e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.238336002709958e-14
model;3.738452335407929e-15
=;3.4596156373085975e-15
"BertModel.from_pretrained(""bert-base-uncased"")";1.3164083409474146e-14
inputs;4.29167015198665e-15
=;3.587869718242476e-15
"tokenizer(""Hello,";6.131917035682713e-15
my;3.343254266783219e-15
dog;3.645331341945938e-15
is;3.5231703281657757e-15
"cute"",";3.793909878127881e-15
"return_tensors=""pt"")";9.269785078082055e-15
outputs;3.730015365021779e-15
=;3.412400625804529e-15
model(**inputs);4.969351200697638e-15
last_hidden_states;5.495979905769598e-15
=;3.4581637913425047e-15
outputs.last_hidden_state;4.559211447046925e-15
***;3.3201312830950144e-15
