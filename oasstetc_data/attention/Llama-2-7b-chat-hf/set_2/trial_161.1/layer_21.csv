text;attention
The;2.084320528856743e-15
easiest;1.6922464855754625e-15
way;1.8049633234915093e-15
to;1.5580625385656863e-15
import;1.928122571759763e-15
the;1.5669197030874522e-15
BERT;9.415147836537638e-15
language;1.9338855223215963e-15
model;2.024308050039049e-15
into;1.7060881346823606e-15
python;1.915721411558886e-15
for;1.659863593452598e-15
use;1.5805431427658478e-15
with;1.64401366054493e-15
PyTorch;4.052064598331083e-15
is;1.747491936985419e-15
using;1.6848146985119957e-15
the;1.6543024736327907e-15
Hugging;3.1934622076108547e-15
Face;2.05413247802785e-15
Transformer's;3.5564610222202186e-14
library,;2.040326873040265e-15
which;1.5627862294921227e-15
has;1.548319805091958e-15
built;1.6514422252750991e-15
in;1.6071609809815663e-15
methods;1.6595255650003437e-15
for;1.5509059133727584e-15
pre-training,;2.0558712131535553e-15
inference,;1.8494618069602104e-15
and;1.5272662544266762e-15
deploying;1.6694094930046163e-15
BERT.;0.9999999999998257
â€˜**;2.1055211747029794e-15
from;1.8993110244610643e-15
transformers;2.0714814569772404e-15
import;1.890987376803523e-15
AutoTokenizer,;3.785187871049111e-15
BertModel;2.738414931470792e-15
import;1.7773882090551704e-15
torch;2.2717744280173715e-15
tokenizer;2.5401998606447157e-15
=;1.800988006770465e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.2843520322674856e-14
model;1.890949787848229e-15
=;1.5987758476664115e-15
"BertModel.from_pretrained(""bert-base-uncased"")";5.5454199727713544e-15
inputs;1.841017247048156e-15
=;1.6476642657278298e-15
"tokenizer(""Hello,";2.543630698943876e-15
my;1.5327337429199774e-15
dog;1.6597648092157171e-15
is;1.543446307837747e-15
"cute"",";1.7134483083910182e-15
"return_tensors=""pt"")";3.2438491847495554e-15
outputs;1.6965540342034623e-15
=;1.5929405762914044e-15
model(**inputs);2.2071470939417014e-15
last_hidden_states;2.2671597852849523e-15
=;1.5545841992839028e-15
outputs.last_hidden_state;1.834442958392063e-15
***;1.5589578784482993e-15
