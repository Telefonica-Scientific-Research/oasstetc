text;attention
The;2.978713444382112e-13
easiest;2.3568327348849453e-13
way;2.6219293688599525e-13
to;2.2307164378386484e-13
import;2.392841462638821e-13
the;2.1078875382248816e-13
BERT;3.845202058430142e-13
language;2.2026461041757803e-13
model;2.3784396507134414e-13
into;2.3423840023723216e-13
python;2.502142474860443e-13
for;2.2528717783002106e-13
use;2.1843487132955572e-13
with;2.1401143293635617e-13
PyTorch;3.5920998056326695e-13
is;2.353265851091014e-13
using;2.5656842429664196e-13
the;2.375014004299299e-13
Hugging;6.960632114695708e-13
Face;2.458261118377828e-13
Transformer's;9.075705198412028e-12
library,;2.928121270923542e-13
which;2.1977290578151623e-13
has;2.179623469656198e-13
built;2.104675036725131e-13
in;2.2127741238907413e-13
methods;2.2205858070970763e-13
for;2.2300477107181837e-13
pre-training,;3.3246335823520025e-13
inference,;2.4560414799120744e-13
and;2.0715664458294167e-13
deploying;2.214363922746521e-13
BERT.;0.9999999999687681
â€˜**;3.120363582331629e-13
from;2.473189969115331e-13
transformers;3.180488877969233e-13
import;2.371504449990923e-13
AutoTokenizer,;3.9916868501751704e-13
BertModel;3.1037674202180717e-13
import;2.170323410909087e-13
torch;2.641445310566743e-13
tokenizer;2.64411097388777e-13
=;2.2165032007189816e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.868432226948207e-12
model;2.1677890857701013e-13
=;2.0525707383877428e-13
"BertModel.from_pretrained(""bert-base-uncased"")";1.4945437554741861e-12
inputs;2.3687912543329643e-13
=;2.136910458495148e-13
"tokenizer(""Hello,";6.306754718299855e-13
my;2.0390827047248298e-13
dog;2.1580229377988178e-13
is;2.0520758757503356e-13
"cute"",";2.5087011985900473e-13
"return_tensors=""pt"")";6.423267378062441e-13
outputs;2.290200855889958e-13
=;2.0180699728020754e-13
model(**inputs);3.951952716217711e-13
last_hidden_states;3.275818454458893e-13
=;2.0120198234816345e-13
outputs.last_hidden_state;3.2284528977847075e-13
***;2.0463203934181038e-13
