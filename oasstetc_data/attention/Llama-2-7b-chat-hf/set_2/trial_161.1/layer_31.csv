text;attention
The;1.9981516846586086e-07
easiest;6.486699989452856e-08
way;4.931369439574405e-08
to;3.4451692948879485e-08
import;4.790242511395784e-08
the;3.78767708294526e-08
BERT;2.1006978015215136e-07
language;4.5939972199639734e-08
model;4.605158936568479e-08
into;4.3453965214457365e-08
python;4.7818908961298774e-08
for;3.833808468306673e-08
use;3.1684612917422905e-08
with;3.408650753847781e-08
PyTorch;2.458659546011885e-07
is;4.071095125296175e-08
using;4.201389466563636e-08
the;4.183832831074277e-08
Hugging;3.2591577887395185e-07
Face;4.4748999049457495e-08
Transformer's;0.9998132022790448
library,;5.595734052027456e-08
which;3.0830994431308176e-08
has;3.272940517518993e-08
built;3.10050602058538e-08
in;3.116301168435216e-08
methods;3.414540798591209e-08
for;3.1719688529060076e-08
pre-training,;1.2959815654361116e-07
inference,;4.3266190771834394e-08
and;2.8177341682332497e-08
deploying;3.6296582365236726e-08
BERT.;6.187299031211093e-08
â€˜**;1.1032752233064826e-07
from;7.263363472841238e-08
transformers;1.2089494322328054e-07
import;5.2408983104613487e-08
AutoTokenizer,;2.1840098334403825e-07
BertModel;8.687456151274189e-08
import;3.922660944510395e-08
torch;5.280393065527701e-08
tokenizer;6.461522046391037e-08
=;3.4641158967799996e-08
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0001810213297222259
model;3.219348054482136e-08
=;2.8206153957379672e-08
"BertModel.from_pretrained(""bert-base-uncased"")";1.684397075830641e-06
inputs;3.748033393423357e-08
=;2.7082139423389047e-08
"tokenizer(""Hello,";1.5613863385269032e-07
my;2.6877247655463445e-08
dog;2.8491982231126994e-08
is;2.8559225723090762e-08
"cute"",";4.678798920480251e-08
"return_tensors=""pt"")";2.6943409071869587e-07
outputs;3.0584797626694045e-08
=;2.4409738028008862e-08
model(**inputs);8.664463706038555e-08
last_hidden_states;9.109072480319655e-08
=;2.385039024399623e-08
outputs.last_hidden_state;5.824391676462864e-08
***;2.3564877554742057e-08
