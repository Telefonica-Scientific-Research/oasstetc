text;attention
The;3.3470483389605043e-12
easiest;2.1339503072625755e-12
way;2.0252860520947572e-12
to;1.8537379298761555e-12
import;2.4374942411706523e-12
the;1.910052277152685e-12
BERT;5.626860463334826e-12
language;2.1569492118223977e-12
model;3.329856275330848e-12
into;3.211284781876064e-12
python;2.9829218877393354e-12
for;2.5756533203604875e-12
use;2.025677573399501e-12
with;1.874546962337836e-12
PyTorch;5.04111286421064e-12
is;3.5031283749579577e-12
using;2.857191479980401e-12
the;2.222724343342544e-12
Hugging;2.7567579402417516e-12
Face;3.625465974832599e-12
Transformer's;2.4315584899617643e-11
library,;3.4379851127662027e-12
which;1.948752206435308e-12
has;1.899419407374452e-12
built;1.6981531526991713e-12
in;2.1694301322427575e-12
methods;2.1545727272439234e-12
for;1.979406107809154e-12
pre-training,;4.889400182927641e-12
inference,;2.8755907901271235e-12
and;1.780596291386039e-12
deploying;2.0790778679596562e-12
BERT.;0.9999999996373834
â€˜**;2.8308304067041496e-12
from;2.5863036850676545e-12
transformers;2.5278670558284722e-12
import;3.232775637800697e-12
AutoTokenizer,;9.370818532042411e-12
BertModel;4.4215820175140185e-12
import;3.766931068179035e-12
torch;2.982154041212718e-12
tokenizer;3.946286192331565e-12
=;3.892064630786703e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.3636285936818234e-10
model;2.661691356746163e-12
=;2.3468184896469495e-12
"BertModel.from_pretrained(""bert-base-uncased"")";2.3191170963422044e-11
inputs;2.4944287910979307e-12
=;2.3567279097165237e-12
"tokenizer(""Hello,";8.79904158402884e-12
my;1.7543424042728951e-12
dog;1.8485584096831477e-12
is;1.7201627876278826e-12
"cute"",";2.8457716409085702e-12
"return_tensors=""pt"")";6.573759231798762e-12
outputs;2.323206516687563e-12
=;2.085627284118033e-12
model(**inputs);6.32262960510019e-12
last_hidden_states;4.099369691110584e-12
=;2.004052846288966e-12
outputs.last_hidden_state;2.911832723878936e-12
***;1.6311587540279725e-12
