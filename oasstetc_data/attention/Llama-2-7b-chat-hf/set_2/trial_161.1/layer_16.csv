text;attention
The;2.0751097203896675e-12
easiest;1.6529834035756589e-12
way;1.6693441425938711e-12
to;1.4502175967812214e-12
import;2.2747368376353966e-12
the;1.5010881740490805e-12
BERT;1.5661993924437663e-11
language;1.9227863684323108e-12
model;2.879238578133634e-12
into;1.880921511685758e-12
python;3.1875898692827793e-12
for;1.6988493324152032e-12
use;1.510735047126573e-12
with;1.4705916155502627e-12
PyTorch;4.7959435632059715e-12
is;1.959750392625269e-12
using;1.9109252353519665e-12
the;1.5624300535125256e-12
Hugging;2.046877267018613e-12
Face;4.609679676462705e-12
Transformer's;3.0604825916699646e-11
library,;2.0170460358511204e-12
which;1.4398084195712221e-12
has;1.3990613885437314e-12
built;1.3213293611435088e-12
in;1.5753380166792984e-12
methods;1.564522411676904e-12
for;1.4422681676076575e-12
pre-training,;3.221783596298338e-12
inference,;2.236173312468744e-12
and;1.351117876526644e-12
deploying;1.6604954771499091e-12
BERT.;0.9999999996909734
â€˜**;2.0647755841161906e-12
from;1.7786565429461233e-12
transformers;2.454655841053706e-12
import;2.064582126801317e-12
AutoTokenizer,;6.376004858090108e-12
BertModel;3.592562150212544e-12
import;2.105636304739379e-12
torch;2.1282182607611264e-12
tokenizer;3.255461944451722e-12
=;2.101078234418083e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.1308457095038809e-10
model;1.8082393650901133e-12
=;1.571857379607511e-12
"BertModel.from_pretrained(""bert-base-uncased"")";2.3355577901060853e-11
inputs;1.9384021814291486e-12
=;1.5794351991291154e-12
"tokenizer(""Hello,";4.577232323724659e-12
my;1.352704318798036e-12
dog;1.4582352214437786e-12
is;1.34075409534793e-12
"cute"",";1.7290086796046168e-12
"return_tensors=""pt"")";4.574921658986636e-12
outputs;1.6911329264932373e-12
=;1.47389035855451e-12
model(**inputs);3.657475277074701e-12
last_hidden_states;3.4133250750179786e-12
=;1.452469974682578e-12
outputs.last_hidden_state;2.2257714749989323e-12
***;1.2643624173045446e-12
