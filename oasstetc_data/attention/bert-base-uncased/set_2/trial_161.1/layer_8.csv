text;attention
The;0.000424550307167903
easiest;0.0005178400670630408
way;0.0004296909270123062
to;0.0004687840357156127
import;0.0007114021029171375
the;0.0006595531933815089
BERT;0.0005402678878281007
language;0.0005677639552675298
model;0.0005621681578498516
into;0.00045359623807365694
python;0.000790587517878653
for;0.0004264982881870263
use;0.0005219813500958973
with;0.000687533393902227
PyTorch;0.001369506172907326
is;0.000540764355946094
using;0.0006548135792101534
the;0.0005676486355954115
Hugging;0.0005092639784267666
Face;0.0004598985283249856
Transformer's;0.001464448636256847
library,;0.001069956518018672
which;0.0005464699582839255
has;0.0005859768292822895
built;0.0005724151096875375
in;0.0006059649982433674
methods;0.0006978917984478363
for;0.0005777067851863352
pre-training,;0.0016328200475200778
inference,;0.0008966709842759621
and;0.0004510836561413924
deploying;0.0008647669977701717
BERT.;0.0020723670503405628
â€˜**;0.0021286846385558714
from;0.0005992052622841844
transformers;0.0005560495599059212
import;0.0005220811063434523
AutoTokenizer,;0.001196247305874567
BertModel;0.0005978699332666595
import;0.0006166794683995512
torch;0.00045632135110617813
tokenizer;0.0006273946801677992
=;0.0005491629600000987
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4645549247706209
model;0.0005170162237975742
=;0.0005488256553396926
"BertModel.from_pretrained(""bert-base-uncased"")";0.4744924345020827
inputs;0.0005534561441420911
=;0.0005161309599973978
"tokenizer(""Hello,";0.007852553548500613
my;0.0005889727059382194
dog;0.0005090228538601306
is;0.0004078721653652862
"cute"",";0.0011936284416623692
"return_tensors=""pt"")";0.006379134360627533
outputs;0.00042173929209504327
=;0.00045985889215325156
model(**inputs);0.0021435711366348353
last_hidden_states;0.0014647548088883961
=;0.0005514716222414916
outputs.last_hidden_state;0.003864581966788313
***;0.0012277016411539837
