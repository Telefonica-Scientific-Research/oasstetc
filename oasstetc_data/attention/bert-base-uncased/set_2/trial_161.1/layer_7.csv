text;attention
The;5.904435582995687e-05
easiest;7.253694477580042e-05
way;6.866988197883152e-05
to;5.8816303907813446e-05
import;9.192753526361517e-05
the;6.141634850904741e-05
BERT;5.418611093961958e-05
language;6.625733387618363e-05
model;6.56343045667019e-05
into;7.161507336508221e-05
python;6.726598293519579e-05
for;7.55359428176246e-05
use;7.133662100067202e-05
with;8.53679708662361e-05
PyTorch;0.00015349371472546435
is;0.00012182778788267181
using;0.00012118716922390594
the;7.272822995097251e-05
Hugging;7.141681400140648e-05
Face;6.196122677242623e-05
Transformer's;0.00019188771750556466
library,;0.0002044178560750247
which;9.111991417632986e-05
has;7.928423823131054e-05
built;9.909427238207711e-05
in;6.261768339962401e-05
methods;8.927310036865406e-05
for;8.583264018213922e-05
pre-training,;0.00021258223008272032
inference,;0.00010699809962036768
and;9.939853829009277e-05
deploying;0.00011644085694245809
BERT.;0.0001614456568198645
â€˜**;0.0002564653878252338
from;8.207703045806135e-05
transformers;6.0474221312149566e-05
import;0.00010253034102805953
AutoTokenizer,;0.00033267346828695887
BertModel;8.975685105728737e-05
import;0.00010608511817950705
torch;6.373216905351128e-05
tokenizer;8.458028554569957e-05
=;8.468218775671239e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5621065810691315
model;6.732089354243676e-05
=;9.037078268338416e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.4264567954636427
inputs;7.543726304026045e-05
=;8.473839463436075e-05
"tokenizer(""Hello,";0.0008595577153932548
my;6.574116466467548e-05
dog;6.651448711003514e-05
is;6.887038742270188e-05
"cute"",";0.000416402029840924
"return_tensors=""pt"")";0.0027359707687753428
outputs;6.333848697928633e-05
=;7.357849463366511e-05
model(**inputs);0.0008599782157850369
last_hidden_states;0.0002160822786601054
=;0.00011061094304374676
outputs.last_hidden_state;0.0008907367894214139
***;0.000255698853826713
