text;attention
The;3.081787598819452e-08
easiest;6.22076666539144e-08
way;3.430178195797362e-08
to;2.9608423193098513e-08
import;4.994584523580845e-08
the;3.026241981773131e-08
BERT;4.29091021017079e-08
language;5.29497721446485e-08
model;4.476257968185584e-08
into;3.4766005631955996e-08
python;5.4110134714260714e-08
for;3.1943892986510776e-08
use;4.404553337339779e-08
with;3.006086314405545e-08
PyTorch;5.129861497551707e-07
is;3.716160207834006e-08
using;4.830186835689529e-08
the;2.8866798822623504e-08
Hugging;4.9981540359266334e-08
Face;3.347506275357484e-08
Transformer's;4.3802434188189044e-07
library,;1.0126735540281746e-07
which;4.0321318653404626e-08
has;3.9624130166795173e-08
built;4.484516812871449e-08
in;2.9298002993302917e-08
methods;6.573511706993528e-08
for;2.982944693330216e-08
pre-training,;5.537950216251954e-07
inference,;1.1186100455413065e-07
and;3.169416070136408e-08
deploying;1.4223546307956425e-07
BERT.;1.0842913591641754e-07
â€˜**;4.993578259489787e-07
from;2.8998119954806648e-08
transformers;7.188417269529479e-08
import;5.0886078440665173e-08
AutoTokenizer,;1.5632549520619194e-06
BertModel;2.613789116875481e-07
import;4.320343605258414e-08
torch;4.951494761258752e-08
tokenizer;1.7111884077640293e-07
=;5.435302737600096e-08
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.8390936120826626
model;3.8601864202736046e-08
=;5.002637255685247e-08
"BertModel.from_pretrained(""bert-base-uncased"")";0.16076147147829878
inputs;5.841183826044872e-08
=;5.299192221129322e-08
"tokenizer(""Hello,";4.417274030773478e-06
my;3.395581243144654e-08
dog;4.1239550959894317e-08
is;3.04814408064732e-08
"cute"",";2.703985749420028e-07
"return_tensors=""pt"")";0.00010574113211578247
outputs;4.5119449387797276e-08
=;5.342621336286774e-08
model(**inputs);6.587205305111273e-06
last_hidden_states;2.0680297026052227e-06
=;5.4761864469872007e-08
outputs.last_hidden_state;1.9417539818254615e-05
***;1.41468263988284e-07
