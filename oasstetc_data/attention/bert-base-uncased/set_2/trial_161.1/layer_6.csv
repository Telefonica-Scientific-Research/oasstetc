text;attention
The;7.597831010262837e-05
easiest;8.236919699804426e-05
way;0.00011868449719542973
to;8.08877536724122e-05
import;9.9695972604075e-05
the;8.608561946260683e-05
BERT;7.095029588132445e-05
language;9.286507681209524e-05
model;8.0582982785274e-05
into;7.860206463990355e-05
python;8.748496521399787e-05
for;0.00010192479794952795
use;8.621819625712265e-05
with;0.00011145519629532842
PyTorch;0.0002103001946957174
is;9.892742613384774e-05
using;0.00010716342057285786
the;0.00012863417326785365
Hugging;0.00011554833384915936
Face;8.819038147522823e-05
Transformer's;0.0002777667811855593
library,;0.00017975677152799814
which;9.56164948692463e-05
has;0.00011210292103679969
built;0.00013471937146606713
in;9.968017910740246e-05
methods;0.0001008844454372356
for;0.00013409759413639507
pre-training,;0.00044227718055388134
inference,;9.884676938528496e-05
and;9.80037277022804e-05
deploying;0.00017418626060619865
BERT.;0.00013952956209372554
â€˜**;0.0002956173634937013
from;0.00010152620842354603
transformers;9.737414887983663e-05
import;0.00011985038911186491
AutoTokenizer,;0.00032158871996976906
BertModel;0.00012320445316648187
import;9.320388794479176e-05
torch;0.00010227981717398798
tokenizer;0.00010949939899581405
=;0.00011447280755339396
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.728801962993868
model;7.765990359017818e-05
=;0.00012881090070888154
"BertModel.from_pretrained(""bert-base-uncased"")";0.2566434122354877
inputs;9.402713836873479e-05
=;0.00011161653127906101
"tokenizer(""Hello,";0.0021703572045475247
my;8.713799067113875e-05
dog;0.00010243030039034275
is;8.268405990971148e-05
"cute"",";0.00025929345329073455
"return_tensors=""pt"")";0.003200456009205247
outputs;8.631533959370532e-05
=;8.342998915754203e-05
model(**inputs);0.0008531815909394879
last_hidden_states;0.000495409368184553
=;0.00012148876483174018
outputs.last_hidden_state;0.0008459478691838303
***;0.0001857442471061571
