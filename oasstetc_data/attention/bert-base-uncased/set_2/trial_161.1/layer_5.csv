text;attention
The;9.305609716331071e-05
easiest;0.00015411150080652837
way;9.656942418917966e-05
to;8.301031195312058e-05
import;0.00011931930187407703
the;9.383351203063964e-05
BERT;8.569148460667157e-05
language;0.00010980213367578594
model;9.198449065713736e-05
into;0.00012007514366794758
python;0.00011064053548413968
for;0.00010879201749033076
use;0.00011149303419420942
with;0.00012912310727812264
PyTorch;0.0002288764563830323
is;0.00015472363070920467
using;0.00011574739919431716
the;0.0001038561944395494
Hugging;8.984148616576536e-05
Face;9.874216694804306e-05
Transformer's;0.00033510431358111726
library,;0.00023626726715746953
which;0.00014481119120096742
has;0.00012712000400272205
built;0.0001813917963040251
in;9.058798754705176e-05
methods;0.00011671399353195085
for;0.00011713025754033786
pre-training,;0.0003516905056520623
inference,;0.00013752527902575164
and;0.00017256073195304344
deploying;0.00019346856798456885
BERT.;0.00019780132792484063
â€˜**;0.0002549287290556269
from;9.950338762171841e-05
transformers;0.00010183431005279534
import;0.00011412240319071709
AutoTokenizer,;0.0003833242935870746
BertModel;0.00019462298154766623
import;0.00011518548507643201
torch;9.092124929015969e-05
tokenizer;0.00011320310636846562
=;0.0001001438918703246
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.36681439357393447
model;9.398329220868631e-05
=;0.00011862158032957003
"BertModel.from_pretrained(""bert-base-uncased"")";0.6194736435699159
inputs;0.00010104448514963561
=;0.00011532643255021255
"tokenizer(""Hello,";0.0008945205726251253
my;9.607918351152459e-05
dog;9.381744776286452e-05
is;0.00010865650437980716
"cute"",";0.00028593056471172844
"return_tensors=""pt"")";0.0030382830054027604
outputs;9.88142522663493e-05
=;0.00010574011882879243
model(**inputs);0.0007823890027003451
last_hidden_states;0.00040846326173318495
=;0.00010186321246122176
outputs.last_hidden_state;0.000823969249369611
***;0.00017520820018038366
