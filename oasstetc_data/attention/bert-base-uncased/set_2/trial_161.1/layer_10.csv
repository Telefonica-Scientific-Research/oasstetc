text;attention
The;1.6227901869133653e-16
easiest;2.3379381605951327e-16
way;1.5492413398829906e-16
to;1.632703644792621e-16
import;3.0819736922379256e-16
the;2.702360921587553e-16
BERT;2.5840953192698083e-16
language;3.7935010330933926e-16
model;3.459515419754928e-16
into;1.5523275700230229e-16
python;6.026916527571225e-16
for;1.4515093065959506e-16
use;1.7896623156023682e-16
with;2.406209612212455e-16
PyTorch;4.802887084502153e-16
is;1.7816741289102395e-16
using;1.6081191306126718e-16
the;1.5284521974469528e-16
Hugging;2.1004945154265547e-16
Face;2.4807805117217373e-16
Transformer's;1.643678106621466e-09
library,;2.9927203789756417e-10
which;1.4938178496014736e-16
has;1.8991742099936342e-16
built;1.8046772992404265e-16
in;1.481277668468913e-16
methods;1.8620215065463721e-16
for;1.6309062218996173e-16
pre-training,;4.1972552548906397e-16
inference,;5.342956245632826e-13
and;1.5279526255244626e-16
deploying;2.866993858501051e-16
BERT.;0.9999999980563803
â€˜**;7.928945622898634e-16
from;1.7032276786813034e-16
transformers;4.888244891161969e-16
import;2.166928794561507e-16
AutoTokenizer,;6.830718906421808e-16
BertModel;2.675430145969153e-16
import;2.2335458282825625e-16
torch;2.4648223083431256e-16
tokenizer;2.8670215421910897e-16
=;1.8528129420252835e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";6.946502464346562e-14
model;1.8578627724387453e-16
=;1.8276516049973152e-16
"BertModel.from_pretrained(""bert-base-uncased"")";4.011194790377645e-14
inputs;2.098781737465954e-16
=;1.8113998819002755e-16
"tokenizer(""Hello,";1.393154154494212e-15
my;1.9924765133051167e-16
dog;4.0874167592002767e-16
is;1.7423756105530912e-16
"cute"",";4.149755299398172e-16
"return_tensors=""pt"")";5.449960726601236e-15
outputs;1.7701244101696333e-16
=;1.7842646985415563e-16
model(**inputs);3.433521755884281e-15
last_hidden_states;4.837149079223053e-16
=;2.6582024637254136e-16
outputs.last_hidden_state;9.980577145422344e-16
***;1.091281819343576e-15
