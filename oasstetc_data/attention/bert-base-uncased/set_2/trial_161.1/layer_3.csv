text;attention
The;1.9231768017521086e-06
easiest;2.0172891904074454e-06
way;1.5005069540183405e-06
to;1.712458210324202e-06
import;2.0372761809961306e-06
the;2.045567567249765e-06
BERT;1.6009182242430174e-06
language;2.071194035599415e-06
model;1.967481486114086e-06
into;1.9300066506080175e-06
python;2.934656685912452e-06
for;1.8401907547378446e-06
use;1.5984576527869293e-06
with;2.1525202123091685e-06
PyTorch;5.158576959279661e-06
is;1.8902741000309842e-06
using;1.7733978049796327e-06
the;1.9276392544171377e-06
Hugging;2.5137493316441792e-06
Face;1.552605037705576e-06
Transformer's;7.299264523159458e-06
library,;9.21075129467674e-06
which;1.4328799503013622e-06
has;1.6082789747308267e-06
built;1.7401062813889125e-06
in;1.6156150183592878e-06
methods;2.2802584254608777e-06
for;2.3529986463762073e-06
pre-training,;1.6155879623140016e-05
inference,;3.144724225004208e-06
and;2.007210819704639e-06
deploying;3.15585495563695e-06
BERT.;7.626060870975107e-06
â€˜**;9.97100895856801e-06
from;2.140296481171659e-06
transformers;3.0307128134293625e-06
import;2.103290137624455e-06
AutoTokenizer,;2.044586003566178e-05
BertModel;3.959313941760405e-06
import;2.017606167435738e-06
torch;1.8666644709455866e-06
tokenizer;1.9340982590169347e-06
=;2.753977418183364e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6679270382237551
model;1.5825875557173977e-06
=;3.408814403428245e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.3311213819764313
inputs;1.585298643119421e-06
=;2.7631782507504358e-06
"tokenizer(""Hello,";4.6976661522168524e-05
my;1.2743245684592094e-06
dog;2.0665720665322217e-06
is;1.4358385710269597e-06
"cute"",";1.245966511594694e-05
"return_tensors=""pt"")";0.000515060048512322
outputs;1.397931183474457e-06
=;2.2508236596409457e-06
model(**inputs);4.2847681372065254e-05
last_hidden_states;2.672410103934484e-05
=;3.548598094989314e-06
outputs.last_hidden_state;0.0001272519931176742
***;6.945026749108086e-06
