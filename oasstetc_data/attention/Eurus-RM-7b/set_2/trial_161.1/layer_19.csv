text;attention
The;0.004247009096839986
easiest;0.0026795453416113236
way;0.0025672591904841854
to;0.0023079218327059447
import;0.005067123547805486
the;0.0026446168907748625
BERT;0.05849980668454378
language;0.00262942200529761
model;0.005487132958376139
into;0.003382674175435875
python;0.005337719850890128
for;0.0028487483663459674
use;0.0025086634311355287
with;0.002618513231528855
PyTorch;0.01279564017124401
is;0.0034016122197139554
using;0.0033115443645164422
the;0.0030530418393367883
Hugging;0.012093463158168238
Face;0.005645724756357828
Transformer's;0.028837799493767715
library,;0.003849211234931368
which;0.002530322971424548
has;0.0024929912911439435
built;0.0022462643300288168
in;0.0025106670087356774
methods;0.002595355134364056
for;0.002691995562871447
pre-training,;0.00607943012569846
inference,;0.0031205169075944074
and;0.0021586382765384263
deploying;0.0027800719169698237
BERT.;0.008359735223361602
â€˜**;0.0038273514228105376
from;0.005738023749388254
transformers;0.005580309613039277
import;0.004106886780975421
AutoTokenizer,;0.02593806716190982
BertModel;0.007427700924649451
import;0.003508551681680105
torch;0.004018239804669312
tokenizer;0.004792611164054139
=;0.005512843558610674
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5837344324825591
model;0.00321719748777016
=;0.0030162349539653425
"BertModel.from_pretrained(""bert-base-uncased"")";0.0624606953937017
inputs;0.0033597958943772576
=;0.0028999681627350834
"tokenizer(""Hello,";0.010639537656374662
my;0.0021948446507285054
dog;0.0025067825510908686
is;0.0021678751155750024
"cute"",";0.003002797735944485
"return_tensors=""pt"")";0.010651053122062996
outputs;0.0026746674297447024
=;0.0024474746282985983
model(**inputs);0.007667769075848983
last_hidden_states;0.00496231659437229
=;0.0027476099327263144
outputs.last_hidden_state;0.0037303707974595426
***;0.002087807882334062
