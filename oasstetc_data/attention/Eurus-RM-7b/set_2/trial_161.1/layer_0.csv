text;attention
The;0.004754218681396366
easiest;0.0030803693795232764
way;0.0021909906199230277
to;0.002506470433740195
import;0.005348141709642664
the;0.015884140869595085
BERT;0.004298426473138007
language;0.0022202104409311182
model;0.0023726335615179574
into;0.002068772426254102
python;0.017864372637871943
for;0.002234619430322146
use;0.0020079402543602513
with;0.0023455818763443727
PyTorch;0.007991684886141014
is;0.001868814380857516
using;0.002131590360367591
the;0.009754193078554326
Hugging;0.003468286628296271
Face;0.001943637116216121
Transformer's;0.00944975741679043
library,;0.0040678301151422765
which;0.001790527944988049
has;0.001756994692883673
built;0.0018691353230528866
in;0.0017453709226569513
methods;0.0018619023005318318
for;0.0018739280397477576
pre-training,;0.01064082458555639
inference,;0.0052438792210111555
and;0.0018091864254806396
deploying;0.003162790204984035
BERT.;0.0047263578830849425
â€˜**;0.003014267338498744
from;0.0016744709321808216
transformers;0.002496321712061895
import;0.00239329594331269
AutoTokenizer,;0.006718918143792739
BertModel;0.002696481692126921
import;0.0022736737915626027
torch;0.002087577451806578
tokenizer;0.0023096086191368424
=;0.001822738635174651
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6326087957597684
model;0.0014811671449091166
=;0.0016762975836234519
"BertModel.from_pretrained(""bert-base-uncased"")";0.15702856535940132
inputs;0.0014285810250773891
=;0.0015718053563238935
"tokenizer(""Hello,";0.005308988299420958
my;0.0013809292399086896
dog;0.0013433425934921053
is;0.0013401737248152073
"cute"",";0.0020848340343305443
"return_tensors=""pt"")";0.005466242531603705
outputs;0.0013167630590447126
=;0.0014342440554296816
model(**inputs);0.0033467360311429588
last_hidden_states;0.002437124221704264
=;0.0013253228275505744
outputs.last_hidden_state;0.00248902972805473
***;0.0011101228438395348
