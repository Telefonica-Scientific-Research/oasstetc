text;attention
The;0.013384233434763726
easiest;0.010508661471991455
way;0.009784776769509385
to;0.00925103994613755
import;0.012325273738989383
the;0.009379048041006128
BERT;0.020377104106351396
language;0.009832320368219922
model;0.011755836756207859
into;0.013708999008840915
python;0.01289436902067815
for;0.011546342849938506
use;0.009090027229935526
with;0.009339602707605504
PyTorch;0.015967393832225927
is;0.01726918054305945
using;0.010161335320631041
the;0.008836871283926169
Hugging;0.01358278253431077
Face;0.011935307279428467
Transformer's;0.02200904891049895
library,;0.013698184199326835
which;0.009449504332142134
has;0.00889041167103646
built;0.007866298628280893
in;0.009645615691329736
methods;0.008914842623919754
for;0.010263448246655128
pre-training,;0.01538104073278456
inference,;0.011439914768223163
and;0.008348967934111816
deploying;0.010701830695201255
BERT.;0.01419175015686644
â€˜**;0.01624147827076798
from;0.01677291204241655
transformers;0.011727542073743697
import;0.01200330088618995
AutoTokenizer,;0.02240353697981921
BertModel;0.01361980800818746
import;0.01116558200733184
torch;0.009219913236585461
tokenizer;0.011829225753451949
=;0.01374570242444933
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.18095704011443367
model;0.01016153307145364
=;0.01003153815344608
"BertModel.from_pretrained(""bert-base-uncased"")";0.09108518564251962
inputs;0.010255385196610343
=;0.00905479001585883
"tokenizer(""Hello,";0.02370217776372146
my;0.008061788083848422
dog;0.008243979398680223
is;0.007727830286360586
"cute"",";0.009701525568972432
"return_tensors=""pt"")";0.019573902096432408
outputs;0.009748156090990707
=;0.008575676189109784
model(**inputs);0.0180245605783908
last_hidden_states;0.0177341358674591
=;0.008757030319775783
outputs.last_hidden_state;0.010771735272248693
***;0.0073716637726096886
