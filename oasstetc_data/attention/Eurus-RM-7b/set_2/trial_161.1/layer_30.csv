text;attention
The;0.000135251996804025
easiest;0.00011381064338617211
way;0.0001081257965362172
to;0.00010110055869397648
import;0.00015856632814946811
the;0.00013241086459912834
BERT;0.00037165858731604175
language;0.00011693512831008664
model;0.00012394983555130907
into;0.00012781741093982743
python;0.0001527087887058975
for;0.0001362842516627514
use;0.00010120048099361795
with;0.00011309463270823125
PyTorch;0.00027665990946182134
is;0.00011891304199016081
using;0.00016151646488430586
the;0.0001475482074578156
Hugging;0.00023366696446831285
Face;0.00014929851104280312
Transformer's;0.0018500215730275467
library,;0.00016882358052144607
which;0.00010571911908200468
has;0.00010958244634869368
built;0.00010386465850162514
in;0.00012589988537247475
methods;0.00010386063454912712
for;0.00012645902439905693
pre-training,;0.0005387685164487823
inference,;0.0001669198494196847
and;9.846824690362831e-05
deploying;0.00011738569231588556
BERT.;0.0003583051263538427
â€˜**;0.0005498148910627101
from;0.0002506970019100178
transformers;0.0002098903406241308
import;0.0002626359507678019
AutoTokenizer,;0.001785216108395449
BertModel;0.0003977235139818038
import;0.00019758601762436372
torch;0.0001586887979753489
tokenizer;0.00031275958098985824
=;0.00022496585876518417
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.9409764430194961
model;0.00011820876870175369
=;0.00010567164591712226
"BertModel.from_pretrained(""bert-base-uncased"")";0.043705413524107596
inputs;0.00014949046759645226
=;0.00012161789122605748
"tokenizer(""Hello,";0.0004390675007933707
my;9.955278174754866e-05
dog;9.453158442135104e-05
is;8.85889905433946e-05
"cute"",";0.0001668883681506419
"return_tensors=""pt"")";0.0009555444809809978
outputs;0.00010080550810179212
=;9.960062269999486e-05
model(**inputs);0.0003488294815940395
last_hidden_states;0.00035590672221505774
=;8.571943929829227e-05
outputs.last_hidden_state;0.000501029949645892
***;8.251443376010676e-05
