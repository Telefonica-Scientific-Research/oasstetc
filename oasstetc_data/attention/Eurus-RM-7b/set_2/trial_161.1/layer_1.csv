text;attention
The;0.008008640954210297
easiest;0.008658319666660313
way;0.007561048023274165
to;0.0069468631315314985
import;0.008782872274575974
the;0.006555613740722073
BERT;0.01036625463764823
language;0.007315180438619586
model;0.006749713125061338
into;0.007318025141781259
python;0.007229314796347516
for;0.006921872765530969
use;0.006537555564339268
with;0.006406556691208478
PyTorch;0.011788222962127014
is;0.0074464217454271965
using;0.007245695705481636
the;0.006214747428536259
Hugging;0.009412593558588188
Face;0.006745046686605346
Transformer's;0.016449455213321213
library,;0.008602050518213387
which;0.0065294593699015335
has;0.006292844704743192
built;0.006759285531257742
in;0.006228764921470879
methods;0.006818443053207963
for;0.006443994142240363
pre-training,;0.014907807819088417
inference,;0.010228693237635852
and;0.006092215805156568
deploying;0.007690319098772977
BERT.;0.011797273710970859
â€˜**;0.013397765996686994
from;0.007045841262734457
transformers;0.007892454421634519
import;0.007394776099556935
AutoTokenizer,;0.013371922107955679
BertModel;0.007864145272847511
import;0.006885382892400143
torch;0.007010128184099542
tokenizer;0.008072840021521059
=;0.00736779125712698
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.33226932980290325
model;0.00606106168414024
=;0.0070937396944711325
"BertModel.from_pretrained(""bert-base-uncased"")";0.16033080451423265
inputs;0.006225179457325297
=;0.006781037559365559
"tokenizer(""Hello,";0.016807843488995475
my;0.005912395929741877
dog;0.006039049626842768
is;0.005846648321803197
"cute"",";0.008177571351960898
"return_tensors=""pt"")";0.021815966486025747
outputs;0.006139930856323239
=;0.006609124865336418
model(**inputs);0.012664673052220226
last_hidden_states;0.011927836241132804
=;0.005990956024683054
outputs.last_hidden_state;0.012755727276970403
***;0.0051969100847041975
