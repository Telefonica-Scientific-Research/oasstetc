text;attention
The;0.003324722916436011
easiest;0.0021095911875176642
way;0.0020608441028129357
to;0.0017743106142543113
import;0.002869623411439049
the;0.0019404286196258017
BERT;0.02695347603237427
language;0.0024111527767289274
model;0.004634676585678586
into;0.0028474418540808326
python;0.004576252927026554
for;0.002548938040814268
use;0.0021038402714883145
with;0.002186274014997221
PyTorch;0.011260004180274575
is;0.0024075461237854853
using;0.002552288898513135
the;0.0023113836859895475
Hugging;0.006790759651088315
Face;0.004031423845285302
Transformer's;0.014389663547845053
library,;0.0030509988030026775
which;0.0019055198842154735
has;0.0018249223822733225
built;0.001766804505784029
in;0.0020365819140523607
methods;0.002140456034969585
for;0.002046009795933648
pre-training,;0.006484132397421611
inference,;0.002866478673678013
and;0.0017261072511798557
deploying;0.0020814464833124016
BERT.;0.005636385633761868
â€˜**;0.003011918568248466
from;0.0031191184640644264
transformers;0.005434160462554598
import;0.003007067602278693
AutoTokenizer,;0.02386531759324079
BertModel;0.006405891356784165
import;0.002779730088924728
torch;0.003101674965127898
tokenizer;0.00413937869098259
=;0.003018111154705469
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7025070923484004
model;0.00216505011255272
=;0.0018878358414899528
"BertModel.from_pretrained(""bert-base-uncased"")";0.04449011160522639
inputs;0.0024498171370143933
=;0.0020206519897265768
"tokenizer(""Hello,";0.007056379359871488
my;0.001769036743932552
dog;0.0020155375947963645
is;0.0017232604601508765
"cute"",";0.002206633747426993
"return_tensors=""pt"")";0.01131808482265033
outputs;0.0021424428842714742
=;0.0017760167152134152
model(**inputs);0.005318256945168024
last_hidden_states;0.005411873786983677
=;0.001741057084381962
outputs.last_hidden_state;0.0028211359671776394
***;0.0016468688590119266
