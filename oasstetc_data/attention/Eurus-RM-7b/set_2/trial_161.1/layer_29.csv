text;attention
The;0.0023342940290959164
easiest;0.0019697938456867434
way;0.0019474191337417847
to;0.001759152451987229
import;0.0024607545173514133
the;0.002164874402509213
BERT;0.005474449133085694
language;0.0019662671677602005
model;0.0028401016328209305
into;0.002313531175104273
python;0.0030272955916806903
for;0.002188791444041481
use;0.0019455182275626278
with;0.002124084203845572
PyTorch;0.004785401516531802
is;0.0024407215604737046
using;0.002959466290152283
the;0.0024172512060479045
Hugging;0.0036418128627835893
Face;0.0029645535641714296
Transformer's;0.018712666451646633
library,;0.0028652110525333213
which;0.0018475688619424182
has;0.0019296605343636752
built;0.0018519675844871492
in;0.0019410873314788738
methods;0.0019570900213324006
for;0.00216531749490988
pre-training,;0.005321329722067934
inference,;0.0025243453910156807
and;0.001684519729804789
deploying;0.0023309539864473363
BERT.;0.010812909963249883
â€˜**;0.005419186561276747
from;0.002549639492344015
transformers;0.003071035271762611
import;0.003020213464356269
AutoTokenizer,;0.0070912091075987305
BertModel;0.0030593745210176445
import;0.0026743031902485797
torch;0.002697919282794149
tokenizer;0.003602096299423917
=;0.0023120616578154677
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1757687608934026
model;0.001987519737523702
=;0.0018998773145295262
"BertModel.from_pretrained(""bert-base-uncased"")";0.6289226845667021
inputs;0.0022917873596236477
=;0.0019489745822456712
"tokenizer(""Hello,";0.005587324357066557
my;0.001619653119744468
dog;0.0017689348203607101
is;0.0016035492167658935
"cute"",";0.002984358580056519
"return_tensors=""pt"")";0.006533751180334358
outputs;0.0017502173108883095
=;0.001720156673866424
model(**inputs);0.00410500511179087
last_hidden_states;0.003443890769496961
=;0.001684586174256351
outputs.last_hidden_state;0.007628941663007754
***;0.0015828256379850053
