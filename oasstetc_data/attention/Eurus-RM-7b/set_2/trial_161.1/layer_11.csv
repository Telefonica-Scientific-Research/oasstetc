text;attention
The;0.026736593160470935
easiest;0.00524443303947553
way;0.00457733811482213
to;0.004010449057389186
import;0.007132243485824223
the;0.004624888407046857
BERT;0.03256993099800823
language;0.005218164685616559
model;0.011576951910410608
into;0.012198331093230805
python;0.014054450323602767
for;0.008823167945009269
use;0.0053344299682713215
with;0.004697484882834052
PyTorch;0.019138927013594348
is;0.009091562639010856
using;0.005253884498515729
the;0.004813936635018278
Hugging;0.009819935592243863
Face;0.00635263271416854
Transformer's;0.02400421894079454
library,;0.007588517931348369
which;0.004481920828670657
has;0.004213929283398389
built;0.0037896129819004337
in;0.004903993614689835
methods;0.004550092808864571
for;0.004355904210628816
pre-training,;0.012120356956031381
inference,;0.007805385702152496
and;0.004309798735819428
deploying;0.004935551474848109
BERT.;0.012511539225913833
â€˜**;0.007626393898591349
from;0.009053322331861615
transformers;0.007467792041493048
import;0.0076596896451603174
AutoTokenizer,;0.050367129339866104
BertModel;0.013991367793665964
import;0.007440654729016512
torch;0.006387024402194523
tokenizer;0.014157870724036693
=;0.00822645647600174
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3843982375249088
model;0.005391822797007309
=;0.005051070096386164
"BertModel.from_pretrained(""bert-base-uncased"")";0.06638797904508667
inputs;0.005328409358760742
=;0.004853913377167394
"tokenizer(""Hello,";0.01316777427744254
my;0.003943337587389006
dog;0.004284945537024145
is;0.003955889509313047
"cute"",";0.005370837766067378
"return_tensors=""pt"")";0.019401229318604187
outputs;0.004506739585660786
=;0.00413326963330927
model(**inputs);0.013238406822239373
last_hidden_states;0.009230531974070606
=;0.0041992160740465705
outputs.last_hidden_state;0.00632598008135218
***;0.003612149392651024
