text;attention
The;0.0029085283369080766
easiest;0.0008853967657601567
way;0.0008203502897831174
to;0.0007221249749412195
import;0.001105359301593034
the;0.0007856652449779892
BERT;0.004323429531216757
language;0.0009562693446723901
model;0.001755492621341497
into;0.0013435256137799758
python;0.0019678954043578977
for;0.0011374900484708568
use;0.0008831470862983808
with;0.000830409367803294
PyTorch;0.004537945454168278
is;0.0014165328776952642
using;0.0010408722238758974
the;0.0010043199802509182
Hugging;0.0026914318322713757
Face;0.0013947936100473176
Transformer's;0.007503067485645914
library,;0.001492362780164297
which;0.0008065141867946003
has;0.0008271344141891259
built;0.0007086274021098737
in;0.0008741127701058458
methods;0.0008576761895990955
for;0.0007947119165031918
pre-training,;0.0031699494732193054
inference,;0.0016583113712178386
and;0.0007694301302649679
deploying;0.0009534484031251013
BERT.;0.002176316102188764
â€˜**;0.0012012055250389085
from;0.0012264007129489682
transformers;0.0014899601724666118
import;0.0013387948860991527
AutoTokenizer,;0.01031712578282161
BertModel;0.0020191019238762205
import;0.0014114304111226945
torch;0.0009758160644139446
tokenizer;0.0018229556490134845
=;0.0017887078815409965
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.8736000161759825
model;0.0010392166797578884
=;0.0008087026716632494
"BertModel.from_pretrained(""bert-base-uncased"")";0.021957435527274485
inputs;0.0011108176309097956
=;0.0009024184262923535
"tokenizer(""Hello,";0.004843750913741802
my;0.0007244149355545551
dog;0.0007572289787756059
is;0.0006817757151764717
"cute"",";0.0010766853085594932
"return_tensors=""pt"")";0.005038918219026475
outputs;0.0008773650215480727
=;0.0007551427799470372
model(**inputs);0.002981187029627123
last_hidden_states;0.0015644210686299644
=;0.0007718512970786453
outputs.last_hidden_state;0.0011601927670955544
***;0.0006543173086747861
