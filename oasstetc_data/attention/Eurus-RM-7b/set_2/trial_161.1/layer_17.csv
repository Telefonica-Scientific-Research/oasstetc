text;attention
The;0.0030030972079397884
easiest;0.00198526409220926
way;0.0017256796148546773
to;0.001518302926485028
import;0.003207036915547245
the;0.0016884298874603413
BERT;0.006310950086339877
language;0.0016400224215272863
model;0.0024597488652118517
into;0.002114440248522995
python;0.0028692937444217167
for;0.0019547117909260938
use;0.0016755558576892272
with;0.0016987109689470762
PyTorch;0.004691985565710394
is;0.0022424823089005007
using;0.0023691764968195553
the;0.0019901641797635226
Hugging;0.0038128796684643996
Face;0.002484556952745745
Transformer's;0.008297279959547817
library,;0.0024346867574732423
which;0.0016631855461069045
has;0.0018255110247929248
built;0.0015475100823264547
in;0.001848163421478685
methods;0.0018067948910220123
for;0.0020714452338667635
pre-training,;0.008208146693010421
inference,;0.002616872572285854
and;0.0015197674133350162
deploying;0.0018083657966803002
BERT.;0.003688500245254931
â€˜**;0.00244158427544274
from;0.0026125244870720777
transformers;0.0024408522249687666
import;0.0023035278953140513
AutoTokenizer,;0.009078683781907923
BertModel;0.0029975489991308727
import;0.002729680199030703
torch;0.002142961406575499
tokenizer;0.0033756899635687633
=;0.002364238597651349
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6840322719886366
model;0.0019952749611587316
=;0.001802250302056185
"BertModel.from_pretrained(""bert-base-uncased"")";0.13672586062143646
inputs;0.0025830853020854523
=;0.0019910447946559526
"tokenizer(""Hello,";0.008104393862286311
my;0.00142943670277298
dog;0.0015479946424118076
is;0.0013879421300174596
"cute"",";0.0022640944354425153
"return_tensors=""pt"")";0.01001755382369698
outputs;0.0019102559641269111
=;0.0017296947838449839
model(**inputs);0.006737019210142183
last_hidden_states;0.005851526236299376
=;0.0018772127094799243
outputs.last_hidden_state;0.0034435229978365847
***;0.0013035532632821591
