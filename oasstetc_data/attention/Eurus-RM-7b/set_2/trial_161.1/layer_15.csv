text;attention
The;0.009027034592295348
easiest;0.0020432893211262022
way;0.0019069978887603864
to;0.0018251453657736954
import;0.0028658823815571944
the;0.0019825455090901104
BERT;0.026486506220313503
language;0.0026905893943683433
model;0.005932393808449758
into;0.003791234173498169
python;0.00531188765279083
for;0.0030424147403642645
use;0.0021327419147124716
with;0.002088819644396395
PyTorch;0.016755512026413278
is;0.002867713130479671
using;0.0023097001877151004
the;0.002338290684024538
Hugging;0.009510697921550208
Face;0.004786166285121211
Transformer's;0.0316396614481165
library,;0.003466720340075471
which;0.0017986345858593332
has;0.0018688234479878577
built;0.001674121686636726
in;0.00191525672839697
methods;0.002036316676310362
for;0.0018285160347630538
pre-training,;0.008103673279577618
inference,;0.003663088238272104
and;0.00177353649255639
deploying;0.002078337932809716
BERT.;0.005859268771488236
â€˜**;0.0028358894373570107
from;0.0031234012985088297
transformers;0.003111659947938999
import;0.00339763606731033
AutoTokenizer,;0.04819216770882169
BertModel;0.008063846346636254
import;0.0033529802985365156
torch;0.002829902813694074
tokenizer;0.003165045228581213
=;0.0031107494264969904
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6485735315858937
model;0.0025324277869487747
=;0.0018870928635734373
"BertModel.from_pretrained(""bert-base-uncased"")";0.03495578470929144
inputs;0.003028814833906614
=;0.001980671881404153
"tokenizer(""Hello,";0.006361425705041189
my;0.0017302356916282595
dog;0.001733519888206197
is;0.0016467486012336832
"cute"",";0.0020901329994390348
"return_tensors=""pt"")";0.016047335942917115
outputs;0.0021908608336520946
=;0.0018010150288804761
model(**inputs);0.004657732473660256
last_hidden_states;0.004356812235370871
=;0.0018855443753303654
outputs.last_hidden_state;0.002356741824691426
***;0.0015987736593983158
