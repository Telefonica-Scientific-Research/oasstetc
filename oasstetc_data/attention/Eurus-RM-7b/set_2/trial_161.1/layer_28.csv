text;attention
The;0.003181573717404229
easiest;0.002930005592535648
way;0.0029055465237203086
to;0.0026364789928252645
import;0.003752961130963368
the;0.003372363602796087
BERT;0.007694878960464691
language;0.0027812800543093883
model;0.003287325600878932
into;0.0033913314480012078
python;0.0038098258323117627
for;0.0032039093986252713
use;0.00254050870140388
with;0.0029170913351215214
PyTorch;0.005504198885374012
is;0.0034730259759724593
using;0.003866375729196865
the;0.00364210113036718
Hugging;0.005078202121187437
Face;0.003893094506828499
Transformer's;0.01814558481665278
library,;0.004201814305654918
which;0.0028853549065773554
has;0.0028114532811690136
built;0.00255222710405322
in;0.0029994217191463973
methods;0.002843680747653505
for;0.0030702981249426634
pre-training,;0.006979204679744431
inference,;0.003764192742624036
and;0.0025647802765310615
deploying;0.0028279930745975885
BERT.;0.008891304445664246
â€˜**;0.01524354498549113
from;0.005468393680868799
transformers;0.004131811256980315
import;0.00439364830468211
AutoTokenizer,;0.010883073828266419
BertModel;0.004612744199718638
import;0.0034908229141497413
torch;0.004636395388160626
tokenizer;0.004459518083937425
=;0.00401999326149407
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.48566705819306477
model;0.0024712360146745983
=;0.0028624570906174373
"BertModel.from_pretrained(""bert-base-uncased"")";0.2404830895456366
inputs;0.0030802203142024495
=;0.002998207784122398
"tokenizer(""Hello,";0.011289382399198524
my;0.0024746212310506627
dog;0.002486709248786024
is;0.002387737886989133
"cute"",";0.0036512442620369
"return_tensors=""pt"")";0.010603817986174989
outputs;0.002659909421919917
=;0.0025467556174055706
model(**inputs);0.006344054051867171
last_hidden_states;0.006062410122319797
=;0.0024676577405548564
outputs.last_hidden_state;0.007438679623714714
***;0.0022854160966151275
