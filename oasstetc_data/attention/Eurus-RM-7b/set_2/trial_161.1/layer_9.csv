text;attention
The;0.0248373532965431
easiest;0.0056168039841242145
way;0.005363869595217564
to;0.004662391010424888
import;0.009112462193640978
the;0.0052737057213739546
BERT;0.06260516955333457
language;0.006590753706747703
model;0.022585813091170543
into;0.01605572478733252
python;0.012806625338293472
for;0.008090640501185131
use;0.005654956984912322
with;0.005245716846704185
PyTorch;0.02693656265843647
is;0.011359658775116743
using;0.006121468394994942
the;0.005150061968445791
Hugging;0.014354648494416965
Face;0.010649066111960325
Transformer's;0.06370712633586491
library,;0.012114714984974186
which;0.005186580540434366
has;0.004917048076002545
built;0.004388089216395737
in;0.005307747761402376
methods;0.005100680004712891
for;0.005051874902378326
pre-training,;0.015500382997428876
inference,;0.009393731090808424
and;0.004781030078928588
deploying;0.006436549657948617
BERT.;0.024017824291397444
â€˜**;0.00775596745814871
from;0.0061572570570499405
transformers;0.009272917172755864
import;0.0079207170015005
AutoTokenizer,;0.07609243251047376
BertModel;0.017450768783485796
import;0.008568790612581685
torch;0.006729769358573842
tokenizer;0.015414668507499427
=;0.007081846034004259
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.23933029974024836
model;0.006272845074117345
=;0.005288171999739352
"BertModel.from_pretrained(""bert-base-uncased"")";0.039584854735659036
inputs;0.007237816943682006
=;0.005114136049227072
"tokenizer(""Hello,";0.015014351643029947
my;0.004586643879512
dog;0.004984012835878361
is;0.004507090007803228
"cute"",";0.005230698144376332
"return_tensors=""pt"")";0.02037265132620259
outputs;0.005188005427588856
=;0.004719824059289065
model(**inputs);0.010744335633009451
last_hidden_states;0.009022362427647939
=;0.005192579783612644
outputs.last_hidden_state;0.00591261170364667
***;0.004274741136602027
