text;attention
The;0.01855911895774472
easiest;0.003284250695617438
way;0.002928411379500377
to;0.0022044572301057708
import;0.004965524401795237
the;0.0024242333442623665
BERT;0.014231837221548586
language;0.0023404645182846726
model;0.006359003928894476
into;0.00734717041900404
python;0.0070035653404605545
for;0.0044875847325067655
use;0.002867685781436965
with;0.002274804884226436
PyTorch;0.009234138841346464
is;0.011400036013346389
using;0.004625166797958649
the;0.0026525172707317656
Hugging;0.005280233010370977
Face;0.0037208988166121675
Transformer's;0.018223887201660502
library,;0.005070443154608433
which;0.002781995814847124
has;0.0022995687066017013
built;0.0019356663587674562
in;0.00255419613477895
methods;0.002592933057385511
for;0.0026518677077241866
pre-training,;0.0072736693553337095
inference,;0.004308836069960922
and;0.002251400351005496
deploying;0.002679312345887915
BERT.;0.01070169960053907
â€˜**;0.00732888355010736
from;0.004386892251861563
transformers;0.004243102605396411
import;0.0037973782132528847
AutoTokenizer,;0.024518759239554065
BertModel;0.006058265543308724
import;0.004872586160544708
torch;0.0030483829312036134
tokenizer;0.007519186078922109
=;0.005660007713802397
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5831390135754789
model;0.002935760458180948
=;0.003031472157327057
"BertModel.from_pretrained(""bert-base-uncased"")";0.09089770088122284
inputs;0.0028808425817625632
=;0.00284483975557842
"tokenizer(""Hello,";0.015103414950245874
my;0.002130878494627452
dog;0.0022518075321695317
is;0.002048626611514693
"cute"",";0.0027718245945713674
"return_tensors=""pt"")";0.010914979406548257
outputs;0.0023894587211604375
=;0.0022486536022728342
model(**inputs);0.007293040128715924
last_hidden_states;0.004623371960557188
=;0.002421638221186797
outputs.last_hidden_state;0.0033226380470530803
***;0.0018000145870163396
