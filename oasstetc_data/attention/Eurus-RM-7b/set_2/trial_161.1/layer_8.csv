text;attention
The;0.012749937010272858
easiest;0.005536807152537807
way;0.005175531009381898
to;0.004562485772174132
import;0.008210023655118104
the;0.004924508136555948
BERT;0.021472287884387008
language;0.005743795986480635
model;0.009933914308781874
into;0.010782575175999197
python;0.014213012839105612
for;0.007306763397074645
use;0.005204145053072012
with;0.004731601160190192
PyTorch;0.014386165892616386
is;0.0076126204373251185
using;0.005499191845987841
the;0.005326247757893986
Hugging;0.009643627322445773
Face;0.008704321413996744
Transformer's;0.028200107532773327
library,;0.009710395384431298
which;0.005007733596811836
has;0.004486943404174522
built;0.004030215092743613
in;0.005100319083899959
methods;0.005103656300929311
for;0.004854654089903144
pre-training,;0.014441470525967651
inference,;0.008130161895352078
and;0.004262869414096284
deploying;0.0064064480809279825
BERT.;0.011010538837670919
â€˜**;0.00875411052078049
from;0.007510027832130577
transformers;0.008926112788938657
import;0.007580392912320998
AutoTokenizer,;0.027619335281568083
BertModel;0.010101379158978671
import;0.006713602216662605
torch;0.006706613827952816
tokenizer;0.00911850672067717
=;0.006694558363164403
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.39022032935586215
model;0.005553123380168862
=;0.005402347256111732
"BertModel.from_pretrained(""bert-base-uncased"")";0.09669192066309637
inputs;0.006342129813123225
=;0.005304248894291737
"tokenizer(""Hello,";0.022647989604943533
my;0.004631745693797922
dog;0.004787318565095905
is;0.004316906412974764
"cute"",";0.005998624884365128
"return_tensors=""pt"")";0.021457604810294715
outputs;0.004891678111185325
=;0.0044351587548683334
model(**inputs);0.010877851226906838
last_hidden_states;0.009227406965204124
=;0.004456832828006376
outputs.last_hidden_state;0.006744184202881928
***;0.0038228825025670133
