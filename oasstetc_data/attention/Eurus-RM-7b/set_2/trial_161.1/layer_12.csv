text;attention
The;0.008878082116299509
easiest;0.0038928491600096326
way;0.0034373364934034787
to;0.003044047047354967
import;0.0056765236066097565
the;0.003489664087785175
BERT;0.03170076997532905
language;0.003782342121803466
model;0.010812183054414693
into;0.009074606431124536
python;0.009978205540218775
for;0.007338801440559932
use;0.004283959530178747
with;0.0035860339083043213
PyTorch;0.021570331663753165
is;0.008707571601448857
using;0.0037751382401396113
the;0.0036410828455046454
Hugging;0.007999859152946952
Face;0.005682391886023537
Transformer's;0.01653356611284564
library,;0.005377307729330784
which;0.0034349685751533404
has;0.0032104069573474143
built;0.002835758076443877
in;0.0036063299320091176
methods;0.003730741471649983
for;0.003658499207392009
pre-training,;0.009920596567156686
inference,;0.006099737756428758
and;0.0032631503554098763
deploying;0.0038481584516762307
BERT.;0.011747948850425466
â€˜**;0.005153758799269676
from;0.005561874887930507
transformers;0.004553117214165042
import;0.0048349802400786245
AutoTokenizer,;0.05301568384789347
BertModel;0.0087919536733865
import;0.005144237712613866
torch;0.00466039302228475
tokenizer;0.009283113618818162
=;0.004732445459178164
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5156074540656281
model;0.004502038982875022
=;0.0036001618410119135
"BertModel.from_pretrained(""bert-base-uncased"")";0.05354397472128422
inputs;0.004663530503990942
=;0.0037703527151904644
"tokenizer(""Hello,";0.012178854960375659
my;0.002880084184181493
dog;0.0030282061153789276
is;0.0028688751655428437
"cute"",";0.00380164940617992
"return_tensors=""pt"")";0.013970661198478547
outputs;0.003325792942121634
=;0.0030724774052782356
model(**inputs);0.009836279994229534
last_hidden_states;0.006126823255286112
=;0.0030994580424480458
outputs.last_hidden_state;0.004068484600931805
***;0.0027043314774860758
