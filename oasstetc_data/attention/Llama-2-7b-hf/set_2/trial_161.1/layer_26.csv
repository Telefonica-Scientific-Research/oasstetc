text;attention
The;2.4267464579888764e-14
easiest;1.9125851867235907e-14
way;1.962090730575036e-14
to;1.6972992594549448e-14
import;2.39589339066697e-14
the;1.8993032455661706e-14
BERT;8.717472296066458e-14
language;1.9845105457238134e-14
model;2.2146522610048586e-14
into;2.2527502518786055e-14
python;2.3490306606631245e-14
for;1.9021840065517887e-14
use;1.7633419259400512e-14
with;1.992892321275772e-14
PyTorch;4.00624107211962e-14
is;1.8713306709235082e-14
using;2.1184320775039087e-14
the;2.0095073262314992e-14
Hugging;3.4269599388424884e-14
Face;2.3696274590891804e-14
Transformer's;9.062833114794621e-13
library,;1.9507030195597533e-14
which;1.7833144556161024e-14
has;1.7265255804392495e-14
built;1.6918298137055756e-14
in;1.7686055523173368e-14
methods;1.824426458351479e-14
for;1.8331509006862448e-14
pre-training,;2.863533983364977e-14
inference,;1.9840500080917917e-14
and;1.6818369016191324e-14
deploying;1.9421321975940474e-14
BERT.;0.9999999999972895
â€˜**;3.5201542941906734e-14
from;2.0234436328205866e-14
transformers;2.2786067940833385e-14
import;2.374097562981178e-14
AutoTokenizer,;3.669623034614304e-14
BertModel;4.1765191190106835e-14
import;1.8977051482455165e-14
torch;2.2561118660907037e-14
tokenizer;2.3223029332881548e-14
=;1.865264689005247e-14
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";3.209175297355192e-13
model;1.8036626729694355e-14
=;1.788513303050541e-14
"BertModel.from_pretrained(""bert-base-uncased"")";1.183237684219245e-13
inputs;1.8377551982881717e-14
=;1.825666102065196e-14
"tokenizer(""Hello,";3.6518064170933454e-14
my;1.6709512601262804e-14
dog;1.7519279362175588e-14
is;1.7770753184308575e-14
"cute"",";2.038758961883086e-14
"return_tensors=""pt"")";4.377832542813256e-14
outputs;1.7594792128700258e-14
=;1.7354408426346555e-14
model(**inputs);3.535946029308913e-14
last_hidden_states;2.473441869644175e-14
=;1.723879004890046e-14
outputs.last_hidden_state;2.348059459087777e-14
***;1.6817533617745246e-14
