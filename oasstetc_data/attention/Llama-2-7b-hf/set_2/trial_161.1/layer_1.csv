text;attention
The;0.00293427072110026
easiest;0.00012025210293112392
way;6.579234983328029e-05
to;0.0022871582317161953
import;7.210752908124616e-05
the;0.24964705897591305
BERT;0.0003289643472783613
language;6.398049015234377e-05
model;5.289811022497254e-05
into;9.97586884417659e-05
python;8.57468920653335e-05
for;0.00019613875470043601
use;4.093198212013846e-05
with;0.0001336469155169764
PyTorch;0.0004024155729169271
is;8.917246041894524e-05
using;4.971976926690306e-05
the;0.013140384629490444
Hugging;0.00019172231742966403
Face;3.561551517640415e-05
Transformer's;0.0009372701381667697
library,;0.018389658680811088
which;3.451429510141584e-05
has;3.393985723837092e-05
built;3.0406212972729062e-05
in;6.56621663706188e-05
methods;3.122615995486612e-05
for;5.288144623462933e-05
pre-training,;0.012403064459209499
inference,;0.0007509199139651042
and;5.85888217532293e-05
deploying;4.623341389463092e-05
BERT.;0.05344826851603877
â€˜**;6.86263475473246e-05
from;3.92900947434442e-05
transformers;4.714606045888379e-05
import;2.969232721298997e-05
AutoTokenizer,;0.0008755799766185112
BertModel;4.4903413679477325e-05
import;2.8726153292862188e-05
torch;4.731354276778813e-05
tokenizer;3.566219749812462e-05
=;4.609729715230158e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5863052832261897
model;2.4978642573609902e-05
=;3.680128210106096e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.055075199847786596
inputs;2.322950439819971e-05
=;3.142235863899712e-05
"tokenizer(""Hello,";0.00025279295715109585
my;2.3607845205779205e-05
dog;2.3172295315339827e-05
is;2.4907066457301543e-05
"cute"",";5.365768584960358e-05
"return_tensors=""pt"")";0.00020376318881815467
outputs;2.2854906178844817e-05
=;2.811098428719331e-05
model(**inputs);0.00011805751674366634
last_hidden_states;5.578881508703002e-05
=;2.2841340068006914e-05
outputs.last_hidden_state;7.183574848481907e-05
***;1.8286940207072043e-05
