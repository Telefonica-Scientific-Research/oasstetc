text;attention
The;6.085092194467686e-14
easiest;3.7105402153021617e-14
way;3.607697506340192e-14
to;3.6490729199715e-14
import;5.370846465173312e-14
the;3.682923949050596e-14
BERT;7.846724657584602e-14
language;4.243660322048177e-14
model;5.831162213290066e-14
into;5.5154291628349825e-14
python;6.526499393541299e-14
for;4.196826689269056e-14
use;3.588585953441454e-14
with;3.357871584930669e-14
PyTorch;6.503606060434592e-14
is;5.0103263482563263e-14
using;3.979742332993191e-14
the;3.415061910798299e-14
Hugging;4.380731737230543e-14
Face;5.6462911048342576e-14
Transformer's;2.816792555140823e-13
library,;5.158634323244145e-14
which;3.6514153491066346e-14
has;3.4530515533506404e-14
built;3.0268795461332575e-14
in;3.846611265554546e-14
methods;3.542300142575038e-14
for;3.4176091116527304e-14
pre-training,;5.743749647589125e-14
inference,;4.1014039250295176e-14
and;3.1363414604654307e-14
deploying;3.689884123060335e-14
BERT.;0.9999999999934408
â€˜**;5.937938807229742e-14
from;5.006301419897257e-14
transformers;5.1284291581558825e-14
import;4.83984576495623e-14
AutoTokenizer,;1.3043537053887822e-13
BertModel;7.860120167980561e-14
import;4.903063506451049e-14
torch;4.330847521571944e-14
tokenizer;5.579806032317501e-14
=;4.842530501997197e-14
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.882986707161124e-12
model;4.7758860359918444e-14
=;4.308323170218411e-14
"BertModel.from_pretrained(""bert-base-uncased"")";4.693254157732864e-13
inputs;4.790497707916088e-14
=;4.4445197255664906e-14
"tokenizer(""Hello,";1.3185499261103257e-13
my;3.3540534944878174e-14
dog;3.5931918501007826e-14
is;3.387919665467456e-14
"cute"",";4.179074132879636e-14
"return_tensors=""pt"")";9.895228826089481e-14
outputs;4.06591826089128e-14
=;3.6126794993896365e-14
model(**inputs);8.192109275876563e-14
last_hidden_states;8.701660911853473e-14
=;3.4695957530143213e-14
outputs.last_hidden_state;5.092412433974963e-14
***;3.0965458374947926e-14
