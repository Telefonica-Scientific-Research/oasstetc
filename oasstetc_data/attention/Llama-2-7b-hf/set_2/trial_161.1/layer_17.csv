text;attention
The;5.621950242538518e-15
easiest;4.642965047225256e-15
way;4.7101322820429275e-15
to;4.469975071474198e-15
import;6.3942250033134484e-15
the;4.69124489975705e-15
BERT;1.8758538052280663e-14
language;5.4612385126507145e-15
model;9.015025318114154e-15
into;5.930205183153678e-15
python;6.5362591421360874e-15
for;5.135004675972989e-15
use;4.506119952843683e-15
with;4.671016537817058e-15
PyTorch;8.987349338825639e-15
is;5.392586167241549e-15
using;5.092146327676648e-15
the;4.980952295127807e-15
Hugging;6.893354792713301e-15
Face;7.73951333276892e-15
Transformer's;1.9769709547929942e-13
library,;5.804680243579548e-15
which;4.515115057127097e-15
has;4.435436234450277e-15
built;4.344350874573238e-15
in;4.813022234190403e-15
methods;4.7825637352568644e-15
for;4.937604448533145e-15
pre-training,;8.480019897611075e-15
inference,;5.7851223335160895e-15
and;4.194269578230489e-15
deploying;5.12013051551231e-15
BERT.;0.9999999999991913
â€˜**;5.947774204574396e-15
from;5.6015881704566675e-15
transformers;7.079837249786452e-15
import;5.3004838773715066e-15
AutoTokenizer,;1.4197300846198985e-14
BertModel;1.081260465259155e-14
import;5.201795919237072e-15
torch;5.773622044551437e-15
tokenizer;1.0057532393999344e-14
=;5.309439301588289e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.870156569101221e-13
model;5.172324248919513e-15
=;4.602850098892259e-15
"BertModel.from_pretrained(""bert-base-uncased"")";5.164706137040804e-14
inputs;5.433426592116661e-15
=;4.779453569268022e-15
"tokenizer(""Hello,";1.2470283296377187e-14
my;4.296203895752995e-15
dog;4.822409362256771e-15
is;4.320246750437552e-15
"cute"",";5.440644375814771e-15
"return_tensors=""pt"")";1.3936920107043309e-14
outputs;5.1177606271304305e-15
=;4.488331542136642e-15
model(**inputs);9.587654139357635e-15
last_hidden_states;9.482318610211473e-15
=;4.538804402190623e-15
outputs.last_hidden_state;7.64997560097e-15
***;4.220725355075591e-15
