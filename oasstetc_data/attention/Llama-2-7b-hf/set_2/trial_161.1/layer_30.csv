text;attention
The;3.3790466499345456e-12
easiest;2.435974660068009e-12
way;2.8764672840615857e-12
to;2.3407564461810442e-12
import;2.59993048042963e-12
the;2.358213446553379e-12
BERT;3.653678009992216e-12
language;2.485040366118283e-12
model;3.1246548647062175e-12
into;2.446981232970257e-12
python;2.562709522515657e-12
for;2.3085687582278466e-12
use;2.3405841628152605e-12
with;2.3321999474023993e-12
PyTorch;3.5099074414502346e-12
is;2.6339289439780505e-12
using;2.5640920099570123e-12
the;2.455081307042444e-12
Hugging;4.616909078142218e-12
Face;2.7469503825678192e-12
Transformer's;2.2938400893524581e-10
library,;2.8209408282769374e-12
which;2.3337774964529113e-12
has;2.2857543837985357e-12
built;2.3176843116451278e-12
in;2.272210650575535e-12
methods;2.628579579264771e-12
for;2.2717716995974804e-12
pre-training,;3.5575922434505443e-12
inference,;2.5799535719508844e-12
and;2.2019515072692293e-12
deploying;2.433613874825445e-12
BERT.;0.9999999994655413
â€˜**;4.001049684289953e-12
from;2.504354745052069e-12
transformers;3.4723350874399907e-12
import;2.851977426755468e-12
AutoTokenizer,;4.469066460113674e-12
BertModel;3.4540720595996754e-12
import;2.423112517138383e-12
torch;2.78204682635765e-12
tokenizer;2.848364983252764e-12
=;2.536175735141701e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.0643517563304366e-10
model;2.3044004336246827e-12
=;2.328322936670992e-12
"BertModel.from_pretrained(""bert-base-uncased"")";2.4951853830260153e-11
inputs;2.4864847700738548e-12
=;2.361297777321268e-12
"tokenizer(""Hello,";5.752957082043479e-12
my;2.430604981284938e-12
dog;2.520622870003077e-12
is;2.2685430730508673e-12
"cute"",";3.242872590940603e-12
"return_tensors=""pt"")";1.1773503155517524e-11
outputs;2.303607991769146e-12
=;2.2036476029293745e-12
model(**inputs);4.412984676162004e-12
last_hidden_states;4.078211700188159e-12
=;2.1733014299812655e-12
outputs.last_hidden_state;4.056486396885377e-12
***;2.1716529205830466e-12
