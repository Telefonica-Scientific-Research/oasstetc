text;attention
The;2.0591732232921676e-13
easiest;1.2723001444030635e-13
way;1.215387784662878e-13
to;1.1529916718495254e-13
import;1.675676401333899e-13
the;1.3118397660595536e-13
BERT;3.395708287532094e-13
language;1.3685664413497323e-13
model;2.630687901986393e-13
into;2.5933038670655e-13
python;3.1432315433908356e-13
for;2.0057986900744544e-13
use;1.2972624833640898e-13
with;1.281771104592063e-13
PyTorch;3.861759475824438e-13
is;2.1878797165034452e-13
using;1.5539348035857226e-13
the;1.445223730288011e-13
Hugging;1.610583548135383e-13
Face;2.5051181180502263e-13
Transformer's;1.1393345189641218e-12
library,;2.196223754926799e-13
which;1.2688661627368193e-13
has;1.230107342619885e-13
built;1.0535730376144727e-13
in;1.3088448103600973e-13
methods;1.26446068591053e-13
for;1.217845768399631e-13
pre-training,;2.651450468587069e-13
inference,;1.8057606793732718e-13
and;1.1685128167099137e-13
deploying;1.3876313317594394e-13
BERT.;0.9999999999780964
â€˜**;2.453801833722333e-13
from;1.4631669429900974e-13
transformers;2.1549958207910422e-13
import;1.927518111582639e-13
AutoTokenizer,;6.241823022073362e-13
BertModel;3.1442128547730644e-13
import;1.8564732786427712e-13
torch;1.7374522964849593e-13
tokenizer;2.1165347710439148e-13
=;1.8813623828398448e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";8.595702051044156e-12
model;1.642420924724033e-13
=;1.4130905877289475e-13
"BertModel.from_pretrained(""bert-base-uncased"")";1.0538719254693193e-12
inputs;1.4173238782601792e-13
=;1.3053156945136017e-13
"tokenizer(""Hello,";3.634161979385396e-13
my;1.1505854562656326e-13
dog;1.2716518445603866e-13
is;1.1516013549211059e-13
"cute"",";1.4832976548290714e-13
"return_tensors=""pt"")";4.1522772849064235e-13
outputs;1.274736493191392e-13
=;1.1922372542673417e-13
model(**inputs);2.9281855640084846e-13
last_hidden_states;2.317192340223325e-13
=;1.1805709322675552e-13
outputs.last_hidden_state;1.4737616916705926e-13
***;1.0582424129356515e-13
