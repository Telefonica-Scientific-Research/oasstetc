text;attention
The;9.281036153946092e-12
easiest;5.175931730392782e-12
way;4.815914574936195e-12
to;4.705531586591134e-12
import;7.793397250257543e-12
the;5.282303564422958e-12
BERT;1.2621651527244941e-11
language;5.136672671858977e-12
model;9.722270876519293e-12
into;1.0340167399284621e-11
python;1.1836618567657392e-11
for;6.644955450240431e-12
use;5.416456568897091e-12
with;5.477303591935361e-12
PyTorch;1.2573989492335607e-11
is;8.119724694024975e-12
using;8.338655348794232e-12
the;5.367550579797275e-12
Hugging;6.558968321176227e-12
Face;8.1173008335864e-12
Transformer's;5.862308257046102e-11
library,;9.976675677147248e-12
which;5.2796802575714965e-12
has;5.16702317054376e-12
built;4.094865223196978e-12
in;5.404862521258553e-12
methods;5.116242300885006e-12
for;5.331218131708208e-12
pre-training,;1.2063600984906663e-11
inference,;7.228774258992739e-12
and;4.542891798173948e-12
deploying;5.576223124902577e-12
BERT.;0.9999999984300885
â€˜**;1.3915912641837845e-11
from;7.50803632997559e-12
transformers;9.78394927281111e-12
import;9.615324695056887e-12
AutoTokenizer,;3.0576897058437555e-11
BertModel;1.4148228072895476e-11
import;8.76853486422616e-12
torch;7.782838028883723e-12
tokenizer;8.9087745473573e-12
=;9.343527369410558e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";8.830605182619928e-10
model;6.925014218309412e-12
=;6.364551732319733e-12
"BertModel.from_pretrained(""bert-base-uncased"")";1.2549540186706326e-10
inputs;7.555409053604576e-12
=;6.600824766436888e-12
"tokenizer(""Hello,";2.340231337196301e-11
my;4.831503650051213e-12
dog;4.596449229327045e-12
is;4.359815563448743e-12
"cute"",";5.544744917762389e-12
"return_tensors=""pt"")";2.677617456559329e-11
outputs;6.0922418656337654e-12
=;5.329316009889296e-12
model(**inputs);2.0593789453493883e-11
last_hidden_states;1.210125322345514e-11
=;6.138441826304621e-12
outputs.last_hidden_state;7.983670222891716e-12
***;4.076809059703422e-12
