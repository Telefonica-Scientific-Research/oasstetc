text;attention
The;7.115437686456509e-16
easiest;5.871702255842615e-16
way;5.945796105565768e-16
to;5.471398296235022e-16
import;5.847426379596405e-16
the;5.598352829346052e-16
BERT;1.0602413009036796e-15
language;6.073456428439074e-16
model;5.983256615041443e-16
into;5.956014879029287e-16
python;6.744193026077282e-16
for;5.610509911360403e-16
use;5.3627978332768e-16
with;5.678031741701499e-16
PyTorch;7.387600665714547e-16
is;5.649331869329735e-16
using;5.82937480818309e-16
the;5.592615756311879e-16
Hugging;8.826743605678887e-16
Face;5.898462731762449e-16
Transformer's;8.059801864117513e-15
library,;5.891767521703183e-16
which;5.353533046632931e-16
has;5.295706341929772e-16
built;5.282660775362311e-16
in;5.42945980355652e-16
methods;5.552084849080579e-16
for;5.392365965053697e-16
pre-training,;6.905223145743903e-16
inference,;5.648413733091525e-16
and;5.30517742619962e-16
deploying;5.771435771203435e-16
BERT.;0.9999999999999489
â€˜**;1.7051267795155323e-15
from;6.202219710092418e-16
transformers;6.096450951703342e-16
import;6.244149744962564e-16
AutoTokenizer,;7.714813744752737e-16
BertModel;7.447963043031658e-16
import;5.5783495290836e-16
torch;5.871003210955126e-16
tokenizer;6.180670167967665e-16
=;6.189132962056966e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";3.7198818797011594e-15
model;5.580762472204302e-16
=;5.6705478273513305e-16
"BertModel.from_pretrained(""bert-base-uncased"")";1.8275556862154817e-15
inputs;5.963548699020372e-16
=;5.617079190339177e-16
"tokenizer(""Hello,";1.1420934963757126e-15
my;5.418905890800977e-16
dog;5.514025739868673e-16
is;5.322297510621086e-16
"cute"",";6.027010142456984e-16
"return_tensors=""pt"")";1.0883593097148825e-15
outputs;5.796497598642169e-16
=;5.376794038105627e-16
model(**inputs);8.327592341749337e-16
last_hidden_states;7.578421176713557e-16
=;5.338976493404749e-16
outputs.last_hidden_state;7.421179986025367e-16
***;5.490016931296607e-16
