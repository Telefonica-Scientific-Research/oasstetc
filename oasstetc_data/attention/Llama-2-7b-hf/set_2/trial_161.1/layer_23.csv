text;attention
The;1.207285266175826e-16
easiest;1.1519670592698597e-16
way;1.2449207289817835e-16
to;1.1009382107877363e-16
import;1.2488388484510932e-16
the;1.1044098496265255e-16
BERT;1.683164120295114e-16
language;1.3589057044189345e-16
model;1.3770430352475284e-16
into;1.232534894879437e-16
python;1.2234041127656832e-16
for;1.15342060982431e-16
use;1.1121941897996436e-16
with;1.1216928068156284e-16
PyTorch;1.5808701857966054e-16
is;1.1089016787499623e-16
using;1.131296003249425e-16
the;1.1087829855042352e-16
Hugging;1.5753743363407616e-16
Face;1.188239654772695e-16
Transformer's;1.92518212407963e-15
library,;1.2875230520338405e-16
which;1.1337556382156728e-16
has;1.110944692512167e-16
built;1.1434019032672808e-16
in;1.1596418765994867e-16
methods;1.1853803700440312e-16
for;1.152603479704988e-16
pre-training,;1.6470296202699084e-16
inference,;1.2554121744297124e-16
and;1.0885925810509972e-16
deploying;1.2036087371345293e-16
BERT.;0.9999999999999898
â€˜**;1.288037865899458e-16
from;1.197815647854811e-16
transformers;1.2743567781091902e-16
import;1.1876862363018554e-16
AutoTokenizer,;1.525442538546098e-16
BertModel;1.3675537176053133e-16
import;1.1377722802370565e-16
torch;1.2574328467529363e-16
tokenizer;1.2498682281360908e-16
=;1.127356105459227e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";5.95688002528392e-16
model;1.1176729047810536e-16
=;1.1013317776299632e-16
"BertModel.from_pretrained(""bert-base-uncased"")";3.8519796103784035e-16
inputs;1.153737535725946e-16
=;1.150712625008394e-16
"tokenizer(""Hello,";1.658294329584951e-16
my;1.0981906800474401e-16
dog;1.122711325619018e-16
is;1.1263443495089162e-16
"cute"",";1.1999081665993387e-16
"return_tensors=""pt"")";2.322903466974588e-16
outputs;1.1665797535673027e-16
=;1.0915625507702565e-16
model(**inputs);1.4508955356666738e-16
last_hidden_states;1.7584721739711755e-16
=;1.0694061043727019e-16
outputs.last_hidden_state;1.5355171000157532e-16
***;1.0577511193755572e-16
