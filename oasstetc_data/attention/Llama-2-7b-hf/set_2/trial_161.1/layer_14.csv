text;attention
The;7.806257589211296e-12
easiest;5.3145155122885924e-12
way;4.791622363494148e-12
to;4.683267233543616e-12
import;7.796995457827648e-12
the;5.322469529866584e-12
BERT;2.909031940369638e-11
language;5.82569169944974e-12
model;1.2193777324772411e-11
into;9.465542698649773e-12
python;9.794453254994536e-12
for;6.795523616295151e-12
use;5.225780042513302e-12
with;5.100786490657598e-12
PyTorch;1.2004217032001712e-11
is;6.8143575163029385e-12
using;6.514424257710757e-12
the;5.871237933430763e-12
Hugging;7.563735760054229e-12
Face;1.2617218380346085e-11
Transformer's;1.8078076301460238e-10
library,;7.678009350513468e-12
which;4.962722699983698e-12
has;4.744823024242795e-12
built;4.327157077744747e-12
in;5.067159551044958e-12
methods;5.0462114677738594e-12
for;5.10870137220205e-12
pre-training,;1.2965625168965592e-11
inference,;8.317729906990938e-12
and;4.679051119799979e-12
deploying;6.1446429556004886e-12
BERT.;0.9999999981037702
â€˜**;7.429184847849726e-12
from;6.3616566632501065e-12
transformers;7.854131647324112e-12
import;7.592819593451438e-12
AutoTokenizer,;8.518583828730727e-11
BertModel;2.046316251512884e-11
import;8.388133661855503e-12
torch;7.628774715872185e-12
tokenizer;1.6860752612563072e-11
=;8.90625727582066e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.0412407029149543e-09
model;6.521127647244908e-12
=;5.923674074207703e-12
"BertModel.from_pretrained(""bert-base-uncased"")";8.310149815638457e-11
inputs;7.204872112517147e-12
=;6.103734822837159e-12
"tokenizer(""Hello,";2.598116905772046e-11
my;4.775260750842297e-12
dog;5.433291408431756e-12
is;4.603210083085312e-12
"cute"",";5.807702811479525e-12
"return_tensors=""pt"")";4.1439875374488115e-11
outputs;6.167998354145969e-12
=;5.439682770324007e-12
model(**inputs);1.596038726525069e-11
last_hidden_states;1.3730817084609648e-11
=;5.5657336372500214e-12
outputs.last_hidden_state;9.856993161623043e-12
***;4.286583003865464e-12
