text;attention
The;1.9903633687231027e-16
easiest;1.8491211552606875e-16
way;1.772693804927195e-16
to;1.586089316983192e-16
import;2.576964509131435e-16
the;1.676726238218096e-16
BERT;4.490976699858521e-16
language;2.1019369653868266e-16
model;2.430930518157629e-16
into;1.985363917784162e-16
python;2.1157168297781704e-16
for;1.743972784287392e-16
use;1.6374176377083133e-16
with;1.7309544630920487e-16
PyTorch;2.9825417814896067e-16
is;1.751744166745383e-16
using;1.8073806862639312e-16
the;1.6828963318855262e-16
Hugging;3.1187354978639183e-16
Face;2.0578938669108844e-16
Transformer's;5.174794933294058e-15
library,;1.7785191496095745e-16
which;1.6347252909949633e-16
has;1.6782543524047946e-16
built;1.6426557953834098e-16
in;1.728174047372851e-16
methods;1.641423425198602e-16
for;1.804498615667567e-16
pre-training,;2.4848518518467786e-16
inference,;1.9970148688516228e-16
and;1.5753689312526992e-16
deploying;1.8558688874950197e-16
BERT.;0.9999999999999802
â€˜**;2.0928006291931979e-16
from;1.7560850656114617e-16
transformers;2.2848204156346467e-16
import;1.8742518306576786e-16
AutoTokenizer,;4.0251498441470844e-16
BertModel;2.8345970142642207e-16
import;1.8312889917322718e-16
torch;2.035871099374297e-16
tokenizer;2.3526455229585524e-16
=;1.7135813781611587e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.5580519516767826e-15
model;1.7057575662876363e-16
=;1.6129422844825536e-16
"BertModel.from_pretrained(""bert-base-uncased"")";8.566520936734144e-16
inputs;1.8168609631601833e-16
=;1.6950594666445118e-16
"tokenizer(""Hello,";2.5226216849542225e-16
my;1.581190313912888e-16
dog;1.7525685113665734e-16
is;1.6074575958585562e-16
"cute"",";1.7665564719871858e-16
"return_tensors=""pt"")";3.89440723646448e-16
outputs;1.6857611942725716e-16
=;1.6114066330040632e-16
model(**inputs);2.3165118785997203e-16
last_hidden_states;2.7420689285591766e-16
=;1.5600095915970134e-16
outputs.last_hidden_state;2.303435038603831e-16
***;1.5476822871622147e-16
