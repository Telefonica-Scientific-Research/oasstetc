text;attention
The;1.0434529452015296e-12
easiest;7.326965153665568e-13
way;7.041139752232037e-13
to;6.636351444016274e-13
import;9.768715791572142e-13
the;7.015830207187158e-13
BERT;2.073962292131149e-12
language;7.719086533430197e-13
model;1.4557010621643442e-12
into;1.4892206846126273e-12
python;1.2335250804455865e-12
for;9.706658995335467e-13
use;7.216520947231198e-13
with;6.803126198920309e-13
PyTorch;1.7530975977764088e-12
is;1.2154957678434258e-12
using;8.919233894246285e-13
the;7.875653861736094e-13
Hugging;9.973146860337907e-13
Face;1.2082118310399864e-12
Transformer's;1.0848680327334918e-11
library,;1.240055117924474e-12
which;7.068274783502147e-13
has;6.844851129095441e-13
built;5.813642072413988e-13
in;7.573475749247261e-13
methods;7.196806947466209e-13
for;7.010882420743375e-13
pre-training,;1.6245522107609514e-12
inference,;1.0489414999291097e-12
and;6.322820097177327e-13
deploying;7.2856006166033e-13
BERT.;0.9999999998243354
â€˜**;1.071281219858796e-12
from;9.143249167051173e-13
transformers;9.771201779344494e-13
import;1.2282073485254663e-12
AutoTokenizer,;3.1676155871875847e-12
BertModel;1.6034328901271004e-12
import;1.3168261847728012e-12
torch;1.0389066076468147e-12
tokenizer;1.2498521706071528e-12
=;1.391207300064394e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";8.851108041048113e-11
model;8.84919485521493e-13
=;8.698716706286898e-13
"BertModel.from_pretrained(""bert-base-uncased"")";1.1543706118492093e-11
inputs;8.541113600984105e-13
=;8.77496300705854e-13
"tokenizer(""Hello,";2.7393345749660557e-12
my;6.283461043464694e-13
dog;6.513427105438526e-13
is;6.064629664208355e-13
"cute"",";9.307118361034843e-13
"return_tensors=""pt"")";3.0299653548169696e-12
outputs;7.567028686924488e-13
=;7.853570009615096e-13
model(**inputs);2.6328065564315716e-12
last_hidden_states;1.627048556244513e-12
=;7.294011788803508e-13
outputs.last_hidden_state;1.1184717097354466e-12
***;5.818396863380345e-13
