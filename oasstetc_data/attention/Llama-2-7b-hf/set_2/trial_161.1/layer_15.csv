text;attention
The;2.2008109751596357e-12
easiest;1.4569975838685492e-12
way;1.3959454519004789e-12
to;1.3083308843603682e-12
import;1.9222530897547372e-12
the;1.418197050831421e-12
BERT;6.011577437632332e-12
language;1.6477267497139807e-12
model;3.468941293461778e-12
into;2.2386164177007256e-12
python;3.4615379196712656e-12
for;1.6610487304734458e-12
use;1.4596607358708313e-12
with;1.4445132140596247e-12
PyTorch;4.3185459572970595e-12
is;1.9502138929123977e-12
using;1.7429505791783224e-12
the;1.6221614863962602e-12
Hugging;2.223935848216303e-12
Face;2.260118573549645e-12
Transformer's;6.415259784882817e-11
library,;1.980211202084063e-12
which;1.4020646658576554e-12
has;1.395413021732857e-12
built;1.2372562650600141e-12
in;1.5031314444870265e-12
methods;1.447422564370515e-12
for;1.4712728891720698e-12
pre-training,;3.3405995739888992e-12
inference,;2.0229361011244156e-12
and;1.3826049537054111e-12
deploying;1.4820173717357113e-12
BERT.;0.9999999995226265
â€˜**;2.27672073244502e-12
from;1.7229741138599043e-12
transformers;2.0643428834696482e-12
import;1.9995452487700123e-12
AutoTokenizer,;7.0691553727745545e-12
BertModel;3.450007987208239e-12
import;2.127818841905886e-12
torch;2.2264549271555427e-12
tokenizer;2.9283894530345196e-12
=;1.8504289347091112e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.1922589621116785e-10
model;1.6430593574419165e-12
=;1.5850758371397971e-12
"BertModel.from_pretrained(""bert-base-uncased"")";5.7039955051140575e-11
inputs;1.6915296434832164e-12
=;1.48003387637931e-12
"tokenizer(""Hello,";6.5573736133360914e-12
my;1.3218167423060909e-12
dog;1.4410406459233802e-12
is;1.29313197325763e-12
"cute"",";1.7637271261302231e-12
"return_tensors=""pt"")";8.93768723335968e-12
outputs;1.566993765414801e-12
=;1.4860624692391298e-12
model(**inputs);5.2091567308356624e-12
last_hidden_states;3.628684348526853e-12
=;1.516238285145964e-12
outputs.last_hidden_state;3.0505094069443934e-12
***;1.1878638091300366e-12
