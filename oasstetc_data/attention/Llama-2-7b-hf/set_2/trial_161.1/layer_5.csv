text;attention
The;2.6587322698803547e-16
easiest;1.4467602344382611e-16
way;1.4592438019191363e-16
to;1.3749884961315154e-16
import;1.7731847200642163e-16
the;1.434241353077657e-16
BERT;2.4992640489795863e-16
language;1.6404735002442302e-16
model;1.7700508235630685e-16
into;1.5442257524156435e-16
python;1.6095161029130337e-16
for;1.5206214270200276e-16
use;1.397267629351749e-16
with;1.4496840472795861e-16
PyTorch;2.3407090256602535e-16
is;1.7724570349225666e-16
using;1.4575837770423088e-16
the;1.36253632493368e-16
Hugging;1.7641864026554336e-16
Face;1.7087326132092067e-16
Transformer's;3.059144284402762e-16
library,;1.679755624839247e-16
which;1.3587223646836498e-16
has;1.345225054186978e-16
built;1.3268676043886024e-16
in;1.4124347096091048e-16
methods;1.3987925723349158e-16
for;1.4215477054021735e-16
pre-training,;2.3043496462937567e-16
inference,;1.6217031866559846e-16
and;1.259820850052802e-16
deploying;1.4682316374828347e-16
BERT.;0.9999999999999862
â€˜**;2.691854701453875e-16
from;1.7205610959949854e-16
transformers;1.635588837130407e-16
import;1.6366485655308824e-16
AutoTokenizer,;2.4452718844184033e-16
BertModel;1.7678134334623633e-16
import;1.5269203680633003e-16
torch;1.598835058887879e-16
tokenizer;1.6773055194394449e-16
=;1.8516963851082208e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.4675784384079292e-15
model;1.5719958931986564e-16
=;1.7354561184734328e-16
"BertModel.from_pretrained(""bert-base-uncased"")";8.232084860547588e-16
inputs;1.5038763860013476e-16
=;1.6335064649340265e-16
"tokenizer(""Hello,";3.357278393742085e-16
my;1.3832342834167632e-16
dog;1.3851692694702519e-16
is;1.29656578972016e-16
"cute"",";1.699763682574215e-16
"return_tensors=""pt"")";3.1980758093426464e-16
outputs;1.416150539212554e-16
=;1.448945361566221e-16
model(**inputs);2.569233488776103e-16
last_hidden_states;2.42658589640985e-16
=;1.3877097540270628e-16
outputs.last_hidden_state;2.1243176992364042e-16
***;1.2252075449056465e-16
