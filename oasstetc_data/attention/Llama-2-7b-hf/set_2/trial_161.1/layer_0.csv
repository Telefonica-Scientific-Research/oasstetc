text;attention
The;0.0010030884385933793
easiest;0.00023170913267746052
way;0.00024738938571304623
to;0.0013397727262948638
import;0.0002120473782135309
the;0.0007741077709459135
BERT;0.0012741609040450924
language;7.963557802593496e-05
model;7.728904127884982e-05
into;0.00017208908469566484
python;0.00011307645422184346
for;0.0003486329876768504
use;8.121428430873243e-05
with;0.00022937646381448993
PyTorch;0.0012019829960035052
is;0.00014469910656370288
using;6.034953269849001e-05
the;0.0001945401989391688
Hugging;0.0011363500537368146
Face;4.646235777434982e-05
Transformer's;0.0048599832491474535
library,;0.0005056729097983377
which;5.5473834352067724e-05
has;7.614508436261352e-05
built;4.094316569806459e-05
in;0.00011262833359185914
methods;3.528221827546081e-05
for;9.601657496224077e-05
pre-training,;0.00369094398698431
inference,;0.00028489563246860973
and;8.383501552218675e-05
deploying;0.0001301505656257028
BERT.;0.00036903505836695215
â€˜**;6.932307013906308e-05
from;5.368307571340007e-05
transformers;7.817444029377911e-05
import;3.946759223711596e-05
AutoTokenizer,;0.0005857813613933101
BertModel;5.578553414697237e-05
import;3.573352696377855e-05
torch;5.314030780862486e-05
tokenizer;4.9201897710967756e-05
=;4.4462860566331936e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.9439951180099623
model;2.247055934268948e-05
=;3.3397306084931454e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.034935674300346595
inputs;2.0541521933988798e-05
=;2.6217388250589962e-05
"tokenizer(""Hello,";0.0001706217649090672
my;2.0826499280450666e-05
dog;1.883386097149079e-05
is;2.202864571965717e-05
"cute"",";3.6250761442748054e-05
"return_tensors=""pt"")";0.0001448831568529054
outputs;1.825141509394698e-05
=;2.0245942141571376e-05
model(**inputs);5.4528049190871507e-05
last_hidden_states;2.9556129715594805e-05
=;1.707089475012978e-05
outputs.last_hidden_state;2.502265673935137e-05
***;1.4727964920303986e-05
