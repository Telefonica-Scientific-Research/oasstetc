text;attention
The;2.6608548517273893e-12
easiest;1.785413033073965e-12
way;1.8154170272776911e-12
to;1.6991287423274201e-12
import;2.120902735430554e-12
the;1.9152901916947647e-12
BERT;3.7002288674650135e-12
language;1.991980353249637e-12
model;2.051060059467735e-12
into;2.272355587137022e-12
python;2.0891133844392396e-12
for;1.944632900752559e-12
use;1.7788168994973209e-12
with;2.0873369783899947e-12
PyTorch;3.4868249554072436e-12
is;1.8226039793398566e-12
using;2.182112421647023e-12
the;2.154562646011801e-12
Hugging;5.882590619839502e-12
Face;2.364982344699745e-12
Transformer's;1.1764322977396076e-10
library,;2.1297693972294823e-12
which;1.7754955667165995e-12
has;1.7158468920075463e-12
built;1.677902791527825e-12
in;1.724074339938221e-12
methods;1.719740109578033e-12
for;1.8055914665608554e-12
pre-training,;2.9927069921077304e-12
inference,;2.03635054032726e-12
and;1.6463688573770937e-12
deploying;1.846412650261923e-12
BERT.;0.9999999995973956
â€˜**;3.823621463634259e-12
from;2.806528642563158e-12
transformers;2.7107033006609254e-12
import;2.4550903435226754e-12
AutoTokenizer,;4.369089028800782e-12
BertModel;3.3409973057381176e-12
import;2.1496949256331893e-12
torch;2.4404341672724875e-12
tokenizer;2.6147138701396657e-12
=;2.111924852232943e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.199050417146857e-10
model;1.8808794420803548e-12
=;1.8191375925894146e-12
"BertModel.from_pretrained(""bert-base-uncased"")";2.2977119408162023e-11
inputs;1.885135737980751e-12
=;1.9286638416254908e-12
"tokenizer(""Hello,";4.513820558872874e-12
my;1.8952971438098643e-12
dog;1.8438897328067638e-12
is;1.8988085450145554e-12
"cute"",";2.743838515365036e-12
"return_tensors=""pt"")";8.654583391689066e-12
outputs;1.6739552406793577e-12
=;1.6869999939426596e-12
model(**inputs);3.0479783182722767e-12
last_hidden_states;3.083902384249939e-12
=;1.641566592019825e-12
outputs.last_hidden_state;2.5469479903120645e-12
***;1.6338548495499822e-12
