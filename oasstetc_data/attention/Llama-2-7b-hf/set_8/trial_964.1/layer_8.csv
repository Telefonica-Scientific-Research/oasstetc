text;attention
A;5.335604889094808e-17
good;2.831261052503176e-17
machine;2.7552415724638687e-17
learning;5.880819794140663e-17
model;4.125729479607902e-17
for;3.229009776465187e-17
binary;3.0739223706041403e-17
classification;3.786406680805402e-17
on;2.930685640827509e-17
the;2.294278945990755e-17
Amazon;3.18036965931849e-17
reviews;3.7105998509905654e-17
dataset;3.744986540171247e-17
is;4.192197995014106e-17
a;3.03249204389106e-17
Long;2.8034289153108096e-17
Short-Term;4.2908210617187115e-17
Memory;4.0684071182103694e-17
(LSTM);1.0563421550265977e-16
network;3.2563353071733474e-17
with;3.333939087568469e-17
a;2.4116581780694622e-17
Convolutional;3.697758822624096e-17
Neural;2.9976492434104475e-17
Network;2.6599669745270492e-17
(CNN);3.879993732064416e-17
layer.;0.9999999999999969
This;5.372633438433873e-17
model;3.3501782197021275e-17
can;2.760991915762075e-17
be;2.335187891053247e-17
fine-tuned;4.3094293048141336e-17
or;2.4976219903654184e-17
trained;2.7489241852828875e-17
from;2.2322859607926936e-17
scratch;2.5214366045022996e-17
depending;2.5862233440255233e-17
on;2.1496490804940166e-17
your;2.3094180117143677e-17
preference.;5.1599756855156614e-17
LSTMs.;1.627009890394473e-16
are;3.4309227070548475e-17
great;2.483863213870098e-17
for;2.6614299671221823e-17
handling;2.5028554033690795e-17
sequential;3.0714350201847994e-17
data,;3.0092504574003967e-17
such;2.4489331864718694e-17
as;2.4562758791815974e-17
text;2.6320194583954457e-17
data,;2.5883171560437905e-17
and;2.3853010969101452e-17
the;2.2460821737894433e-17
CNN;3.346038888195616e-17
layer;2.8572898861545324e-17
can;2.5579713186046377e-17
help;2.5412718909351268e-17
extract;2.3737096017322016e-17
relevant;2.2963717940284642e-17
features;2.4282596872792238e-17
from;2.1868998046686706e-17
the;2.112667860492723e-17
text;2.253374632433054e-17
data.;3.711026897582473e-17
For;2.4013289943223388e-17
inference;3.100044396590781e-17
speed,;3.8303996392873356e-17
you;2.547118556295434e-17
can;2.4016159878461092e-17
use;2.4783112320352755e-17
the;2.250682958078925e-17
TensorRT;5.034163591487329e-17
library;2.9168489683314214e-17
by;2.53975232784089e-17
NVIDIA;3.21682579331464e-17
to;2.6524833510258046e-17
optimize;2.5682031268637076e-17
the;2.1681540198934298e-17
model;2.5634940605994542e-17
for;2.3924958521396932e-17
deployment;2.48585791186102e-17
on;2.1568932966105313e-17
GPUs.;3.652150033882695e-17
In;2.418784185708937e-17
terms;2.3471793514781242e-17
of;2.3341334317877063e-17
metrics,;4.206462554893979e-17
you;2.2267626065913353e-17
can;2.2638174064345585e-17
use;2.358235841692469e-17
accuracy,;3.3501578018521633e-17
precision,;2.7733506724167625e-17
and;2.1943134176548738e-17
AUC;2.5528276493399092e-17
to;2.49980105547222e-17
evaluate;2.3563738871755124e-17
the;2.1083181868169108e-17
performance;2.1890534378763137e-17
of;2.1101230922264887e-17
the;2.0649761303574197e-17
model.;2.910597732183708e-17
During;2.3037256591544165e-17
training,;3.1962086273995016e-17
you;2.1580521339829812e-17
can;2.2339243975255657e-17
use;2.3376514609532836e-17
binary;2.368843439749072e-17
cross-entropy;2.8156796034226077e-17
loss;2.5202099033115553e-17
as;2.370616441520773e-17
the;2.1328827637933717e-17
loss;2.1392218268225354e-17
function;2.1285451126354853e-17
to;2.0900576619621673e-17
optimize.;2.0987406781590295e-17
