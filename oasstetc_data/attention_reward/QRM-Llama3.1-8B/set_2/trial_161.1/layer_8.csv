text;attention
The;0.008085964101549593
easiest;0.008253855050589771
way;0.007497001121514359
to;0.006865638537822978
import;0.011877840147225236
the;0.007805544889344355
BERT;0.043971440200581774
language;0.008292187172085018
model;0.012898204368168709
into;0.009268730212868396
python;0.01051024381013348
for;0.007419574565378498
use;0.008007754300866612
with;0.007216958067931343
PyTorch;0.02000738315207134
is;0.008830316482570629
using;0.007870257412625498
the;0.006834601336756348
Hugging;0.010067354363335041
Face;0.016979107108877144
Transformer's;0.021445611167889424
library,;0.014130946715818212
which;0.007501724090013632
has;0.006891989185514216
built;0.006021101781100759
in;0.007456125976894468
methods;0.00793372416639191
for;0.006954435523408505
pre-training,;0.0160347254847344
inference,;0.010793095788628465
and;0.0068502421003521835
deploying;0.0073416378043877455
BERT.;0.015676198540051405
â€˜**;0.011139886942170569
from;0.01008637444213138
transformers;0.009938420902909615
import;0.008668842115085562
AutoTokenizer,;0.05260290857159033
BertModel;0.03260515956739198
import;0.009519399877877938
torch;0.008504471355225671
tokenizer;0.010765241838346722
=;0.009507300383079722
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.24897148486917361
model;0.008605874714953237
=;0.007616715876352998
"BertModel.from_pretrained(""bert-base-uncased"")";0.05156582030828383
inputs;0.013628344391098767
=;0.00751988439629394
"tokenizer(""Hello,";0.02451477358285289
my;0.0066582278068104
dog;0.007297763054842224
is;0.006392876160410441
"cute"",";0.008656770780498889
"return_tensors=""pt"")";0.022124375767881847
outputs;0.008558434194985346
=;0.006625374402156999
model(**inputs);0.013479178079524997
last_hidden_states;0.010144547230362039
=;0.006493840763167942
outputs.last_hidden_state;0.007969981748377227
***;0.006246211148681469
