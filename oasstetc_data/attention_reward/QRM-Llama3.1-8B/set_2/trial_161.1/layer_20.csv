text;attention
The;0.013178720411536669
easiest;0.014364058162004291
way;0.013944403273300536
to;0.014212070638186025
import;0.014616575129832221
the;0.012895633892059923
BERT;0.017496444059523848
language;0.012333648160545163
model;0.013505041417833128
into;0.013799731418093805
python;0.013869928085378812
for;0.013462070957862875
use;0.013832440732235291
with;0.012625375975600546
PyTorch;0.01753331399930513
is;0.013860594753559757
using;0.013915868957899676
the;0.013210085897682019
Hugging;0.014127355854373665
Face;0.015343949051715487
Transformer's;0.018487577794607236
library,;0.015656340860330263
which;0.013187008017776503
has;0.013271945914851653
built;0.012045679773598206
in;0.013503265785761477
methods;0.013540354150129797
for;0.013031737030438568
pre-training,;0.017004910423366607
inference,;0.014360493107076648
and;0.01222589582554444
deploying;0.01240090696259217
BERT.;0.015386971323195924
â€˜**;0.015338064003993478
from;0.015574079076348283
transformers;0.014228572294508546
import;0.015432108965411804
AutoTokenizer,;0.02739313568966836
BertModel;0.021837050970541182
import;0.014622151102833185
torch;0.017147271544615
tokenizer;0.015578584706969191
=;0.013656924583464444
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08341018806363741
model;0.014293809680502995
=;0.013248140668102107
"BertModel.from_pretrained(""bert-base-uncased"")";0.02855935849851406
inputs;0.016054465756202303
=;0.013344436845741482
"tokenizer(""Hello,";0.020911757949464944
my;0.012307966993599628
dog;0.012763781241142432
is;0.012026995562261895
"cute"",";0.013193836009418456
"return_tensors=""pt"")";0.019997269847821117
outputs;0.0146513518084497
=;0.01316457973691426
model(**inputs);0.017771981767244534
last_hidden_states;0.017680248302122763
=;0.013228065008562412
outputs.last_hidden_state;0.014090644378014366
***;0.012262781146131543
