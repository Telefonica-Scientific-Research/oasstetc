text;attention
The;0.008439985879897203
easiest;0.006416444125300465
way;0.005942627242717039
to;0.005986505701778285
import;0.012131429112481074
the;0.00659089898743715
BERT;0.03343382612482922
language;0.006117521730143653
model;0.009023480331749687
into;0.007684387252510871
python;0.0073447385497700845
for;0.006247493863722089
use;0.006823915875506552
with;0.005112912338906315
PyTorch;0.022215881572619806
is;0.0094030962995792
using;0.007106732497906738
the;0.006236915331583925
Hugging;0.008930523143837465
Face;0.011653891839552443
Transformer's;0.01867714350275572
library,;0.013745493834648577
which;0.006203164060549491
has;0.005509055309489197
built;0.004838737989812733
in;0.006417195683312088
methods;0.005776646367397366
for;0.005547077383904397
pre-training,;0.01358124997714063
inference,;0.007929784400327123
and;0.005291263693166131
deploying;0.005369442364448496
BERT.;0.016584856399789044
â€˜**;0.011197280342426676
from;0.008908205228675643
transformers;0.007118259788268496
import;0.007854586375354687
AutoTokenizer,;0.048100946495341586
BertModel;0.017990103808982416
import;0.009639019209103901
torch;0.010395366132519804
tokenizer;0.010373876063414644
=;0.008877148342878331
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3811312506534565
model;0.007214348893331085
=;0.0067360939936798
"BertModel.from_pretrained(""bert-base-uncased"")";0.04049413056343059
inputs;0.007281184088352595
=;0.006132235370591512
"tokenizer(""Hello,";0.0202788602988378
my;0.005221834936185215
dog;0.005343754332047651
is;0.004791406650083611
"cute"",";0.0058084381378473975
"return_tensors=""pt"")";0.015244442215979268
outputs;0.006132224222555461
=;0.005015352301571795
model(**inputs);0.010952364443423119
last_hidden_states;0.008528494101274191
=;0.004889811202667134
outputs.last_hidden_state;0.005454294469998399
***;0.004580368569150335
