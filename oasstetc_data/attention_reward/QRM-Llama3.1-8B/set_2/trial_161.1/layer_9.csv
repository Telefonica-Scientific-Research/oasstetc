text;attention
The;0.010025032237072492
easiest;0.009831251274011266
way;0.008354797385168514
to;0.008012636000516047
import;0.01325276289854223
the;0.009936336334791221
BERT;0.04007430702480495
language;0.00865585915554309
model;0.011867377806037138
into;0.01065905711499226
python;0.012233393095456213
for;0.0084375183497481
use;0.009545162960661569
with;0.00798303769735543
PyTorch;0.02652816887495892
is;0.00994294865627539
using;0.01039630505109031
the;0.0077533774772752615
Hugging;0.014753113115519315
Face;0.02380254008320927
Transformer's;0.025637548614628424
library,;0.017022496354792582
which;0.008372823682853026
has;0.00714579733824625
built;0.006368883379394993
in;0.008941745239046655
methods;0.009268519803528826
for;0.007596510194997025
pre-training,;0.017457744447717388
inference,;0.01090471944772013
and;0.007161472489492781
deploying;0.008131476597165475
BERT.;0.016019628624232455
â€˜**;0.010963526527201412
from;0.009338433004545921
transformers;0.011945023382189902
import;0.01031236224298612
AutoTokenizer,;0.04607509621683168
BertModel;0.02381316031650311
import;0.01019730201088227
torch;0.01271834460388393
tokenizer;0.009292255360959071
=;0.011145006567969438
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.21453512798246963
model;0.008899281839426074
=;0.008078635898963914
"BertModel.from_pretrained(""bert-base-uncased"")";0.04587256831468688
inputs;0.010389387520184885
=;0.007567589490847962
"tokenizer(""Hello,";0.022110526353003236
my;0.006715115056696267
dog;0.0074005515084626374
is;0.0066055744867690915
"cute"",";0.008580280801932445
"return_tensors=""pt"")";0.024280795189062034
outputs;0.00866272838430788
=;0.006972066247489831
model(**inputs);0.013551713949196801
last_hidden_states;0.009975802805018284
=;0.006974195421108617
outputs.last_hidden_state;0.00839151475501557
***;0.0065616849545601565
