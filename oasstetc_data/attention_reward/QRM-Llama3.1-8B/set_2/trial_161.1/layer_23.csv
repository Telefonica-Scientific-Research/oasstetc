text;attention
The;0.014781396260052327
easiest;0.016027906640656293
way;0.014989753398148013
to;0.015070384360704653
import;0.01583666343851622
the;0.01514590978310284
BERT;0.01928669514442573
language;0.013940109373160403
model;0.015050907528293666
into;0.014845391281472595
python;0.014952239628202446
for;0.01489739234228931
use;0.014368487031680628
with;0.014444379195415013
PyTorch;0.017784769375825987
is;0.014327830688298071
using;0.014770616815060091
the;0.014648923718225428
Hugging;0.01565463373596466
Face;0.017673108100599393
Transformer's;0.01683558534055299
library,;0.016503181711102474
which;0.014439211932108635
has;0.014410425118011634
built;0.013618861771489268
in;0.01448067467768088
methods;0.014447872177464433
for;0.014748800847919437
pre-training,;0.016902923396931526
inference,;0.016606008160325113
and;0.013737142136893463
deploying;0.014559750180146661
BERT.;0.016905811495655475
â€˜**;0.015186410713968833
from;0.015213655715550403
transformers;0.014629318506404775
import;0.015124021788642937
AutoTokenizer,;0.022292755393298693
BertModel;0.018132069518155977
import;0.01502890331420842
torch;0.01811435135772168
tokenizer;0.016159161444612227
=;0.014329287802107727
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04322489446961073
model;0.014587925540438694
=;0.014287377523759744
"BertModel.from_pretrained(""bert-base-uncased"")";0.023671734589110524
inputs;0.016568567443580925
=;0.014433008881595264
"tokenizer(""Hello,";0.020435980170265825
my;0.013934419888630055
dog;0.01490423604160973
is;0.013653364995678449
"cute"",";0.015378766296707492
"return_tensors=""pt"")";0.020948586235045454
outputs;0.01449898148272551
=;0.01381595080870644
model(**inputs);0.01633063283670823
last_hidden_states;0.016342232971024946
=;0.01378010967925913
outputs.last_hidden_state;0.014697715528055011
***;0.013601832276440363
