text;attention
The;0.01372086145550809
easiest;0.014108905986025414
way;0.015296105719098188
to;0.013388886382199214
import;0.014937475734598178
the;0.013788481297456528
BERT;0.02096735046390297
language;0.012875701330264697
model;0.013739630531685159
into;0.013514264759804339
python;0.014200351686394204
for;0.013360788295388436
use;0.013401368352303376
with;0.013143613141016131
PyTorch;0.01926103348842491
is;0.01389401223088369
using;0.013476385635238383
the;0.013800821048123947
Hugging;0.018477747122044634
Face;0.01598715456803443
Transformer's;0.017705824731332982
library,;0.014853544407183287
which;0.013350196162531153
has;0.013148098596242426
built;0.0129989854773371
in;0.013811702277581368
methods;0.01360198099343141
for;0.012943967862357922
pre-training,;0.015418412804261981
inference,;0.014217094590555613
and;0.012786152458469834
deploying;0.013152207427248468
BERT.;0.01606693555889419
â€˜**;0.01633019390530242
from;0.015117640049597291
transformers;0.014545592093791878
import;0.015760203792424043
AutoTokenizer,;0.020603813408687166
BertModel;0.02109201646095244
import;0.015414956569029938
torch;0.01739979506966226
tokenizer;0.014077472887499835
=;0.01374063060326627
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06649467994116615
model;0.014188358512512324
=;0.013700371902361803
"BertModel.from_pretrained(""bert-base-uncased"")";0.03240708251561212
inputs;0.01664487879201123
=;0.01413523837553202
"tokenizer(""Hello,";0.02232627923295712
my;0.012689763891831537
dog;0.012893387829013442
is;0.012544307850004061
"cute"",";0.014087668076833136
"return_tensors=""pt"")";0.020795527203186597
outputs;0.015183626800475204
=;0.013082835133401206
model(**inputs);0.017890671654483777
last_hidden_states;0.01614751927435641
=;0.013057165234556087
outputs.last_hidden_state;0.015159557461813147
***;0.013092722901856377
