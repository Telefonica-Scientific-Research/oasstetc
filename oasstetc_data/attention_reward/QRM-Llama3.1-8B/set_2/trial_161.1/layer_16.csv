text;attention
The;0.007488572541718147
easiest;0.007687170287439491
way;0.007640437225317946
to;0.007342066631553259
import;0.00879331806046846
the;0.006590967171108405
BERT;0.017759793068032024
language;0.006465301897037524
model;0.007449134948931621
into;0.006741737290170061
python;0.008037356064387517
for;0.006504608362916457
use;0.006387563812023708
with;0.0059019013949687905
PyTorch;0.02050232501023193
is;0.008491473549462869
using;0.007614673384605875
the;0.007142885172919961
Hugging;0.011713412010443361
Face;0.015854442625238923
Transformer's;0.016665375895730913
library,;0.01076763641025047
which;0.006972420962967724
has;0.006285417160278674
built;0.005882989949167027
in;0.006775743392506193
methods;0.0066680946069408165
for;0.006235164754734199
pre-training,;0.011314017369302978
inference,;0.007759173190750978
and;0.005563144111473217
deploying;0.005774467888333301
BERT.;0.010307779121421086
â€˜**;0.011415867329870375
from;0.01051043421375906
transformers;0.008046530576920652
import;0.007608379903518726
AutoTokenizer,;0.023452152131759947
BertModel;0.02067250271189766
import;0.009263484401986667
torch;0.008993541453343355
tokenizer;0.009684419212145585
=;0.00742366351788242
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3949884399542607
model;0.0076570162371780185
=;0.006740555384787136
"BertModel.from_pretrained(""bert-base-uncased"")";0.05478926719115665
inputs;0.009195253614836833
=;0.006350769419833575
"tokenizer(""Hello,";0.015065724696540488
my;0.005786037370935638
dog;0.006308216013193248
is;0.005434668065266007
"cute"",";0.006676314401892709
"return_tensors=""pt"")";0.019973230012795988
outputs;0.007755268911112214
=;0.005899740889395864
model(**inputs);0.010285026625619545
last_hidden_states;0.011885206733624313
=;0.005701390905877044
outputs.last_hidden_state;0.007716064891384817
***;0.0056402679043908305
