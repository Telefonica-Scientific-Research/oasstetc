text;attention
The;0.012550551888640925
easiest;0.01230514083652678
way;0.012513683224247434
to;0.012357099735988985
import;0.014477592647525907
the;0.012862213714983537
BERT;0.026839623375587682
language;0.011549825900606088
model;0.015147510630584813
into;0.012376877297097768
python;0.013970351341730311
for;0.0123555832920721
use;0.012601012274568776
with;0.011408906783415768
PyTorch;0.02168797651908191
is;0.012078138167892473
using;0.012266137560915897
the;0.012196696351939634
Hugging;0.014874435620592416
Face;0.018285326991327157
Transformer's;0.02109601026602529
library,;0.014955560626531026
which;0.01231745417593503
has;0.012247661861100781
built;0.01110582349952658
in;0.0120268011472293
methods;0.011885463895614798
for;0.011795311247989753
pre-training,;0.01564308308126119
inference,;0.013270211545540941
and;0.011186895219873344
deploying;0.011199128682294219
BERT.;0.01492980839176661
â€˜**;0.012988932069285516
from;0.013627029373665748
transformers;0.01565985674374046
import;0.012801625338424936
AutoTokenizer,;0.03577024819422043
BertModel;0.027797175794781876
import;0.012988172020004027
torch;0.016707320590237495
tokenizer;0.015315572117644137
=;0.012506443072945418
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09663446053942583
model;0.013779984463509042
=;0.012068843527964269
"BertModel.from_pretrained(""bert-base-uncased"")";0.029238400486411926
inputs;0.016644488886985553
=;0.011734086146028016
"tokenizer(""Hello,";0.022967221607506146
my;0.01170948858706384
dog;0.012524991299673605
is;0.010898714989866227
"cute"",";0.012269700043108322
"return_tensors=""pt"")";0.02261606581880472
outputs;0.013479856237110385
=;0.011238010940424634
model(**inputs);0.015344842655309651
last_hidden_states;0.016568990545656997
=;0.011348799669596643
outputs.last_hidden_state;0.013298225533269434
***;0.011108554911319415
