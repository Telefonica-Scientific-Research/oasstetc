text;attention
The;0.014280912626856993
easiest;0.013084719610118275
way;0.012161848807577034
to;0.012168065892896197
import;0.018896186607925667
the;0.012501661385866007
BERT;0.020160486502716478
language;0.012884176360641361
model;0.01272516482878766
into;0.01432510795790085
python;0.013663820331504421
for;0.013798605940424745
use;0.011832834927202659
with;0.011932376712871525
PyTorch;0.01623222454335174
is;0.014074022329498975
using;0.013194199881482748
the;0.01191738551196915
Hugging;0.01449643856836937
Face;0.013824902014355095
Transformer's;0.022604318354288232
library,;0.018307917423766196
which;0.012651883452815838
has;0.0119118146187703
built;0.010911515241495019
in;0.012051453841406656
methods;0.012688288782890984
for;0.012887945275578364
pre-training,;0.02341423601088012
inference,;0.014040003840482467
and;0.011166357570044042
deploying;0.011923098656793994
BERT.;0.018027601433947972
â€˜**;0.017929860394919336
from;0.016471985247428107
transformers;0.014817290722838994
import;0.013922133654791035
AutoTokenizer,;0.02689867515523822
BertModel;0.01733279954652797
import;0.014693555314239864
torch;0.011555851404676519
tokenizer;0.013712704786936246
=;0.01572456358153417
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07994939024488552
model;0.013269678807267421
=;0.014384653721910633
"BertModel.from_pretrained(""bert-base-uncased"")";0.041627484318244104
inputs;0.01496224172048901
=;0.014012204063154959
"tokenizer(""Hello,";0.030531213565578978
my;0.011610352525092882
dog;0.011565902008816444
is;0.010963411932294264
"cute"",";0.012434687218284609
"return_tensors=""pt"")";0.020330012556440256
outputs;0.012255842582794706
=;0.011962172576078848
model(**inputs);0.016708749817943302
last_hidden_states;0.015082707759296276
=;0.011757251074010329
outputs.last_hidden_state;0.01199688843005622
***;0.010794159422753745
