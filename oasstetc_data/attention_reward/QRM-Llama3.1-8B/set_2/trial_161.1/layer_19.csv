text;attention
The;0.012995161757579761
easiest;0.01359013801439811
way;0.013228599964638349
to;0.013149835396634654
import;0.014254830408270813
the;0.0130136354442053
BERT;0.0214815815935908
language;0.012665831609104873
model;0.014026337501401327
into;0.012974048409883154
python;0.014356183584445126
for;0.012934611094523658
use;0.013259431736337573
with;0.012361355732314335
PyTorch;0.019051234168272473
is;0.013322737742643448
using;0.013181186641202339
the;0.012648925680888167
Hugging;0.01581316876516726
Face;0.015267412645199584
Transformer's;0.018566733503590088
library,;0.014924003061833625
which;0.013004688223153172
has;0.012952820841578283
built;0.012069588799876378
in;0.012919090522577673
methods;0.01331515488491447
for;0.012680399516403191
pre-training,;0.01722422520259037
inference,;0.014269917410295386
and;0.012234322395334367
deploying;0.012671445944125795
BERT.;0.015282171698068163
â€˜**;0.014639983360500668
from;0.015019619929868782
transformers;0.015447613942844184
import;0.014696886019728335
AutoTokenizer,;0.025985296304067797
BertModel;0.019250839282408217
import;0.014694380208732656
torch;0.015499621901518806
tokenizer;0.01789409679942248
=;0.014730393373423124
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07851200935855043
model;0.013947250650431297
=;0.013209527043960067
"BertModel.from_pretrained(""bert-base-uncased"")";0.029064850253837413
inputs;0.019851408904949408
=;0.013408226401676215
"tokenizer(""Hello,";0.022860480307250096
my;0.01233959030629642
dog;0.013063252058662211
is;0.012130580367932565
"cute"",";0.013991880706066082
"return_tensors=""pt"")";0.024796093836410484
outputs;0.013943560697236658
=;0.012616163822928387
model(**inputs);0.01667827348493419
last_hidden_states;0.01721849414712102
=;0.012599433832911231
outputs.last_hidden_state;0.013898429091636848
***;0.012320953709651779
