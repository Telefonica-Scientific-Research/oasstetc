text;attention
The;0.011555693277579587
easiest;0.011741245093308096
way;0.012066775822142368
to;0.01169078322940149
import;0.01220929951467657
the;0.011656791448158595
BERT;0.015755198738749768
language;0.011689910488689675
model;0.012098498890291009
into;0.011477182784633746
python;0.01366943547274783
for;0.011255490080172414
use;0.0112921683274247
with;0.011070530611993147
PyTorch;0.016769121924010286
is;0.01192686096085794
using;0.011538497263935386
the;0.011845902503098239
Hugging;0.017501845055626575
Face;0.013745085278916208
Transformer's;0.015870210921839472
library,;0.014961712590039478
which;0.011498257650062732
has;0.011057083763504191
built;0.010719432897128336
in;0.011455914212992805
methods;0.011840962097029074
for;0.01135133593540821
pre-training,;0.015508592953859117
inference,;0.013694219238495278
and;0.010644662061200745
deploying;0.010936850239406349
BERT.;0.015342952593183863
â€˜**;0.01525779127050195
from;0.011978461772671607
transformers;0.011969973156357078
import;0.011974438145655068
AutoTokenizer,;0.025302528430106646
BertModel;0.01777053945595399
import;0.012182162973887677
torch;0.015728362905899927
tokenizer;0.016124594073460376
=;0.011595304677045086
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.17319629707551473
model;0.011922924844456258
=;0.010984648070657682
"BertModel.from_pretrained(""bert-base-uncased"")";0.03873496555514433
inputs;0.013415039513758362
=;0.011966368555517588
"tokenizer(""Hello,";0.02034786472199298
my;0.010831284853226484
dog;0.012174804221773104
is;0.010630044014964212
"cute"",";0.012764631291667392
"return_tensors=""pt"")";0.01924132292706932
outputs;0.011906903268416286
=;0.010628287702077653
model(**inputs);0.015805973372555588
last_hidden_states;0.014379853597178874
=;0.010679299995130244
outputs.last_hidden_state;0.012193512328504396
***;0.010873313308291978
