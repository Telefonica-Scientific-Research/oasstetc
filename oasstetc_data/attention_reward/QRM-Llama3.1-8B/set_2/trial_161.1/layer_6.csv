text;attention
The;0.0138421181726068
easiest;0.012989093596811671
way;0.011406073738728211
to;0.010853433703021617
import;0.017863039451938425
the;0.011422306618591022
BERT;0.01702970022442848
language;0.010031890180050315
model;0.011881307393104546
into;0.01300448355603275
python;0.011567085170989214
for;0.011661344315509413
use;0.010694181586035268
with;0.010577564321074913
PyTorch;0.013602138178325736
is;0.014885426256163164
using;0.011538963913755296
the;0.010253625802049693
Hugging;0.012140824112447008
Face;0.013733512221048334
Transformer's;0.0212132388779913
library,;0.01661339181658084
which;0.010781155618431832
has;0.011108854698475256
built;0.009002252314431662
in;0.011228746685754616
methods;0.01082681801897343
for;0.011221621726369395
pre-training,;0.018161316236280665
inference,;0.012160060642906432
and;0.009526157150181826
deploying;0.010325845462236145
BERT.;0.01713186820553802
â€˜**;0.015584979644935843
from;0.012465090456543789
transformers;0.013204843606642444
import;0.014535486239210938
AutoTokenizer,;0.028363221566991607
BertModel;0.014167027711078816
import;0.016499537813071733
torch;0.010584842628070712
tokenizer;0.012725212748421448
=;0.014412881163183514
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.13892325894024268
model;0.013100147269250363
=;0.011533446519874497
"BertModel.from_pretrained(""bert-base-uncased"")";0.056359532798801425
inputs;0.012674446323075817
=;0.010295866672731331
"tokenizer(""Hello,";0.03300064236058779
my;0.009982614427058813
dog;0.01009351251858872
is;0.009531892978996656
"cute"",";0.0122561796633059
"return_tensors=""pt"")";0.024971733493574105
outputs;0.011927640665329515
=;0.009555063064401393
model(**inputs);0.018509715274553876
last_hidden_states;0.013796086296767445
=;0.009778329514027444
outputs.last_hidden_state;0.01170698068577461
***;0.009180346988043446
