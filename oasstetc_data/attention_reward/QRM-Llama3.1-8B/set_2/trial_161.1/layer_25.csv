text;attention
The;0.013696464410319726
easiest;0.014279263146290825
way;0.014584430828704576
to;0.013949792320498616
import;0.013998392785052367
the;0.014107067482410559
BERT;0.0164960639234946
language;0.013439525878747435
model;0.014104718352213995
into;0.013566811306140042
python;0.014180182943457992
for;0.01371115342136102
use;0.013612152626398597
with;0.013644490011360193
PyTorch;0.017807633965981874
is;0.01483623516642186
using;0.013732724891248826
the;0.014316696086503256
Hugging;0.015871366810382155
Face;0.015971124229898943
Transformer's;0.0163932276126539
library,;0.015883739965253336
which;0.01388833066573265
has;0.013611875858288902
built;0.013412296694784097
in;0.013727738467952832
methods;0.014976936593522845
for;0.013781566506354391
pre-training,;0.01587702925934089
inference,;0.01464142442529074
and;0.01322317366953796
deploying;0.013940885314750177
BERT.;0.018347756155977092
â€˜**;0.017835631605818365
from;0.015271680940326852
transformers;0.014016482428510292
import;0.014588478632951554
AutoTokenizer,;0.02001639130988757
BertModel;0.01699133672770971
import;0.015709268737052192
torch;0.017600368460581398
tokenizer;0.015393175691555858
=;0.014563102737767212
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.05541881613918649
model;0.014453316563695976
=;0.013906718324886741
"BertModel.from_pretrained(""bert-base-uncased"")";0.03736395653374103
inputs;0.015640807407848623
=;0.014346945249855062
"tokenizer(""Hello,";0.0228845823950438
my;0.013519896792172121
dog;0.014428632220802467
is;0.013127237791041356
"cute"",";0.015071909881344929
"return_tensors=""pt"")";0.022213455137682536
outputs;0.014110783568435838
=;0.01330791676327821
model(**inputs);0.016541532095989405
last_hidden_states;0.016801804626541465
=;0.013390390244389531
outputs.last_hidden_state;0.016227025295482658
***;0.013646083920093364
