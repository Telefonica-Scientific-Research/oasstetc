text;attention
The;0.011854928295204336
easiest;0.00502855472027712
way;0.0043311988474603645
to;0.011168985824524559
import;0.007743211843969561
the;0.010365667153609252
BERT;0.009762245179468844
language;0.004445704033565223
model;0.004522790935110798
into;0.005915253811162788
python;0.008073069264246993
for;0.012583138875726117
use;0.00449284267500421
with;0.012809000025736332
PyTorch;0.01119944168023964
is;0.0073034205185527785
using;0.005249714953270883
the;0.009659711744410143
Hugging;0.010533192042607303
Face;0.004150167964364149
Transformer's;0.011277547966258027
library,;0.0268743141334483
which;0.0044106886061161385
has;0.005796436753128221
built;0.004550718692658548
in;0.009734064986028945
methods;0.004391589821334522
for;0.010625903656888784
pre-training,;0.027325519918423465
inference,;0.022989219596288687
and;0.011377028968052325
deploying;0.004720013577104714
BERT.;0.027076125095661086
â€˜**;0.005898963270185024
from;0.0072749163933461085
transformers;0.0051173994631133176
import;0.006025734517019275
AutoTokenizer,;0.02486867084224331
BertModel;0.005028164268409065
import;0.0060420481380571295
torch;0.005633865562986644
tokenizer;0.0053623465943353245
=;0.008517325147503163
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.27929454298826983
model;0.004043989637191697
=;0.008015804994361837
"BertModel.from_pretrained(""bert-base-uncased"")";0.17746235561093718
inputs;0.004159024129822039
=;0.007023123497671142
"tokenizer(""Hello,";0.0349480697469661
my;0.005153062192395915
dog;0.0038936532120457156
is;0.005202880119441023
"cute"",";0.006812375464421812
"return_tensors=""pt"")";0.015247730331788657
outputs;0.003726298931518171
=;0.005911182248158543
model(**inputs);0.008755125107566084
last_hidden_states;0.005109517701755462
=;0.004821018787749461
outputs.last_hidden_state;0.004880353329370497
***;0.003429045611467415
