text;attention
The;0.013944571637677226
easiest;0.014230207136331333
way;0.012847382767787513
to;0.012127747402340494
import;0.014953242127376505
the;0.012356358169060475
BERT;0.014065110772057486
language;0.012892408750105193
model;0.011634211373340624
into;0.012352628562205654
python;0.012021747244057333
for;0.012833238098417346
use;0.012290639414350928
with;0.012074523949648404
PyTorch;0.018108218968978
is;0.012492286309141025
using;0.012373399915011078
the;0.012466466161565062
Hugging;0.014714966301384112
Face;0.012624940035465918
Transformer's;0.01433340859482386
library,;0.015642199442815304
which;0.011884475870040807
has;0.011813506820610845
built;0.011839019616762586
in;0.011693166100668203
methods;0.012261381930666101
for;0.011745832702800576
pre-training,;0.01899991017609609
inference,;0.0138272710413284
and;0.011261164895790593
deploying;0.011845800662586926
BERT.;0.016417841811517776
â€˜**;0.016132935954864862
from;0.0121419487223554
transformers;0.014033941913744607
import;0.013735174867273877
AutoTokenizer,;0.017799543273758836
BertModel;0.013487123818806892
import;0.01166790154626298
torch;0.01218347187381963
tokenizer;0.012666818521777538
=;0.01227356497456323
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.10101476550642732
model;0.010996859715331427
=;0.011867736491206378
"BertModel.from_pretrained(""bert-base-uncased"")";0.08166921779554692
inputs;0.011423416078884072
=;0.011549864294358118
"tokenizer(""Hello,";0.024629973965611247
my;0.011472991093305672
dog;0.011341509651789499
is;0.011118386148491784
"cute"",";0.013308463335683264
"return_tensors=""pt"")";0.032570110805470384
outputs;0.011795044104377336
=;0.011351002727925979
model(**inputs);0.0183533137568158
last_hidden_states;0.015134347226982304
=;0.010988494754463276
outputs.last_hidden_state;0.015549150051324345
***;0.010773652265967219
