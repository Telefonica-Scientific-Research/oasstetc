text;attention
The;0.012993588637758761
easiest;0.013214187633278073
way;0.014247103453444647
to;0.01252032091070511
import;0.01348959337426013
the;0.01290681592808627
BERT;0.01804752091636917
language;0.012163072427247932
model;0.013631047561619394
into;0.012391819668601091
python;0.013994959934878499
for;0.012230473516510217
use;0.012656530680421368
with;0.011906278898521387
PyTorch;0.01880325241841493
is;0.012956786919040396
using;0.012707962137771305
the;0.012968284192385686
Hugging;0.014268362086796077
Face;0.015952466206460757
Transformer's;0.016707028167140387
library,;0.014636739721643656
which;0.012719047758419082
has;0.012270341739660864
built;0.011823169865936648
in;0.012404722327333024
methods;0.013801494226443339
for;0.012209643532020559
pre-training,;0.015262166935454683
inference,;0.013896733762754274
and;0.011322486963693191
deploying;0.011835271268946396
BERT.;0.01604526558809638
â€˜**;0.020054529853225554
from;0.013104383731883907
transformers;0.012976882467650708
import;0.013623798973869552
AutoTokenizer,;0.021823121763468387
BertModel;0.019695980176057244
import;0.012883672508652765
torch;0.015334583069920228
tokenizer;0.013712669145157645
=;0.012328792228724125
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.11254307972622911
model;0.013059057411513485
=;0.011817566941233915
"BertModel.from_pretrained(""bert-base-uncased"")";0.043335518841059815
inputs;0.013873082989135909
=;0.012240847839801977
"tokenizer(""Hello,";0.020984689765769726
my;0.011775278792895532
dog;0.012363816491130799
is;0.01134280124821406
"cute"",";0.013438835335982807
"return_tensors=""pt"")";0.02229862197393888
outputs;0.013678308434804972
=;0.011396457015693221
model(**inputs);0.01699074636282233
last_hidden_states;0.01756791265875222
=;0.011484315969044341
outputs.last_hidden_state;0.01345135896087137
***;0.011834749962381376
