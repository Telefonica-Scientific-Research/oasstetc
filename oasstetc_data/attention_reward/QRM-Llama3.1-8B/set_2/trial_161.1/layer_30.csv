text;attention
The;0.006049858623984749
easiest;0.00613393140264602
way;0.006096039840150339
to;0.005867412131841304
import;0.006851592569012243
the;0.005621130723845409
BERT;0.010911084547599384
language;0.004922383131375965
model;0.005476774846870601
into;0.005777269797629643
python;0.006632645064967326
for;0.0061954701794068095
use;0.005079945433073945
with;0.005282781796673574
PyTorch;0.013737352485762432
is;0.005050578273887348
using;0.00547208915654241
the;0.005822348097841911
Hugging;0.008158885078353169
Face;0.008109656965216386
Transformer's;0.009523166126774636
library,;0.006546426196636456
which;0.0051457973793652065
has;0.005093675877031795
built;0.0046212323522108675
in;0.005084681129257137
methods;0.0050162684941170155
for;0.0050807528823551915
pre-training,;0.009142399918230529
inference,;0.0064742512704509675
and;0.004402132098292725
deploying;0.004719796379850893
BERT.;0.008462193072611703
â€˜**;0.01649912940306671
from;0.006069129949685506
transformers;0.005461658178404352
import;0.007073660280084335
AutoTokenizer,;0.019346052746739048
BertModel;0.011007079999589822
import;0.007467158368525111
torch;0.008093247118040256
tokenizer;0.007192291218419482
=;0.005242242324406721
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5212689567362965
model;0.005587358480148848
=;0.004812681416276079
"BertModel.from_pretrained(""bert-base-uncased"")";0.06383909157090663
inputs;0.006347412691170978
=;0.005244108210713758
"tokenizer(""Hello,";0.013516418541509327
my;0.004848934875849585
dog;0.004808553434046387
is;0.004440121059152226
"cute"",";0.005990854208889658
"return_tensors=""pt"")";0.012311893741852273
outputs;0.004984860259283475
=;0.004330777605207327
model(**inputs);0.008544136407884059
last_hidden_states;0.008130424815177088
=;0.004372683515459219
outputs.last_hidden_state;0.006069395750094718
***;0.004537683769254399
