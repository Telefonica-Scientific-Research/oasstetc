text;attention
The;0.008019321381762087
easiest;0.007632886471679175
way;0.006793225232298536
to;0.007175794532799908
import;0.009691432006898735
the;0.007306537730153572
BERT;0.017474744914674812
language;0.006396676004338742
model;0.007910379033286483
into;0.006901837394717628
python;0.0070328720604779195
for;0.006941580594002968
use;0.006434998523415137
with;0.005911148862749519
PyTorch;0.014994997408785009
is;0.0070650429366707745
using;0.00798294125469393
the;0.007368110488576189
Hugging;0.010886024141668402
Face;0.014501551804531922
Transformer's;0.022505434897600186
library,;0.011436803542452198
which;0.006868170034221111
has;0.0068403729825533225
built;0.0059446860193886055
in;0.006643484564645796
methods;0.006722889121541235
for;0.006396511487758561
pre-training,;0.014549497192728564
inference,;0.007975822767332205
and;0.005487941212870832
deploying;0.005865296864602739
BERT.;0.009131929260478362
â€˜**;0.008281380560433215
from;0.008539348591469433
transformers;0.00873463664904212
import;0.007962311404817658
AutoTokenizer,;0.028183755572080825
BertModel;0.02110755167923851
import;0.007998784406423796
torch;0.009897659475177484
tokenizer;0.009895085873351384
=;0.0068516797751589364
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3844894030741189
model;0.0073569634802634065
=;0.006408466242895958
"BertModel.from_pretrained(""bert-base-uncased"")";0.05574635060713707
inputs;0.0086713668107769
=;0.0063797862770058325
"tokenizer(""Hello,";0.019408623163410552
my;0.005732618167881914
dog;0.006881067816495081
is;0.005575010240265039
"cute"",";0.006553887815451515
"return_tensors=""pt"")";0.02185103410318143
outputs;0.007541755639644002
=;0.005828411347081906
model(**inputs);0.011335558460921111
last_hidden_states;0.013306795730685944
=;0.005729993986391439
outputs.last_hidden_state;0.007409038975036064
***;0.005550731347807687
