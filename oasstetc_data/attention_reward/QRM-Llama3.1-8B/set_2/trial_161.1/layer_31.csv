text;attention
The;0.00509379191648599
easiest;0.005030110207005652
way;0.005106544293904424
to;0.004875722250198931
import;0.005659609863640411
the;0.005399648239229114
BERT;0.007480874747575602
language;0.004303216175526045
model;0.0053605724833949864
into;0.005039332956185191
python;0.005388604043023871
for;0.005509130830912861
use;0.004855415598942917
with;0.00489875692741044
PyTorch;0.011045694294548592
is;0.004556455835824519
using;0.005337548612091243
the;0.005111128240465552
Hugging;0.006022536996670093
Face;0.005543902160304652
Transformer's;0.0087683618973738
library,;0.006352322050860105
which;0.004640755961489272
has;0.004711154766556687
built;0.004329267446087991
in;0.004793300019593854
methods;0.004765146289768404
for;0.004770866288311077
pre-training,;0.008681118426146365
inference,;0.006216489542408781
and;0.004069893758509848
deploying;0.004711131178852404
BERT.;0.008596424627442329
â€˜**;0.009121900997172899
from;0.004937833609324868
transformers;0.005029248903835482
import;0.005970334302833156
AutoTokenizer,;0.01177926468560277
BertModel;0.008091650643576184
import;0.005252829276063876
torch;0.006293553864564009
tokenizer;0.005569844232323281
=;0.004707900639189941
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4675125360418935
model;0.005088043050468294
=;0.00452671418133293
"BertModel.from_pretrained(""bert-base-uncased"")";0.16603278104480174
inputs;0.005685175323362978
=;0.004697034963640267
"tokenizer(""Hello,";0.014948803219397339
my;0.004465203806645701
dog;0.004367124690202065
is;0.004079895600289115
"cute"",";0.006023351198334561
"return_tensors=""pt"")";0.014699521047950734
outputs;0.004362980064589105
=;0.003901300584033507
model(**inputs);0.011288263734349751
last_hidden_states;0.00815144174643143
=;0.004010114478511976
outputs.last_hidden_state;0.007889006272422816
***;0.004491518870113604
