text;attention
The;0.011028731138948403
easiest;0.010634605153719556
way;0.011634171428220113
to;0.011862774001940102
import;0.011621426896380566
the;0.011406233951524822
BERT;0.015884692661152843
language;0.010053820754268042
model;0.011645513137515877
into;0.012405474440002905
python;0.011459297711370947
for;0.011385127732982584
use;0.01021566651888582
with;0.010592655433207431
PyTorch;0.016738701906045157
is;0.01071960694924878
using;0.010815332042579465
the;0.010902140158151155
Hugging;0.012753175774710704
Face;0.014227536572271044
Transformer's;0.01811831452247761
library,;0.012687848887706443
which;0.010491425461270049
has;0.010764621441660049
built;0.009781218701802954
in;0.01073920974959392
methods;0.010533118211673556
for;0.011280373990975923
pre-training,;0.018880256550860005
inference,;0.012917575650280455
and;0.009443640005543269
deploying;0.010060746474681206
BERT.;0.013773832001725793
â€˜**;0.013400214285248712
from;0.01078428952535133
transformers;0.011096795848306357
import;0.01125994730330955
AutoTokenizer,;0.018949546332174584
BertModel;0.015289138612168953
import;0.010403647438318894
torch;0.013471654816900845
tokenizer;0.011362563045689255
=;0.0103405655800042
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.19875403481313253
model;0.010601319970872743
=;0.009973202179732956
"BertModel.from_pretrained(""bert-base-uncased"")";0.06266601578346762
inputs;0.011372048325410944
=;0.01038535847713033
"tokenizer(""Hello,";0.024329377696597394
my;0.010054846430514236
dog;0.010860330976816804
is;0.009711489922526104
"cute"",";0.012328418426420256
"return_tensors=""pt"")";0.021164422500176538
outputs;0.01060164065883786
=;0.009323514721450283
model(**inputs);0.017135425871258044
last_hidden_states;0.014787985454504372
=;0.00949214451671644
outputs.last_hidden_state;0.012607335863988529
***;0.010033858609595853
