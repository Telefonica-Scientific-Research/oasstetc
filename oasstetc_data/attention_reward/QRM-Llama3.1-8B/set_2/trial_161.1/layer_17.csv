text;attention
The;0.009901444886157589
easiest;0.010232761030854505
way;0.009824992053524523
to;0.009912561997645406
import;0.01322312891550975
the;0.010109173884103427
BERT;0.024638766716969494
language;0.008924079513381974
model;0.010665616124800706
into;0.00982234835681652
python;0.011067264025918643
for;0.009132423669828416
use;0.010115915091545925
with;0.009012284255191023
PyTorch;0.021994866296291944
is;0.010389098096648878
using;0.011480679588200815
the;0.00985495526469594
Hugging;0.01218822466786112
Face;0.01653035988576697
Transformer's;0.019078018287604716
library,;0.012605614930828625
which;0.009210404224560643
has;0.009142950772984451
built;0.007985793084842301
in;0.00987326686364457
methods;0.009800956232693121
for;0.009588744866033883
pre-training,;0.014630009446122907
inference,;0.01163360759636503
and;0.00812572833678674
deploying;0.009003449524457169
BERT.;0.01163019414507857
â€˜**;0.011673169319969614
from;0.013497063460247349
transformers;0.011939095934922938
import;0.011503984183866653
AutoTokenizer,;0.038454203683281306
BertModel;0.025160387209011904
import;0.013363266559489263
torch;0.01378083187009644
tokenizer;0.012538814488951224
=;0.010112432718488015
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.23251363455469404
model;0.01002581391976917
=;0.008996134505263629
"BertModel.from_pretrained(""bert-base-uncased"")";0.0371295433627282
inputs;0.011864981274829219
=;0.008665306651728837
"tokenizer(""Hello,";0.019813438403913444
my;0.008143539248821708
dog;0.008713638473923294
is;0.007853649042789937
"cute"",";0.009070779346512629
"return_tensors=""pt"")";0.02121602288090554
outputs;0.009738365503916145
=;0.008077409367051468
model(**inputs);0.013230169549755264
last_hidden_states;0.014113562020431087
=;0.008135850133909963
outputs.last_hidden_state;0.011215020001254275
***;0.008130209695761437
