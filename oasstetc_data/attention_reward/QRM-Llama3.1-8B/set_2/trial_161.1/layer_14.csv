text;attention
The;0.008387109944962126
easiest;0.005996904982474743
way;0.005825290592439767
to;0.006199594999117913
import;0.008715351210343263
the;0.006085106654124623
BERT;0.022728668858242658
language;0.00525815103483713
model;0.007499187742114311
into;0.0057482526386204245
python;0.006049559592198665
for;0.005473689448625786
use;0.005423699959940341
with;0.005055733006233347
PyTorch;0.019046258745085205
is;0.006735239553709798
using;0.0065717011125796
the;0.00668330251780408
Hugging;0.00913998849659712
Face;0.013518259732160807
Transformer's;0.019182242516425443
library,;0.009454278478027362
which;0.0054927967019105725
has;0.005303367681723342
built;0.004779712858548317
in;0.005208208674518889
methods;0.00492911494481691
for;0.004930064738818154
pre-training,;0.014461201108760247
inference,;0.007092868670261644
and;0.004668023820390372
deploying;0.0049304857771997074
BERT.;0.009963684108168546
â€˜**;0.006867142756340654
from;0.007984076728615128
transformers;0.006034702961937275
import;0.007063905477163542
AutoTokenizer,;0.02876984870169377
BertModel;0.02675480978263509
import;0.00806729433893752
torch;0.009632589636202131
tokenizer;0.0075862282450506555
=;0.0066766158765049705
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4437645611264958
model;0.006551153375401119
=;0.005433961275696307
"BertModel.from_pretrained(""bert-base-uncased"")";0.043570117348702975
inputs;0.00815525254668209
=;0.005316365239362589
"tokenizer(""Hello,";0.018267721279323898
my;0.004773572090078869
dog;0.005201501581407153
is;0.004548299039749964
"cute"",";0.005515381656627705
"return_tensors=""pt"")";0.01886629697812562
outputs;0.006676658657053673
=;0.005077560389493401
model(**inputs);0.00945924875291986
last_hidden_states;0.011202790671976357
=;0.004984638453719996
outputs.last_hidden_state;0.005866025842988044
***;0.004794578287332779
