text;attention
The;0.011594948305408387
easiest;0.011807976283605616
way;0.012139595673922962
to;0.012634291562896171
import;0.012375168296746905
the;0.012526544207987744
BERT;0.016447619164403304
language;0.011191912773214033
model;0.012454959328466839
into;0.011917956300108355
python;0.013082604495363867
for;0.01180803276451181
use;0.01157264804451556
with;0.011381689039635365
PyTorch;0.020839413807873262
is;0.011789322291380579
using;0.011540387941106239
the;0.012280478234564327
Hugging;0.014322102133409184
Face;0.014922010014413466
Transformer's;0.015515671176790952
library,;0.012435566943835994
which;0.011673111431279444
has;0.011300728580122673
built;0.010633122360384556
in;0.011424788046410806
methods;0.01108385935996101
for;0.011734879292651145
pre-training,;0.016623900896079565
inference,;0.012988247493037793
and;0.010700955219714542
deploying;0.011689254397034457
BERT.;0.014984950298526846
â€˜**;0.015190990862841525
from;0.011637112861779746
transformers;0.012777049994140612
import;0.012482917264057222
AutoTokenizer,;0.0198723283690677
BertModel;0.01644492855649328
import;0.012111488200896685
torch;0.014718779766316343
tokenizer;0.012671943068139251
=;0.01117173000755915
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.16079236507991992
model;0.011135631622849966
=;0.011024205510211249
"BertModel.from_pretrained(""bert-base-uncased"")";0.049585931209696024
inputs;0.015378282810508011
=;0.01165093340962784
"tokenizer(""Hello,";0.02149570194525351
my;0.010910715328256881
dog;0.01230757298535251
is;0.010892486153724847
"cute"",";0.01239530040614675
"return_tensors=""pt"")";0.025886963839722266
outputs;0.011707145219998289
=;0.010564678538561502
model(**inputs);0.01383143730630214
last_hidden_states;0.015252668926195808
=;0.010626758860902003
outputs.last_hidden_state;0.0133252340720503
***;0.01073802166406497
