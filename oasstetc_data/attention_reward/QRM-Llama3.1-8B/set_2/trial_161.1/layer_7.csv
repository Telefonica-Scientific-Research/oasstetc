text;attention
The;0.00987404993004528
easiest;0.008703246083683976
way;0.008644208906312177
to;0.00838977786981281
import;0.013092827188954596
the;0.009483125655484814
BERT;0.0312516182332858
language;0.008058581607700272
model;0.01613534866298732
into;0.011356435127618247
python;0.012136246902882668
for;0.009067624777446257
use;0.008278137758744931
with;0.008116234798434227
PyTorch;0.018190639752000175
is;0.010778421757793193
using;0.009553130933143899
the;0.00822527387036939
Hugging;0.010074804677703428
Face;0.012709656423015758
Transformer's;0.02324582702520374
library,;0.01726933223324163
which;0.009588864647765923
has;0.008620184999129704
built;0.007107766921699788
in;0.008806091497061944
methods;0.00880195271020645
for;0.008474641600250339
pre-training,;0.01455911239980335
inference,;0.010568669035328099
and;0.007537982218882559
deploying;0.008063620271297676
BERT.;0.01902478288323722
â€˜**;0.01337590608429075
from;0.011474598151844798
transformers;0.011830362122621178
import;0.010711851591960923
AutoTokenizer,;0.027887733089142077
BertModel;0.01658793222614707
import;0.013020536763028535
torch;0.01108034990380722
tokenizer;0.011082807014194224
=;0.013238249004912259
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.21663437911395186
model;0.0104940944149034
=;0.009241363645134528
"BertModel.from_pretrained(""bert-base-uncased"")";0.05294184835570341
inputs;0.01226415018956727
=;0.008819710946853647
"tokenizer(""Hello,";0.04104180245251428
my;0.0077757266754503555
dog;0.00820308115306965
is;0.007465058420143901
"cute"",";0.009366713853500448
"return_tensors=""pt"")";0.01964284161507057
outputs;0.010710834547804282
=;0.00783170961273649
model(**inputs);0.016249271988182563
last_hidden_states;0.012684153024677439
=;0.007830063943457612
outputs.last_hidden_state;0.00949469692046604
***;0.007229953814335505
