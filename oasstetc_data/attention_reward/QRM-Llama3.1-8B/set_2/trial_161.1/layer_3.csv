text;attention
The;0.014954390760328963
easiest;0.016522141830498358
way;0.014736788826013229
to;0.013975951076276779
import;0.017041160128190104
the;0.013997382547289624
BERT;0.017377826476605192
language;0.01409473695985155
model;0.014476021840918448
into;0.014266020524029217
python;0.014371057735338219
for;0.014388465231619123
use;0.013946428839001816
with;0.013769915268994128
PyTorch;0.015870630054465505
is;0.014323556719716756
using;0.014074524095511373
the;0.013393853567249904
Hugging;0.015622685179418859
Face;0.017010742255980697
Transformer's;0.019183554296024224
library,;0.017799061819240954
which;0.013907754329848756
has;0.013715540132703125
built;0.0138944178865697
in;0.014134500197085705
methods;0.01544790355341276
for;0.013932015075229988
pre-training,;0.017746586249290788
inference,;0.014787143860389124
and;0.013120125920906387
deploying;0.01384919736268967
BERT.;0.01570569732588823
â€˜**;0.01693553752638732
from;0.015575749162840356
transformers;0.01976321532777142
import;0.016018774554760442
AutoTokenizer,;0.019322468343251145
BertModel;0.015593963384485777
import;0.014938278247011043
torch;0.014099835049879174
tokenizer;0.014666393195029765
=;0.01534912205680688
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0479612870118066
model;0.013833883753514993
=;0.014558543307924669
"BertModel.from_pretrained(""bert-base-uncased"")";0.035712095574301946
inputs;0.014332173012565593
=;0.014043773415406496
"tokenizer(""Hello,";0.023424131321668686
my;0.01335093597889372
dog;0.01486260437934083
is;0.013041569910906138
"cute"",";0.014061601635576623
"return_tensors=""pt"")";0.02178529304075941
outputs;0.013891396630126897
=;0.013556256977604247
model(**inputs);0.016624700884467995
last_hidden_states;0.016176325548019004
=;0.01330749546529084
outputs.last_hidden_state;0.014556444626675704
***;0.013218372750348954
