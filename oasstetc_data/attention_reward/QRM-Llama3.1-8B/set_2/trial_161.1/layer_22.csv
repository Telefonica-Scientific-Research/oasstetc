text;attention
The;0.014047933833477412
easiest;0.014483221387309883
way;0.013814109878432153
to;0.013619099436461009
import;0.015125793009845338
the;0.013897726168216067
BERT;0.019505285924504397
language;0.012869268840146024
model;0.014689702792696527
into;0.013318097740130255
python;0.014484455048770823
for;0.013280467055424506
use;0.0137038183367258
with;0.012900913288839008
PyTorch;0.018350540024804334
is;0.013753654787092956
using;0.013836054859724976
the;0.01358842671152546
Hugging;0.01632296131833807
Face;0.015510866213938544
Transformer's;0.018452944681218043
library,;0.01605414419876717
which;0.013499654642833392
has;0.01365284083815921
built;0.01231092744695072
in;0.01346110226336171
methods;0.01362904694403008
for;0.013182149298013833
pre-training,;0.01627917628732488
inference,;0.015468497746172323
and;0.0124174953903666
deploying;0.012650522513313738
BERT.;0.017171656009627972
â€˜**;0.019253010922060807
from;0.014160962823066822
transformers;0.013966873006541694
import;0.01418396437031801
AutoTokenizer,;0.023197931958571432
BertModel;0.020321187031270938
import;0.014624257100488244
torch;0.016555164684031023
tokenizer;0.016257563064638485
=;0.013760952369738687
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06034157894121681
model;0.014425055657975519
=;0.0130775494905153
"BertModel.from_pretrained(""bert-base-uncased"")";0.03073383989358597
inputs;0.01717039860910161
=;0.01347630644054067
"tokenizer(""Hello,";0.022347459302169147
my;0.01243644456830338
dog;0.012812540105150401
is;0.012092184282946034
"cute"",";0.014127358665258506
"return_tensors=""pt"")";0.02500176365895596
outputs;0.015129121061632576
=;0.012944160576143933
model(**inputs);0.01719695828353987
last_hidden_states;0.020219117404304526
=;0.012837303451208181
outputs.last_hidden_state;0.015340780243676758
***;0.01267565711650554
