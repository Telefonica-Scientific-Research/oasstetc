text;attention
The;0.013563931764267038
easiest;0.013423880553545556
way;0.012449239056046304
to;0.01230261737942778
import;0.01667227723808874
the;0.013157747235992846
BERT;0.01712166108350944
language;0.012674295194952561
model;0.01241625070814152
into;0.01380954499883928
python;0.013338866503525424
for;0.01324399289332749
use;0.012652575653947104
with;0.01280264581535541
PyTorch;0.015168433849581874
is;0.014591966387849258
using;0.013194823640613276
the;0.012056520340097101
Hugging;0.01485852855939499
Face;0.01481953383994642
Transformer's;0.019356785729135073
library,;0.016358376469320663
which;0.012561615563242642
has;0.012169364475717809
built;0.011716523725863588
in;0.013009386929812131
methods;0.013059272429545035
for;0.01236559945318375
pre-training,;0.01847326728142073
inference,;0.014368577929994138
and;0.011671365241738533
deploying;0.012530644938734986
BERT.;0.01693240742525341
â€˜**;0.01815231635269375
from;0.014711490133039323
transformers;0.015644123405618767
import;0.013868244594402794
AutoTokenizer,;0.02198460509200271
BertModel;0.015367496266759489
import;0.014030021373984897
torch;0.012125746258954049
tokenizer;0.01408310663179486
=;0.013969325567609901
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0865028384633225
model;0.014427544387777257
=;0.013401558026281861
"BertModel.from_pretrained(""bert-base-uncased"")";0.04935153399956277
inputs;0.015636328563681717
=;0.01291694549767776
"tokenizer(""Hello,";0.025118761275767783
my;0.011897324187588059
dog;0.01213942190533571
is;0.011479430648172135
"cute"",";0.013583155812531238
"return_tensors=""pt"")";0.0239566331200408
outputs;0.013480759227242739
=;0.011864687447958069
model(**inputs);0.01670375920697459
last_hidden_states;0.017535467761027133
=;0.011753392389354857
outputs.last_hidden_state;0.013856153644512531
***;0.011565308468918104
