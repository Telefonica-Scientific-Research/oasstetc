text;attention
The;0.015541756152559014
easiest;0.016230528563348248
way;0.015046950132779402
to;0.014616673133640557
import;0.016738935273045596
the;0.014842764708233088
BERT;0.016212251999075743
language;0.01493271799664571
model;0.014713776852240582
into;0.015215261972655797
python;0.014517456581329776
for;0.014872993942476388
use;0.014885168788266486
with;0.014596900314551497
PyTorch;0.015634339849319695
is;0.015552161972185686
using;0.015500534591817517
the;0.014381558643224922
Hugging;0.01597298508133333
Face;0.015110100269570732
Transformer's;0.01582079375090211
library,;0.01618826520871824
which;0.014736656614486612
has;0.014541225365112773
built;0.015033344184217248
in;0.015045663854663141
methods;0.014878435052637978
for;0.014556788143082878
pre-training,;0.01858965254876782
inference,;0.015267118161882472
and;0.014015564694988818
deploying;0.014534873228460622
BERT.;0.01623344062244646
â€˜**;0.01810707685273936
from;0.01580289597764614
transformers;0.016196895727344802
import;0.015999251252923206
AutoTokenizer,;0.01913329107040418
BertModel;0.01551711089285221
import;0.015334450624166062
torch;0.014563807839796722
tokenizer;0.015197304389461904
=;0.01663436764374935
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04054430982226261
model;0.015967158797834325
=;0.0160819714478471
"BertModel.from_pretrained(""bert-base-uncased"")";0.02478880107434749
inputs;0.015345635489604447
=;0.016704360929796463
"tokenizer(""Hello,";0.022525260124102855
my;0.014433720125988526
dog;0.01410468527727846
is;0.013898169323603604
"cute"",";0.014839592078989597
"return_tensors=""pt"")";0.01914947748594236
outputs;0.01456599868921605
=;0.015027024837717479
model(**inputs);0.016483433580637145
last_hidden_states;0.01581631587620904
=;0.014164278962521041
outputs.last_hidden_state;0.014663120646491392
***;0.013852594909859234
