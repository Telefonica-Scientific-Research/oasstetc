text;attention
The;0.009577890692893594
easiest;0.009018868617443557
way;0.008515546950520089
to;0.008949093939440099
import;0.013209657408788343
the;0.009584253340304813
BERT;0.048368417636085514
language;0.008352691576245803
model;0.01149624255246363
into;0.010383526713965913
python;0.010227584079717775
for;0.008101157330120842
use;0.00924892374443694
with;0.007905764436370942
PyTorch;0.019012090796130227
is;0.010640179827415415
using;0.009861737930552469
the;0.008498605025890015
Hugging;0.013258617390870238
Face;0.015213693847481974
Transformer's;0.020458574381508225
library,;0.015128401996138545
which;0.008005918193624421
has;0.007510801862075422
built;0.0069595397606290145
in;0.009014348010493976
methods;0.008491422561082854
for;0.00812029877007881
pre-training,;0.016203797763036747
inference,;0.012431586972487186
and;0.007563186049777704
deploying;0.007459289779511274
BERT.;0.015467081244181013
â€˜**;0.010339946222813198
from;0.011241846541700623
transformers;0.009557710582685987
import;0.010252356334429268
AutoTokenizer,;0.06222832986655029
BertModel;0.02391937419757696
import;0.010763835217150164
torch;0.010321295070298544
tokenizer;0.010349516378139523
=;0.01072331508394314
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.21337759874641224
model;0.010029263735423845
=;0.008637721883050503
"BertModel.from_pretrained(""bert-base-uncased"")";0.04017699382278886
inputs;0.011578716547572178
=;0.008935854698328886
"tokenizer(""Hello,";0.029304786185656108
my;0.007689143400548541
dog;0.008487667667035362
is;0.007236093896984429
"cute"",";0.008682306837918641
"return_tensors=""pt"")";0.018049299929280165
outputs;0.008670760945631361
=;0.007405454409127931
model(**inputs);0.014203985663171279
last_hidden_states;0.012565746644631934
=;0.0074481159243320145
outputs.last_hidden_state;0.008502585464302756
***;0.007081586920751992
