text;attention
The;0.013411365904533014
easiest;0.013772475202203507
way;0.013259806263041227
to;0.013524152765627621
import;0.014384166712606353
the;0.013175555454964438
BERT;0.02115154418510793
language;0.012644915484687434
model;0.013477093764640293
into;0.01414857462378093
python;0.014126783358060589
for;0.013853579869792108
use;0.013686142429353648
with;0.013219026190752964
PyTorch;0.020712867924111724
is;0.013265193763087189
using;0.013687253128471771
the;0.013298391466594086
Hugging;0.015137651369952047
Face;0.017122343168373536
Transformer's;0.020692985560686973
library,;0.015679246766727474
which;0.01410065267083961
has;0.01427025732400702
built;0.011950879679747822
in;0.01353790883959422
methods;0.01333288173234271
for;0.013915658367920134
pre-training,;0.018497181106199474
inference,;0.015159662098087038
and;0.012304273614909107
deploying;0.013060055594292177
BERT.;0.016490902243274524
â€˜**;0.019474215813694545
from;0.01395567895239943
transformers;0.013096162169426571
import;0.013462979393466655
AutoTokenizer,;0.021137900236759613
BertModel;0.01779569015117864
import;0.014548529020950747
torch;0.015599685429911116
tokenizer;0.014653019759857068
=;0.013660777952551611
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06723065148206935
model;0.013068710237142985
=;0.012968074785017237
"BertModel.from_pretrained(""bert-base-uncased"")";0.038004458600242826
inputs;0.014993985069132609
=;0.013308824597323729
"tokenizer(""Hello,";0.024199498361074885
my;0.012087966232927948
dog;0.01282279977658421
is;0.012103422702850431
"cute"",";0.013603520466555328
"return_tensors=""pt"")";0.021960273389827877
outputs;0.013867121356885054
=;0.012416896506495904
model(**inputs);0.01774938836690372
last_hidden_states;0.017687015792754966
=;0.012424605146071958
outputs.last_hidden_state;0.01538742495134656
***;0.0126792946702257
