text;attention
The;0.005126372144622418
easiest;0.004786190567700362
way;0.004308442147221078
to;0.004275249047985877
import;0.0077457005280140686
the;0.004771748176263862
BERT;0.017852408052452477
language;0.003815825703830421
model;0.005743215362626113
into;0.004698680875132244
python;0.005299101799148381
for;0.004092572313993493
use;0.0040793212376992695
with;0.0035310121849843313
PyTorch;0.013995485179951098
is;0.0066231253451822176
using;0.004454995176570495
the;0.003824641247086929
Hugging;0.007437798589963071
Face;0.007236036319766789
Transformer's;0.01685193850181742
library,;0.007398911404010925
which;0.0040062051478408
has;0.0036676935111579476
built;0.003260265563476056
in;0.0041110052796229125
methods;0.00367938476075716
for;0.003577695945650503
pre-training,;0.012737134357762471
inference,;0.005919621419426806
and;0.0036448068484443226
deploying;0.003562516774785551
BERT.;0.010038253829295173
â€˜**;0.00755626812571161
from;0.008034562715888503
transformers;0.005250605051567907
import;0.005937127235722298
AutoTokenizer,;0.023500831431558736
BertModel;0.011893474411583341
import;0.005721671082008717
torch;0.004727428448054978
tokenizer;0.006049935669152545
=;0.005635626248105967
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5183315988893746
model;0.005272193509289549
=;0.004467613873485872
"BertModel.from_pretrained(""bert-base-uncased"")";0.0839654120460245
inputs;0.0055407686218126315
=;0.004260854716011574
"tokenizer(""Hello,";0.022937467372852693
my;0.003451040640897913
dog;0.0037651139615765493
is;0.0031778153361532143
"cute"",";0.004131535541865819
"return_tensors=""pt"")";0.014847341787581366
outputs;0.004385255729572949
=;0.0034359901798933926
model(**inputs);0.01031495303238897
last_hidden_states;0.00647143500348976
=;0.0034695820640160383
outputs.last_hidden_state;0.004213887647365173
***;0.003099254282749904
