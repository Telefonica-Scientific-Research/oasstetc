text;attention
The;0.013323843569935089
easiest;0.007739350672613794
way;0.007197199061393224
to;0.008504804459664119
import;0.013082013287110292
the;0.007961993753004012
BERT;0.047721640383261106
language;0.007778641997254038
model;0.012263637977348597
into;0.007627586793112808
python;0.007907383092008061
for;0.007623008582919648
use;0.008270510991444152
with;0.006962697856348865
PyTorch;0.03702699216092141
is;0.00884960961300557
using;0.008499329649168238
the;0.008846225171321569
Hugging;0.01153335383014231
Face;0.016374040560734923
Transformer's;0.03286537577579867
library,;0.010808856284295409
which;0.006685793797349307
has;0.006749011958434814
built;0.006496006691748313
in;0.007120531009441383
methods;0.006899689975351358
for;0.006721798931136271
pre-training,;0.015207079611303428
inference,;0.010258284794173103
and;0.006960777044072705
deploying;0.006493555259994592
BERT.;0.010949750242813699
â€˜**;0.009424642505240102
from;0.010082210897522065
transformers;0.008407239966901521
import;0.009275404180021323
AutoTokenizer,;0.032756327206890014
BertModel;0.04829558332028536
import;0.009852761485362006
torch;0.012626845829645713
tokenizer;0.009481600643539537
=;0.009073675918452177
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.23544232765014242
model;0.011092745559910916
=;0.007167781111548792
"BertModel.from_pretrained(""bert-base-uncased"")";0.04695084444190714
inputs;0.011135115290924256
=;0.0075858509099297556
"tokenizer(""Hello,";0.022081693074480307
my;0.006523115733418197
dog;0.007075686344822452
is;0.006081473112510363
"cute"",";0.007681871530744724
"return_tensors=""pt"")";0.025097795031279423
outputs;0.008791303399622715
=;0.006537711755657863
model(**inputs);0.01073533333026137
last_hidden_states;0.011508568332452692
=;0.00643886388172483
outputs.last_hidden_state;0.0073837715353680085
***;0.0061014811808090055
