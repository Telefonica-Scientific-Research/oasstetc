text;attention
The;0.013020019585079038
easiest;0.011902927076848858
way;0.01137171200615556
to;0.010560639128507122
import;0.018644946260251996
the;0.011146158042750794
BERT;0.027469824553805262
language;0.0106760654381118
model;0.012734886478908735
into;0.012025364201925962
python;0.01319174281678862
for;0.011900569033034842
use;0.01174389249536948
with;0.01116093694893694
PyTorch;0.01573491245577479
is;0.012851486167748545
using;0.011690179084101464
the;0.01100791718988262
Hugging;0.012049417230555604
Face;0.01875330519860665
Transformer's;0.06321053925347232
library,;0.014262057748860541
which;0.010554553816470753
has;0.010471642705532
built;0.009926192380803424
in;0.011353709148707033
methods;0.011162717283010186
for;0.010928230055902594
pre-training,;0.021240962408759934
inference,;0.013274993589926315
and;0.010202851375822716
deploying;0.011683138149485546
BERT.;0.015779673310550156
â€˜**;0.013569224073053627
from;0.01470516465684745
transformers;0.01349662752227675
import;0.012446685446754173
AutoTokenizer,;0.025030440224395788
BertModel;0.014879870927455347
import;0.01659851596221358
torch;0.012511735423938517
tokenizer;0.01566499489242026
=;0.013971321353501681
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09487593612070336
model;0.012232391837916011
=;0.012163448159688185
"BertModel.from_pretrained(""bert-base-uncased"")";0.04573074267199745
inputs;0.013871969208455029
=;0.011662413886960418
"tokenizer(""Hello,";0.023698089710774868
my;0.010260737232077074
dog;0.010757646434909318
is;0.010332488486633689
"cute"",";0.011970896670554272
"return_tensors=""pt"")";0.01777168930746616
outputs;0.011517719340089098
=;0.010447308010205537
model(**inputs);0.017704277962664123
last_hidden_states;0.016111860356190527
=;0.010631701193704572
outputs.last_hidden_state;0.012034073145581676
***;0.009661867160122977
