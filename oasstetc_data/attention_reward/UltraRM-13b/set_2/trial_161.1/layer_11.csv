text;attention
The;0.007719220076453665
easiest;0.00689487522996926
way;0.006652985863965521
to;0.005765261181302147
import;0.010695269304682384
the;0.006707889308720173
BERT;0.015848900813172417
language;0.0063103261390144915
model;0.00825003164553189
into;0.006669670725262708
python;0.00676837003698746
for;0.006350545373873682
use;0.006035830055620521
with;0.0057766041631929395
PyTorch;0.010656343435777184
is;0.007938887664154584
using;0.006711276650614724
the;0.007113170942720281
Hugging;0.007491490807709067
Face;0.014149525538133714
Transformer's;0.06341306525453266
library,;0.009497648531035179
which;0.00621844753044181
has;0.005965052546181509
built;0.005450531092039338
in;0.006943184929825738
methods;0.006820928100305926
for;0.006437002567974043
pre-training,;0.013543281320731071
inference,;0.009322466927513035
and;0.005837943321948409
deploying;0.0070675620815408
BERT.;0.011877800534007223
â€˜**;0.008611639021950892
from;0.008576553792128664
transformers;0.008917548128732857
import;0.008955323562884431
AutoTokenizer,;0.02505920455327626
BertModel;0.010109074035676954
import;0.009776375377020169
torch;0.009233179877478999
tokenizer;0.011134975533742417
=;0.009202103662642071
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3053888123616364
model;0.007343254481501538
=;0.008061597298256448
"BertModel.from_pretrained(""bert-base-uncased"")";0.1103137564401112
inputs;0.0075280814475792894
=;0.007369577736519422
"tokenizer(""Hello,";0.021579085785987072
my;0.005625367968102287
dog;0.006105889779576382
is;0.005890133101945534
"cute"",";0.008497383384856349
"return_tensors=""pt"")";0.019879709946828052
outputs;0.0063545639934556196
=;0.006255338092699197
model(**inputs);0.01582575069651093
last_hidden_states;0.010260408887814548
=;0.0058624640833035545
outputs.last_hidden_state;0.008174166359241775
***;0.0052072909136051245
