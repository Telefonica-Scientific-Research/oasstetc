text;attention
The;0.01391603615421236
easiest;0.013111817876653224
way;0.015650797358488778
to;0.01187659680965552
import;0.017394563014002534
the;0.011502399292215642
BERT;0.017541888595170158
language;0.01222766196899446
model;0.014753586067755137
into;0.011239765526272332
python;0.012309585868929166
for;0.010964771198503526
use;0.012641117026914886
with;0.010369871732951914
PyTorch;0.02097363799128437
is;0.016672961263019394
using;0.01155651623285669
the;0.010882732023200132
Hugging;0.014068695582826501
Face;0.013205077971492478
Transformer's;0.08227187617425089
library,;0.020956138059430807
which;0.011582644158547246
has;0.010878400151915775
built;0.011181573762520873
in;0.011116274967726242
methods;0.01217126436537413
for;0.010368976928112707
pre-training,;0.016251140159989907
inference,;0.013652501385613337
and;0.010352077454192234
deploying;0.012395336820355804
BERT.;0.021175708957970538
â€˜**;0.017049064932327024
from;0.012627147142970599
transformers;0.012789949078312386
import;0.011282398899089817
AutoTokenizer,;0.014981824665078483
BertModel;0.012645335086477893
import;0.01318747123834475
torch;0.0126325686072681
tokenizer;0.014811842679915636
=;0.011125859696392439
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07260110370279792
model;0.011602907756690099
=;0.01018820315547736
"BertModel.from_pretrained(""bert-base-uncased"")";0.08182267583294735
inputs;0.011258197334644926
=;0.010418432796792568
"tokenizer(""Hello,";0.015985070114247414
my;0.010373800686823888
dog;0.010314935600728818
is;0.010142357285694311
"cute"",";0.01237399413882747
"return_tensors=""pt"")";0.015558391972357324
outputs;0.010151723762247778
=;0.00987599738266768
model(**inputs);0.014248706015622633
last_hidden_states;0.012266759504770505
=;0.009731134141946091
outputs.last_hidden_state;0.011296938187280187
***;0.009441215699858896
