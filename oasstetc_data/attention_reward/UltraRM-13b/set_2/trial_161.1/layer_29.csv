text;attention
The;0.015580266840288737
easiest;0.016321899403425957
way;0.01637135563223423
to;0.014661547471282397
import;0.016116656526590504
the;0.014510822890913051
BERT;0.017371867201559782
language;0.015359219421107583
model;0.015520004543520356
into;0.014319181555592829
python;0.015493918168767619
for;0.014236011627593899
use;0.01526401390164546
with;0.013926867634734927
PyTorch;0.019741786640597957
is;0.015793614229395514
using;0.014533824420238598
the;0.01410721316249294
Hugging;0.01954102514937968
Face;0.015336769715712234
Transformer's;0.0297497391260267
library,;0.01764977961740476
which;0.014451558705879558
has;0.014102007336220863
built;0.014635416488310333
in;0.014187551030533157
methods;0.01480349732076158
for;0.013705065968401587
pre-training,;0.01817521085052871
inference,;0.015391268549541216
and;0.01366606656895184
deploying;0.015255801244841733
BERT.;0.018564347675807922
â€˜**;0.017182255360759197
from;0.014596493766492039
transformers;0.016176121578123096
import;0.014424774933631713
AutoTokenizer,;0.018357184213253594
BertModel;0.015591561767545581
import;0.014603885468603932
torch;0.01535934461361961
tokenizer;0.01578099699286435
=;0.014113405319172476
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04077171980794217
model;0.014097434881711645
=;0.013647056239422686
"BertModel.from_pretrained(""bert-base-uncased"")";0.03400363537615289
inputs;0.013842443374132473
=;0.013505555918233244
"tokenizer(""Hello,";0.016607838171517116
my;0.013513408903108691
dog;0.013625296817180861
is;0.013417387417117812
"cute"",";0.014572008315932784
"return_tensors=""pt"")";0.016661486655998828
outputs;0.013425719930809986
=;0.013305468043079388
model(**inputs);0.015606407318650576
last_hidden_states;0.014585938383884002
=;0.013172504928972461
outputs.last_hidden_state;0.01399967193413182
***;0.013007816947670916
