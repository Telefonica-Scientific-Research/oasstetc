text;attention
The;0.012359455573016575
easiest;0.013509008764573633
way;0.013182456944871612
to;0.011612638444246098
import;0.013583096305804258
the;0.011473797076463463
BERT;0.014494895451226873
language;0.012425633758589016
model;0.011453716547841499
into;0.011710140890508616
python;0.013954602635769664
for;0.01176611130699526
use;0.012407004157919843
with;0.01159218718673409
PyTorch;0.018227999532297062
is;0.012119043424234418
using;0.011996148715211945
the;0.011499763602124421
Hugging;0.018537504687850124
Face;0.012856295569081837
Transformer's;0.018144925079727297
library,;0.014373407637639094
which;0.011957834041203357
has;0.011544361258671806
built;0.012416407880837388
in;0.011417916123322734
methods;0.012430591939089417
for;0.011262742496240725
pre-training,;0.0180710084304597
inference,;0.013676300305642281
and;0.01160822221927356
deploying;0.012700720567781402
BERT.;0.015024315439955558
â€˜**;0.01632572749815677
from;0.011925021004532596
transformers;0.014239638544913866
import;0.012839925679372474
AutoTokenizer,;0.02004538306510132
BertModel;0.014278907535711639
import;0.011903645152506184
torch;0.014256065781577523
tokenizer;0.013907236959890422
=;0.012359721886052394
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.11766979811605055
model;0.01125551042793172
=;0.01196136816223575
"BertModel.from_pretrained(""bert-base-uncased"")";0.06714409570404772
inputs;0.012133604834669699
=;0.011643239431362256
"tokenizer(""Hello,";0.021370466228753833
my;0.011548144092518928
dog;0.011966865402279784
is;0.011135982559247977
"cute"",";0.013677801627372949
"return_tensors=""pt"")";0.022932422975378564
outputs;0.012002826772773693
=;0.01141969192968987
model(**inputs);0.018847169220487407
last_hidden_states;0.016930669585906388
=;0.010975015330138504
outputs.last_hidden_state;0.017265610734171037
***;0.010648189761963365
