text;attention
The;0.009564611695946261
easiest;0.008147780439206422
way;0.007762917288305216
to;0.006968080481045954
import;0.018261256474892237
the;0.008629369709511284
BERT;0.021665146704349963
language;0.009000158999877434
model;0.012384099048103567
into;0.00870123869555382
python;0.008555839469885517
for;0.008007472820226296
use;0.008089862540996619
with;0.006964150180307013
PyTorch;0.022522792365348
is;0.009158670356222774
using;0.007915170547899374
the;0.008507254853087563
Hugging;0.00946270408963428
Face;0.01631547239123968
Transformer's;0.05329375484017854
library,;0.011823143236184648
which;0.007365914422256238
has;0.0069895599120799986
built;0.006662562001180394
in;0.008065494543568658
methods;0.007590346579420604
for;0.0071786696679636275
pre-training,;0.017494286609572154
inference,;0.01314195194695654
and;0.007015911838150394
deploying;0.009306517772524664
BERT.;0.016242994129090832
â€˜**;0.01004435169058436
from;0.009403428877038827
transformers;0.012297405027532946
import;0.011491819262095474
AutoTokenizer,;0.027398243611214426
BertModel;0.01405465779550156
import;0.013985947884832817
torch;0.010536379405540783
tokenizer;0.0137798306154577
=;0.011166132930191463
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2435568219227524
model;0.008963358779723245
=;0.008391337066488433
"BertModel.from_pretrained(""bert-base-uncased"")";0.064907578759507
inputs;0.008697263235263446
=;0.00781663086230051
"tokenizer(""Hello,";0.017150929988343114
my;0.006553212548389545
dog;0.007069713868460361
is;0.006492530397978733
"cute"",";0.008087536653986264
"return_tensors=""pt"")";0.022919997878344013
outputs;0.007874913826483677
=;0.007345659536510717
model(**inputs);0.018210159928110526
last_hidden_states;0.010527167675507865
=;0.007448057573636113
outputs.last_hidden_state;0.00886169143986444
***;0.006212084307592588
