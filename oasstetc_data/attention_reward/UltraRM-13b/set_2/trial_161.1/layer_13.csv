text;attention
The;0.009636381985847018
easiest;0.008142523815874986
way;0.007940930221467024
to;0.007706164697418119
import;0.01639957021858891
the;0.008869974876010423
BERT;0.023952288322187706
language;0.008520301800725164
model;0.012361157785651933
into;0.008112127550475145
python;0.008089975623626332
for;0.007779303365743494
use;0.007968704485615192
with;0.0074430103197081965
PyTorch;0.018454911826177735
is;0.008705143728531548
using;0.008087504243358202
the;0.00879928543065916
Hugging;0.00983589193540767
Face;0.015348143959854706
Transformer's;0.06258574127835312
library,;0.0120019098070421
which;0.0076180539159163545
has;0.007634239888011992
built;0.007259760612331988
in;0.008232963920603154
methods;0.007825490108932609
for;0.007626976418136942
pre-training,;0.016230555901601725
inference,;0.011840898451631747
and;0.007316972248403915
deploying;0.009130317003511561
BERT.;0.01300861937760058
â€˜**;0.009129288845956537
from;0.008879235512521143
transformers;0.012065370388977956
import;0.011852711032360741
AutoTokenizer,;0.03643753618945037
BertModel;0.017958879629963488
import;0.012006376690310263
torch;0.012524229436276216
tokenizer;0.01652766215824142
=;0.013731631919182988
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2179469075799606
model;0.009837661661748999
=;0.00891506968086976
"BertModel.from_pretrained(""bert-base-uncased"")";0.068146492651204
inputs;0.008990805881298096
=;0.008240178006914593
"tokenizer(""Hello,";0.019116544739786853
my;0.007204471168692492
dog;0.008198449950288155
is;0.007383357471970989
"cute"",";0.008957469529440555
"return_tensors=""pt"")";0.01997380193377793
outputs;0.008443973035834596
=;0.00792412116831682
model(**inputs);0.015647885345650626
last_hidden_states;0.01066846597056721
=;0.007497541845418486
outputs.last_hidden_state;0.008513645222973609
***;0.006814440227037884
