text;attention
The;0.0007712734276057071
easiest;0.00023293924850454264
way;0.0002964001108308314
to;0.001268943092322648
import;0.00027202118151919004
the;0.0005253954668465702
BERT;0.0008475399491468842
language;0.0002047039090207839
model;0.00019951335686833026
into;0.0004508569833786026
python;0.0002712913125334827
for;0.0007640061401420065
use;0.00022412091439015576
with;0.000642303378699529
PyTorch;0.00154855495613644
is;0.0006184889139548383
using;0.00020558458472675457
the;0.00034455561241734397
Hugging;0.0013110173020702196
Face;0.00016729777842491395
Transformer's;0.006897888541075325
library,;0.0012067809742729305
which;0.00023516432031324904
has;0.00029822144468943565
built;0.00014785331714760975
in;0.00045124272454747267
methods;0.0001453757907050658
for;0.0003717269716888913
pre-training,;0.005721838577787901
inference,;0.0007337486266220844
and;0.00033612094852221084
deploying;0.00036310185772960306
BERT.;0.0018193057324288123
â€˜**;0.0002530758772250946
from;0.0002494774878807287
transformers;0.0003122613289650201
import;0.00014134853388915594
AutoTokenizer,;0.00164497237338331
BertModel;0.00020351791390569012
import;0.0001330147927901847
torch;0.00021837370163021316
tokenizer;0.00020689221108410532
=;0.00020939610662922063
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.9084077583888776
model;0.00010295987982018597
=;0.00015866502263473634
"BertModel.from_pretrained(""bert-base-uncased"")";0.0553325451753196
inputs;9.605316140362798e-05
=;0.00012581526014940846
"tokenizer(""Hello,";0.00047658284383682596
my;9.745467522471588e-05
dog;9.151525342451242e-05
is;0.00011260240777337234
"cute"",";0.00016030344116679923
"return_tensors=""pt"")";0.0005285604253351525
outputs;8.655902746164552e-05
=;9.833052467572157e-05
model(**inputs);0.00023494482376811788
last_hidden_states;0.0001397397545038567
=;8.5205169318037e-05
outputs.last_hidden_state;0.00012238233253176922
***;7.451465832091349e-05
