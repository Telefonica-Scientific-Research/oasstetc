text;attention
The;0.016190713840466685
easiest;0.015849456630095463
way;0.016092508659575375
to;0.014656191949590548
import;0.016861052203797185
the;0.01482120053112431
BERT;0.022371026263120083
language;0.015389761170302234
model;0.015585375802037674
into;0.014337859059545287
python;0.015123445556817043
for;0.014281167043328971
use;0.015309767619792327
with;0.013957835440132864
PyTorch;0.02439744068237915
is;0.015426459985862768
using;0.014715272141040408
the;0.014119836624173081
Hugging;0.0215244967984499
Face;0.015331374987066491
Transformer's;0.028365049070431376
library,;0.01697138063377144
which;0.013802025651818952
has;0.013550931285468557
built;0.014077101249399595
in;0.013578855701078021
methods;0.014053336061481764
for;0.013232397989448861
pre-training,;0.018512018160600868
inference,;0.01494578513076534
and;0.013177811812402337
deploying;0.014711262044697603
BERT.;0.01889061338101312
â€˜**;0.01613473960311856
from;0.014134150366207524
transformers;0.01622403889354502
import;0.01403033372470989
AutoTokenizer,;0.019518938006342638
BertModel;0.015870349821369745
import;0.014099494691018224
torch;0.015091335573505085
tokenizer;0.015359270165695839
=;0.01356381181357465
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04682764874599158
model;0.01354167479150876
=;0.013127988172684351
"BertModel.from_pretrained(""bert-base-uncased"")";0.03483541050521482
inputs;0.013252366873454668
=;0.012893243415449642
"tokenizer(""Hello,";0.016047894326223108
my;0.012877646791842131
dog;0.012965121627734609
is;0.01279693755334941
"cute"",";0.013928199856164785
"return_tensors=""pt"")";0.01620837371630874
outputs;0.012778089898255398
=;0.012688052030068769
model(**inputs);0.014846468425850183
last_hidden_states;0.013933337030693634
=;0.012537468752578199
outputs.last_hidden_state;0.013306763098637504
***;0.012370040567826803
