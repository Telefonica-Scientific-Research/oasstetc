text;attention
The;0.012386235982731685
easiest;0.013368485820187428
way;0.012559766308140827
to;0.012224877278875624
import;0.01625598727055399
the;0.01193288824963298
BERT;0.014618380158071015
language;0.012540464595841695
model;0.012370935509643292
into;0.012773253789346023
python;0.012354262024531158
for;0.012768660848366297
use;0.012563358889730616
with;0.012439315334651199
PyTorch;0.01281301355875785
is;0.013065787816358905
using;0.012625238336095534
the;0.011829359119426633
Hugging;0.014322275479762898
Face;0.014710520969805524
Transformer's;0.01678553220061
library,;0.014596035827460609
which;0.012364236701921433
has;0.012166156828221615
built;0.011948393016314177
in;0.013162114120010027
methods;0.013176593588418934
for;0.01320583449760059
pre-training,;0.01897947069481695
inference,;0.014718611815840634
and;0.011909623029213712
deploying;0.01304672314615687
BERT.;0.014384851163869079
â€˜**;0.01591955588663852
from;0.016394862661919158
transformers;0.015080338266427101
import;0.015494319449392573
AutoTokenizer,;0.022520748709672515
BertModel;0.014022694846690498
import;0.014200217778083573
torch;0.013225930123518422
tokenizer;0.014094189921994129
=;0.0146269122491693
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.11812020785964149
model;0.0135458971959173
=;0.013881990073773174
"BertModel.from_pretrained(""bert-base-uncased"")";0.03370790544863868
inputs;0.013338381896020721
=;0.013815418100706904
"tokenizer(""Hello,";0.024348219652067824
my;0.011701833067836492
dog;0.01250124035527436
is;0.011821464981704678
"cute"",";0.013020759380291154
"return_tensors=""pt"")";0.020080689292997677
outputs;0.012615176789986212
=;0.01214189310880723
model(**inputs);0.02056754895816289
last_hidden_states;0.019797082317607396
=;0.011971527128568034
outputs.last_hidden_state;0.015072768620734082
***;0.011402981906792065
