text;attention
The;0.013757471171355203
easiest;0.012178164655339434
way;0.013118802359707424
to;0.011624846675168266
import;0.019818637518424616
the;0.01295369576416349
BERT;0.029910390822377467
language;0.013086329876248247
model;0.01676966580148799
into;0.011607478914193985
python;0.012726816565398133
for;0.011544693047717393
use;0.013611602423377174
with;0.011309614195397139
PyTorch;0.026363009858828685
is;0.014222051928443375
using;0.012895890569386047
the;0.012101937264777055
Hugging;0.01532309952674275
Face;0.017045390848549904
Transformer's;0.11480773497547107
library,;0.015665864104586483
which;0.010976965061900063
has;0.010917876516884221
built;0.01100137727596016
in;0.011202440163406517
methods;0.012163442106509199
for;0.010848450044412714
pre-training,;0.016074950334893562
inference,;0.014350446939553996
and;0.010540605638530046
deploying;0.012118323975570285
BERT.;0.018621709769227793
â€˜**;0.016106822195818635
from;0.012549398051459252
transformers;0.012858513409246163
import;0.011847757235882914
AutoTokenizer,;0.016739084126616337
BertModel;0.013307790475503239
import;0.012854859077170648
torch;0.012204215557855476
tokenizer;0.013557504046171282
=;0.011276127959756533
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04462775652836546
model;0.011666385794939815
=;0.010739675978703735
"BertModel.from_pretrained(""bert-base-uncased"")";0.053522485929121975
inputs;0.011461512963175415
=;0.010674700267701266
"tokenizer(""Hello,";0.014161916300747519
my;0.010501142528703574
dog;0.010678617813515576
is;0.010283532970758284
"cute"",";0.012044048791359768
"return_tensors=""pt"")";0.015307923427220885
outputs;0.01060356686786945
=;0.010327801529775624
model(**inputs);0.013545398631052138
last_hidden_states;0.012919431117874199
=;0.01030302318973843
outputs.last_hidden_state;0.012004534246058114
***;0.010064696293848078
