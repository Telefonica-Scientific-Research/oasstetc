text;attention
The;0.007239825488534897
easiest;0.007031124667878864
way;0.007092512844871548
to;0.0062079103755979595
import;0.008733779093080589
the;0.006591747724236307
BERT;0.02643164501600022
language;0.007103483777507991
model;0.009683046508350893
into;0.006335666016075009
python;0.006606458332450813
for;0.006148344661041654
use;0.0066532946462523855
with;0.006281482478075908
PyTorch;0.013986471162054369
is;0.007234069909774983
using;0.007313281239161012
the;0.00730932537698698
Hugging;0.009352070151960507
Face;0.014107518793961184
Transformer's;0.21189242040783385
library,;0.008818226174950237
which;0.006184599403992599
has;0.006241600369471377
built;0.006146252702931693
in;0.00631852398519977
methods;0.006410528568697859
for;0.006104108476413164
pre-training,;0.011341597696529244
inference,;0.009883554403597537
and;0.006002306040655459
deploying;0.0069841442864736316
BERT.;0.015358927017391144
â€˜**;0.008683411788982812
from;0.0071562745397017935
transformers;0.009532323359120378
import;0.008270699099191776
AutoTokenizer,;0.02906826608238624
BertModel;0.014333373560108513
import;0.008688677865443521
torch;0.008748878684785237
tokenizer;0.010616831262147713
=;0.008046325195692608
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.10502933999890333
model;0.00835679059097644
=;0.007057162273754193
"BertModel.from_pretrained(""bert-base-uncased"")";0.15763535873544032
inputs;0.007915834583525518
=;0.006643164322498528
"tokenizer(""Hello,";0.0137250849577554
my;0.005987875242297161
dog;0.006490892187492359
is;0.006087841604089197
"cute"",";0.00728441504554426
"return_tensors=""pt"")";0.015782990268548187
outputs;0.0065319443715541
=;0.006148842072432343
model(**inputs);0.011692248748186725
last_hidden_states;0.009614750565343262
=;0.0060507336063974895
outputs.last_hidden_state;0.007853454972211887
***;0.005836366617497457
