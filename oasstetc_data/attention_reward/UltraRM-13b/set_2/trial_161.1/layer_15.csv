text;attention
The;0.00644003566943448
easiest;0.005098026341386617
way;0.004896116881872647
to;0.0045419897281628345
import;0.00922794158692708
the;0.00532847303193514
BERT;0.026283318525591792
language;0.005337490697707287
model;0.009639382147036408
into;0.005245275538605993
python;0.005304982891453963
for;0.004682877150197907
use;0.005302988982636733
with;0.004424406623684225
PyTorch;0.014628022530809836
is;0.006734233166470084
using;0.005391085119343089
the;0.005427735786099147
Hugging;0.00697246585251924
Face;0.020132855948540383
Transformer's;0.05821814530844111
library,;0.007783649773975472
which;0.004535613453827162
has;0.004532932978088743
built;0.004239329401224453
in;0.004775539485585507
methods;0.004383886957497322
for;0.004434729317434052
pre-training,;0.009888580381650703
inference,;0.008910672299734567
and;0.0042411614697066636
deploying;0.0048007252445832985
BERT.;0.01565308686213614
â€˜**;0.0055693016931175265
from;0.005340201932373789
transformers;0.008294639806130887
import;0.006970432297005688
AutoTokenizer,;0.03673017292117723
BertModel;0.01208776292477143
import;0.008304046628676792
torch;0.007385292020985737
tokenizer;0.009042968890873398
=;0.007132466879682493
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3454760343180934
model;0.006224520240595208
=;0.005538807086598358
"BertModel.from_pretrained(""bert-base-uncased"")";0.14385471341691558
inputs;0.005705263094005464
=;0.004939923254616667
"tokenizer(""Hello,";0.01232599746190216
my;0.0041417830495732305
dog;0.00457734322559311
is;0.0042035477416550145
"cute"",";0.005397134002430512
"return_tensors=""pt"")";0.01473053974048707
outputs;0.0048009173236080015
=;0.0042752221326450065
model(**inputs);0.009221757733517091
last_hidden_states;0.006859461682469283
=;0.004263111681415922
outputs.last_hidden_state;0.005215260386090721
***;0.003953619298693067
