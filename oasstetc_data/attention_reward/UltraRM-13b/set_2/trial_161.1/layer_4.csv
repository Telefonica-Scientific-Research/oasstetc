text;attention
The;0.013170672579715124
easiest;0.013661438071882519
way;0.012502564955820222
to;0.012066773302994053
import;0.013782373881634925
the;0.012150178527033047
BERT;0.014533062838113099
language;0.012810210947334113
model;0.012317495935362518
into;0.012374479114592335
python;0.012041618665768436
for;0.012459579799612755
use;0.012508666181899962
with;0.012760787768298485
PyTorch;0.013898893875469349
is;0.013298886437986723
using;0.012519909277671232
the;0.011827279886117072
Hugging;0.015172448211702565
Face;0.014248256304202726
Transformer's;0.017580467539180546
library,;0.014593097794828088
which;0.01219113223097914
has;0.011919193653181615
built;0.012241220515013512
in;0.013360551274954027
methods;0.012851978542060234
for;0.012402002563162925
pre-training,;0.021320270284986533
inference,;0.013262707302855442
and;0.01169339731035419
deploying;0.012717179070459236
BERT.;0.01442173465273952
â€˜**;0.0159229784625132
from;0.015335333518153062
transformers;0.01451194988498887
import;0.014166134335948684
AutoTokenizer,;0.021415993876560863
BertModel;0.014732173253885622
import;0.013397488417960653
torch;0.013168289434369181
tokenizer;0.014205023240745173
=;0.014083368568670542
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.12418359671981503
model;0.013329098847774834
=;0.013457226365162495
"BertModel.from_pretrained(""bert-base-uncased"")";0.03745433544269972
inputs;0.013692088712105034
=;0.013792284038338055
"tokenizer(""Hello,";0.02038387977412731
my;0.012031302844952282
dog;0.012200054645790588
is;0.011999359732222093
"cute"",";0.0129091220067413
"return_tensors=""pt"")";0.018752101263126607
outputs;0.012936089463012309
=;0.01284518864679411
model(**inputs);0.019403531180277274
last_hidden_states;0.020223654591796092
=;0.012167270962147184
outputs.last_hidden_state;0.015414562876256801
***;0.011226009595098925
