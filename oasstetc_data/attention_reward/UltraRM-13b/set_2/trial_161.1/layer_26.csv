text;attention
The;0.0161033595779595
easiest;0.016045070165200074
way;0.017443200650750057
to;0.014356627234629315
import;0.016900283656548814
the;0.014420401961410848
BERT;0.01877246991962443
language;0.014824312053477453
model;0.016014972588817667
into;0.014051150435382211
python;0.015081229251022489
for;0.013958238458582977
use;0.01514492043905214
with;0.013436551709261575
PyTorch;0.02023743872166326
is;0.01684913116869089
using;0.014283388947305952
the;0.013631334194068295
Hugging;0.017604317816202395
Face;0.015007008921844468
Transformer's;0.038506329111259355
library,;0.01882566870528061
which;0.014253864562292466
has;0.013758810208398323
built;0.014091494963001295
in;0.013780210038803704
methods;0.014858817989077204
for;0.013276328814490406
pre-training,;0.01683998554339282
inference,;0.015456017915220482
and;0.013271725946364653
deploying;0.01497857447772351
BERT.;0.02027297179129906
â€˜**;0.017894421396819515
from;0.014682089977569937
transformers;0.015244171744324114
import;0.014108936788141202
AutoTokenizer,;0.017913045688388047
BertModel;0.015337518506073273
import;0.015075987486669358
torch;0.015050991351061641
tokenizer;0.016126184898943102
=;0.0140898979483785
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.036930378850463824
model;0.013889541576048842
=;0.01327935352663599
"BertModel.from_pretrained(""bert-base-uncased"")";0.035675170517300424
inputs;0.013732127865970422
=;0.013250565262200545
"tokenizer(""Hello,";0.016346311496661776
my;0.013235102745403148
dog;0.01342094255427336
is;0.01310786965622355
"cute"",";0.014448608999017872
"return_tensors=""pt"")";0.015896210030182507
outputs;0.013133585712454677
=;0.01299339514440004
model(**inputs);0.015371672205514757
last_hidden_states;0.014195615297709435
=;0.01288463718443483
outputs.last_hidden_state;0.013622992580817372
***;0.01272646506981928
