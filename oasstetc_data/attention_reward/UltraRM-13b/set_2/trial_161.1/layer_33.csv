text;attention
The;0.01548328322698767
easiest;0.016173502019780652
way;0.016030856488266337
to;0.013656423741541192
import;0.016347373933187186
the;0.013657529179655799
BERT;0.021917011419482907
language;0.014238883673844809
model;0.014677575843113846
into;0.01309416077409882
python;0.014281043787639397
for;0.01284342047491457
use;0.014215554545602133
with;0.012324320865261291
PyTorch;0.026286238904682232
is;0.014712839666402513
using;0.01320646854063793
the;0.012565660639102472
Hugging;0.02362541625742765
Face;0.014042434334397998
Transformer's;0.034666348355531985
library,;0.017135981498098415
which;0.012406857796485253
has;0.012019010792431393
built;0.01257408171470145
in;0.012023430828809304
methods;0.012729036949835215
for;0.011449874958026511
pre-training,;0.020604901255358826
inference,;0.014457815729525586
and;0.011424524125392752
deploying;0.013801688528850513
BERT.;0.01976100992885849
â€˜**;0.01592490585060216
from;0.01272511085324952
transformers;0.01526599104341809
import;0.012397770928800868
AutoTokenizer,;0.020554657371813116
BertModel;0.014486454254829645
import;0.012387679486677731
torch;0.013847871159999164
tokenizer;0.014126401176979662
=;0.011941913489063804
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08772261052010497
model;0.011623848278862708
=;0.011278196168271188
"BertModel.from_pretrained(""bert-base-uncased"")";0.05034437082137434
inputs;0.0113059197111655
=;0.01095488857884866
"tokenizer(""Hello,";0.015320033706948977
my;0.010935101309763948
dog;0.011040245612018822
is;0.010821351591797337
"cute"",";0.012343879728511715
"return_tensors=""pt"")";0.015319322040964534
outputs;0.010707929010858569
=;0.01061429760633534
model(**inputs);0.013394898595811026
last_hidden_states;0.012134279925902974
=;0.010428586115243036
outputs.last_hidden_state;0.011395559809135496
***;0.010221364474713973
