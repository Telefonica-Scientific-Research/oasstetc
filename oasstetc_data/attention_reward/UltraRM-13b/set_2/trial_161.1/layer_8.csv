text;attention
The;0.010964129584637196
easiest;0.011369782641693086
way;0.010720013543596547
to;0.009913802354538006
import;0.01506707916592542
the;0.010157461930033768
BERT;0.025272380247743132
language;0.010379028865854753
model;0.012972136310190502
into;0.011463309728903184
python;0.012978596454302138
for;0.010881238898556906
use;0.011153284798793987
with;0.010848505008083612
PyTorch;0.015517704980660494
is;0.011322335566020419
using;0.010859310416150859
the;0.010546129610718148
Hugging;0.011734303673362662
Face;0.022736620607416873
Transformer's;0.029770819080783842
library,;0.015863304245169668
which;0.010276084733068546
has;0.010070437472074891
built;0.009578020149920818
in;0.010956558310527754
methods;0.011880295447421791
for;0.011078520879579098
pre-training,;0.018118938571041684
inference,;0.013539233732162466
and;0.009858292017127462
deploying;0.012164524939811506
BERT.;0.015335174939437068
â€˜**;0.01336059110072427
from;0.014649315275322237
transformers;0.015084844096261527
import;0.012589614282460795
AutoTokenizer,;0.03165809484362146
BertModel;0.015177271651372856
import;0.01552039733069978
torch;0.012668839816680716
tokenizer;0.019801843401555073
=;0.012838466243310174
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.12631278070573576
model;0.012946926310496053
=;0.0124656276428195
"BertModel.from_pretrained(""bert-base-uncased"")";0.052689534033021006
inputs;0.013642162903925042
=;0.011477097449579313
"tokenizer(""Hello,";0.024719572450869007
my;0.009870253034408664
dog;0.011344966934525498
is;0.010148459138592554
"cute"",";0.011950247004684167
"return_tensors=""pt"")";0.0177762440752037
outputs;0.011722843260627655
=;0.009977578343258563
model(**inputs);0.017846494251007167
last_hidden_states;0.014774441620201686
=;0.009953248732448062
outputs.last_hidden_state;0.012363899701898563
***;0.009320985459380765
