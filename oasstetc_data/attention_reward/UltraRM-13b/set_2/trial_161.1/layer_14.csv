text;attention
The;0.006829171785307785
easiest;0.006209689346085093
way;0.005812260048305067
to;0.0052823052073906135
import;0.009307987195128698
the;0.006611222893968679
BERT;0.03538679676419158
language;0.0066539049222257615
model;0.012022963342868778
into;0.006175091924280718
python;0.007171311634570681
for;0.0056640837907782965
use;0.006092270541121713
with;0.005278201159493407
PyTorch;0.017753614864055807
is;0.0069927843961680915
using;0.006025902723148906
the;0.006272852729490721
Hugging;0.007733180822982752
Face;0.04244733260354373
Transformer's;0.07030180259530913
library,;0.011394528392156813
which;0.005737992431294157
has;0.005550073518782566
built;0.005370305041775789
in;0.00583569978473481
methods;0.005589293522686405
for;0.005419331538330354
pre-training,;0.014076429487881055
inference,;0.010289941643102736
and;0.005217210057007626
deploying;0.006678239898330551
BERT.;0.015790320402548453
â€˜**;0.007133516473529076
from;0.006226775262523462
transformers;0.009777540870676787
import;0.00930090066904374
AutoTokenizer,;0.03294663775418772
BertModel;0.013456588767812831
import;0.008506561346588235
torch;0.00979505456687109
tokenizer;0.011202442050037825
=;0.008628986925641891
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.28418041421170814
model;0.0066292152678897815
=;0.006243954153401894
"BertModel.from_pretrained(""bert-base-uncased"")";0.07753991591520425
inputs;0.008187078534797044
=;0.0063172495885054514
"tokenizer(""Hello,";0.01922036324115604
my;0.005057211044797527
dog;0.0057484723146717384
is;0.005135768278644398
"cute"",";0.0065395838795346516
"return_tensors=""pt"")";0.020263503849851157
outputs;0.006248673918544486
=;0.005474820258511404
model(**inputs);0.01596019162768111
last_hidden_states;0.008528020613962973
=;0.005396882027284316
outputs.last_hidden_state;0.0064757545717970275
***;0.004903825006096712
