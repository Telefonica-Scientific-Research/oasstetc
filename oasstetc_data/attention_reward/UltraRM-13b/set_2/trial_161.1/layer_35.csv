text;attention
The;0.014204872944191994
easiest;0.013979910539771719
way;0.014429213025507785
to;0.01182412612683087
import;0.015063890668193585
the;0.011578844045980486
BERT;0.02639651142602231
language;0.012340462269719283
model;0.012764789343811391
into;0.010886794999881622
python;0.012209239465982104
for;0.010912739829237043
use;0.01204095683220918
with;0.010216364693186162
PyTorch;0.029933005879558328
is;0.012740500956296807
using;0.011154096352540317
the;0.010406761811278432
Hugging;0.025141120267827197
Face;0.012267462870436067
Transformer's;0.04071347731946266
library,;0.016154136731978398
which;0.010341403119493393
has;0.009807794092932502
built;0.010306472546267537
in;0.009743208793681601
methods;0.01038972582247897
for;0.00931059117855677
pre-training,;0.01995485637100673
inference,;0.012237556104801412
and;0.009202623443873853
deploying;0.011619558564021346
BERT.;0.020425446549873348
â€˜**;0.015024346697973524
from;0.010565543158005638
transformers;0.013756224078919212
import;0.010324347354760306
AutoTokenizer,;0.0202399065236924
BertModel;0.013471252191526962
import;0.01047401992985963
torch;0.01230141394297841
tokenizer;0.012651122508639895
=;0.009772528821109345
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.14839189043469397
model;0.009627749673861461
=;0.00909971637394071
"BertModel.from_pretrained(""bert-base-uncased"")";0.07639884512825025
inputs;0.00913949605154356
=;0.008706206647290342
"tokenizer(""Hello,";0.014045311752297007
my;0.00864109047958532
dog;0.008785041409953932
is;0.008536142070828198
"cute"",";0.01029892393840309
"return_tensors=""pt"")";0.014333753619851999
outputs;0.008506594127783918
=;0.008392961153508372
model(**inputs);0.011955289664502522
last_hidden_states;0.010321967496745925
=;0.00817800855667757
outputs.last_hidden_state;0.009448450155213868
***;0.00791334107071141
