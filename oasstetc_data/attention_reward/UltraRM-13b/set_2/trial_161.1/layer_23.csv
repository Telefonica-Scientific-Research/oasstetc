text;attention
The;0.015814808751135483
easiest;0.0159723141142443
way;0.016582589472962317
to;0.014156515551472943
import;0.017304209700987987
the;0.014324722001954804
BERT;0.019761441773599474
language;0.015099199561176372
model;0.016126952352731736
into;0.014070496527616033
python;0.015336732821221624
for;0.013968752907132568
use;0.015075076597953685
with;0.013531284816988119
PyTorch;0.020860557514923123
is;0.01601749097466896
using;0.014229870439394046
the;0.013550433518896905
Hugging;0.01752389809854952
Face;0.015331882244737484
Transformer's;0.037270896696026824
library,;0.018680223685079274
which;0.013934403831512447
has;0.013413219589245622
built;0.014085152836644235
in;0.013964818789708868
methods;0.014766481489281828
for;0.013122275552388561
pre-training,;0.01742413134412265
inference,;0.015651023429157324
and;0.01322705196326778
deploying;0.014780785749048037
BERT.;0.019472943806909254
â€˜**;0.018095948979124647
from;0.014816702953688289
transformers;0.01576813667513493
import;0.01393696508380271
AutoTokenizer,;0.01839028741691425
BertModel;0.015929566987167715
import;0.014898741963982217
torch;0.015288971855999867
tokenizer;0.016316482332828938
=;0.013814597341433045
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.03873353626683961
model;0.013980900653649354
=;0.01314238909006555
"BertModel.from_pretrained(""bert-base-uncased"")";0.034330544432302275
inputs;0.013785977670195144
=;0.013170262887567261
"tokenizer(""Hello,";0.016699314378601334
my;0.013215214702355876
dog;0.013330352481463118
is;0.012978239577865012
"cute"",";0.01443492710755545
"return_tensors=""pt"")";0.015991053301733425
outputs;0.013109237067092343
=;0.012891937318565186
model(**inputs);0.015283297941256121
last_hidden_states;0.014240779268453184
=;0.012792051318144423
outputs.last_hidden_state;0.013550544983956418
***;0.012650399457522107
