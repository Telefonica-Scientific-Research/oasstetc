text;attention
The;4.3741834433618786e-05
easiest;4.080526283143286e-05
way;4.1353148324059044e-05
to;3.0307084662937688e-05
import;5.758526182357978e-05
the;3.361284659779441e-05
BERT;0.00015994115947669
language;3.1038109574074217e-05
model;3.497514582263536e-05
into;2.6802560063923615e-05
python;2.8692559975850055e-05
for;2.5594525799104254e-05
use;2.9323355410709926e-05
with;2.3152681504019553e-05
PyTorch;0.0002396338153596845
is;3.392076106468197e-05
using;3.137041443285563e-05
the;2.7134272615976832e-05
Hugging;0.00019895040125367716
Face;3.2059280848774025e-05
Transformer's;0.0006677149276680252
library,;5.7209132823219776e-05
which;2.3909852603280082e-05
has;2.1797847121339152e-05
built;2.4767776804809495e-05
in;2.2415872554094168e-05
methods;2.4961250287413123e-05
for;2.0426590276893483e-05
pre-training,;0.0002370374127321012
inference,;4.487168534704844e-05
and;2.0174949234377143e-05
deploying;3.949053150184522e-05
BERT.;0.00013218137360816317
â€˜**;6.060278991861748e-05
from;2.81072291147426e-05
transformers;5.2838683873381465e-05
import;2.714158554421407e-05
AutoTokenizer,;0.00023363617872595148
BertModel;5.844577137623341e-05
import;2.9384758939215847e-05
torch;4.8498433397510204e-05
tokenizer;5.229730824913998e-05
=;2.389506424324948e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.8091270802317656
model;2.4211511827669883e-05
=;2.0804801610207594e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.1862161144742228
inputs;2.277579767296147e-05
=;1.9767460569322046e-05
"tokenizer(""Hello,";0.00018336650240001996
my;2.033430198498096e-05
dog;2.097380676672645e-05
is;1.947441876983969e-05
"cute"",";5.739001831104377e-05
"return_tensors=""pt"")";0.000569323132616099
outputs;2.0191635092529236e-05
=;1.837011949886623e-05
model(**inputs);0.00021856734654527346
last_hidden_states;0.00012381403346913513
=;1.7766630515187806e-05
outputs.last_hidden_state;0.00016243066385039108
***;1.544162469040184e-05
