text;attention
The;0.013123237186462659
easiest;0.0148694680140984
way;0.013072862089329593
to;0.012295438445695276
import;0.014293418869496952
the;0.012179056860780533
BERT;0.014199830290354157
language;0.012880403867937085
model;0.01216160870710469
into;0.012721191515841458
python;0.01268821988906286
for;0.013247594069903477
use;0.013323304985927766
with;0.012892366844244168
PyTorch;0.014949459415517907
is;0.013790031697711206
using;0.013582410093449074
the;0.012352411154735317
Hugging;0.017464330803728823
Face;0.01415476968671575
Transformer's;0.017999449995496995
library,;0.014979375925758145
which;0.01287811661385471
has;0.012845115898953837
built;0.012622447113263968
in;0.013295888269745038
methods;0.013529887525897457
for;0.013237632638680087
pre-training,;0.018563289537104025
inference,;0.014364214655379374
and;0.012110115942552763
deploying;0.013427187900810227
BERT.;0.01541132753153772
â€˜**;0.016086182395864956
from;0.013814837004343954
transformers;0.014968328423371018
import;0.014831020056473134
AutoTokenizer,;0.021472777669597973
BertModel;0.014025305190731467
import;0.014119376287668585
torch;0.01436466971902175
tokenizer;0.014351433173907132
=;0.014411972699507843
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08180175958819141
model;0.012378336029265273
=;0.012944887819656975
"BertModel.from_pretrained(""bert-base-uncased"")";0.050656126114574575
inputs;0.014008401450412178
=;0.012774505168247833
"tokenizer(""Hello,";0.024549225411650267
my;0.012320077111293891
dog;0.012947938602599348
is;0.012459350774853033
"cute"",";0.014954339362695045
"return_tensors=""pt"")";0.0243784499705586
outputs;0.012743370832681776
=;0.012823945168000575
model(**inputs);0.021840922610022948
last_hidden_states;0.0171389339083565
=;0.01243312375777259
outputs.last_hidden_state;0.017578317702994124
***;0.011316621958553654
