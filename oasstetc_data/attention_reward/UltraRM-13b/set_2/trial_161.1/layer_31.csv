text;attention
The;0.014769887783174758
easiest;0.015137681977211019
way;0.015358818966575222
to;0.013523981407342409
import;0.01535631585354444
the;0.013178580810450988
BERT;0.018383873396811818
language;0.014070557690589765
model;0.01446835606260442
into;0.01295246883547347
python;0.013824983511322348
for;0.012971650919312853
use;0.013774838311838511
with;0.012361279469459044
PyTorch;0.022002647671028025
is;0.014449955502947942
using;0.012983476926220277
the;0.012487646620684162
Hugging;0.020498099335902763
Face;0.013789182851901354
Transformer's;0.03821775732662893
library,;0.01717398363402535
which;0.012801164173993452
has;0.012369153100048689
built;0.013017835368501117
in;0.012363580677776827
methods;0.013013610762980613
for;0.011896042243417813
pre-training,;0.01871334290004297
inference,;0.014278302974588396
and;0.011803233638305172
deploying;0.013970677645191644
BERT.;0.018969811087806005
â€˜**;0.016382829619456484
from;0.013022437407836963
transformers;0.015596598216776208
import;0.012691097467669237
AutoTokenizer,;0.019398909809186436
BertModel;0.014548916579218784
import;0.012909981247502058
torch;0.014187818843364047
tokenizer;0.014780936173353644
=;0.01242235851254025
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08090864097848946
model;0.012241677806318279
=;0.011766552194632265
"BertModel.from_pretrained(""bert-base-uncased"")";0.05634881108090714
inputs;0.011859710063669643
=;0.011480090362919753
"tokenizer(""Hello,";0.016272500622699103
my;0.01150794818907952
dog;0.01157748891826389
is;0.011355436936798795
"cute"",";0.013152980240017653
"return_tensors=""pt"")";0.01645481843401447
outputs;0.011335451901712847
=;0.011190229285054896
model(**inputs);0.014514121182415305
last_hidden_states;0.01309155659606612
=;0.011000993126695821
outputs.last_hidden_state;0.012320375297528312
***;0.01074595346811017
