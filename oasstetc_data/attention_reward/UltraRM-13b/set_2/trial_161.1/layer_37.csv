text;attention
The;0.003531359539181438
easiest;0.0030058574797199996
way;0.0032626220625775007
to;0.0022922298129800017
import;0.0039260338374506615
the;0.0023009495306226626
BERT;0.012477284552956528
language;0.0022996393243908913
model;0.0025368337145887923
into;0.00176861268282882
python;0.0021366636752885647
for;0.0018381356811565846
use;0.0021952432462830546
with;0.0015255075412193967
PyTorch;0.02022672371010043
is;0.002309433428812035
using;0.0019280455938718883
the;0.0016611223754907341
Hugging;0.013065468549617694
Face;0.0020646556031289082
Transformer's;0.040496196769927
library,;0.003967453146559962
which;0.001521292890867046
has;0.0013951020784788012
built;0.0014986121014232716
in;0.0013780937307036117
methods;0.0015746320728365127
for;0.001240958096341702
pre-training,;0.008273836944866745
inference,;0.00226264159315345
and;0.001221026028187496
deploying;0.002025586788035078
BERT.;0.006667048292743731
â€˜**;0.0033303300832951787
from;0.0016020314438662479
transformers;0.002836341415216351
import;0.001469291651197149
AutoTokenizer,;0.007914531499824884
BertModel;0.002808251436896117
import;0.0015283445628803056
torch;0.0023395668807567422
tokenizer;0.0023411224637957897
=;0.0012878848491374984
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6631556126612725
model;0.0012294754932666591
=;0.0010972072423874966
"BertModel.from_pretrained(""bert-base-uncased"")";0.1270741585483704
inputs;0.0010741957286068613
=;0.000992163094393856
"tokenizer(""Hello,";0.0026267492008658147
my;0.0009903182571242978
dog;0.000999958374790179
is;0.000954674145177716
"cute"",";0.0014197723449476062
"return_tensors=""pt"")";0.002998849769811222
outputs;0.000937573335567621
=;0.0009142287093845521
model(**inputs);0.0019187984983380943
last_hidden_states;0.0014189486958511945
=;0.0008635703425169032
outputs.last_hidden_state;0.0011924594782665773
***;0.0008086873158031628
