text;attention
The;0.00913650149085159
easiest;0.008078118316173668
way;0.00791670750652168
to;0.006881312478573494
import;0.009875340906327239
the;0.006744793277642242
BERT;0.02819352455191273
language;0.007096048845894656
model;0.011084295669100073
into;0.006832016718936286
python;0.007243394133334831
for;0.006764388652917684
use;0.006874458459549061
with;0.0065087660945078365
PyTorch;0.028530470712476018
is;0.008965003624110221
using;0.008230906551369852
the;0.00782886792376644
Hugging;0.009754157709700059
Face;0.02835587697539795
Transformer's;0.0894940642168099
library,;0.010611690915036283
which;0.006545605001231766
has;0.006796186671486862
built;0.006389531114018728
in;0.007048744731124251
methods;0.006495696789724873
for;0.006730158426927119
pre-training,;0.011149087339075003
inference,;0.009777989955416571
and;0.006046063350892272
deploying;0.00708486874556573
BERT.;0.01749578205657004
â€˜**;0.008917704356929072
from;0.007661786938798878
transformers;0.01223565122892472
import;0.009865397713273192
AutoTokenizer,;0.02754977034703869
BertModel;0.011637259676646668
import;0.009858941564334372
torch;0.009475580329238066
tokenizer;0.010544339051292623
=;0.010411165127380306
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.21514441985437863
model;0.008570163674528106
=;0.0077814169783862455
"BertModel.from_pretrained(""bert-base-uncased"")";0.11287049746875877
inputs;0.007412390618190135
=;0.00717634224460526
"tokenizer(""Hello,";0.013787331581044992
my;0.0060321927901212475
dog;0.007062761110239036
is;0.006042466902833489
"cute"",";0.00687129805728761
"return_tensors=""pt"")";0.01722863190283999
outputs;0.006741379676839836
=;0.006501540125242235
model(**inputs);0.01122786440344838
last_hidden_states;0.0096049045947347
=;0.006336836704235482
outputs.last_hidden_state;0.007076506919733493
***;0.005813038145752884
