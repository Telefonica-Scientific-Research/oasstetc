text;attention
The;0.01111483040288956
easiest;0.011664490744664933
way;0.01075436616917168
to;0.009979562145118711
import;0.01466605879406125
the;0.009828067959274564
BERT;0.018124137134375103
language;0.009785817302517527
model;0.010435631022257303
into;0.010818588153948755
python;0.011142320807307374
for;0.010583526903982808
use;0.010867446620185512
with;0.010969751345421691
PyTorch;0.012832443109582538
is;0.011452057185861762
using;0.011145167815642683
the;0.0103752296774743
Hugging;0.01154472352550781
Face;0.017143834686061454
Transformer's;0.020502148196767375
library,;0.013837149487036161
which;0.010175558598620101
has;0.010085881095663357
built;0.009273015695522054
in;0.01062750167284929
methods;0.011398428727427865
for;0.010746168531453852
pre-training,;0.018072697931173425
inference,;0.011990660602509924
and;0.009530974641013478
deploying;0.011339915931964976
BERT.;0.013566818598467919
â€˜**;0.013543789043676679
from;0.015238910057587287
transformers;0.015096017670710248
import;0.01212091416422121
AutoTokenizer,;0.02617908712595901
BertModel;0.01333130483405612
import;0.013285829106018885
torch;0.01195920309662608
tokenizer;0.014729447141008671
=;0.012563102231856835
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.17827694765356478
model;0.011606254415906129
=;0.011946416260386417
"BertModel.from_pretrained(""bert-base-uncased"")";0.045116560792000285
inputs;0.01209919185715978
=;0.011146224606962223
"tokenizer(""Hello,";0.031412546464737484
my;0.009645058191663319
dog;0.011458179999893361
is;0.009798938302098607
"cute"",";0.012234408354016107
"return_tensors=""pt"")";0.019445338677283083
outputs;0.011176786275058732
=;0.009894175816996576
model(**inputs);0.018530839790854715
last_hidden_states;0.017934479125084907
=;0.010021919385757043
outputs.last_hidden_state;0.014769741432453997
***;0.00906341691062423
