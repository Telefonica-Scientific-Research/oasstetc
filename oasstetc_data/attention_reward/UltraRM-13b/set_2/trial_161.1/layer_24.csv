text;attention
The;0.016417976138627048
easiest;0.016200321092554624
way;0.01721598154213456
to;0.014331483915567196
import;0.01715710126898924
the;0.014250148402035095
BERT;0.0167674627127015
language;0.014700189236123016
model;0.016145765660804938
into;0.013858535162425035
python;0.01526760726070715
for;0.01384050520315407
use;0.014824728035333691
with;0.013255615899038552
PyTorch;0.019720799844917425
is;0.016342094738067737
using;0.014020278381041987
the;0.013361823433019709
Hugging;0.015720933961215634
Face;0.014525310913464259
Transformer's;0.04424808396704573
library,;0.01895006434201874
which;0.014022919672117492
has;0.013546042214666598
built;0.013945127917450903
in;0.01380632994462716
methods;0.014799713482798578
for;0.01306826320299503
pre-training,;0.017373436809159468
inference,;0.015524943552057033
and;0.013094125402988549
deploying;0.014918650534867406
BERT.;0.019253560179552487
â€˜**;0.019663264199313482
from;0.014976561109277768
transformers;0.014983560795605115
import;0.01373356862208565
AutoTokenizer,;0.016900019888105567
BertModel;0.01488518594974123
import;0.01487446358116138
torch;0.014666494010690197
tokenizer;0.01591261989011556
=;0.013931240695201547
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.036323963080729924
model;0.013779173125863232
=;0.013117772455673183
"BertModel.from_pretrained(""bert-base-uncased"")";0.03994115886586779
inputs;0.013635381222715327
=;0.013134168605607429
"tokenizer(""Hello,";0.01642294487396195
my;0.01316706758868882
dog;0.013307194612197485
is;0.012997429454752704
"cute"",";0.014401258486586353
"return_tensors=""pt"")";0.0159529553908999
outputs;0.013076609269585232
=;0.012877436844447594
model(**inputs);0.015608684531263788
last_hidden_states;0.01423980748465459
=;0.012750262715847182
outputs.last_hidden_state;0.013666889374989836
***;0.012596939250102449
