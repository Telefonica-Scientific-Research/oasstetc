text;attention
The;0.012353919965825261
easiest;0.01464383969434661
way;0.012041672511269027
to;0.011599460618579607
import;0.013607424398281512
the;0.01117488919546992
BERT;0.016579117189746858
language;0.011819470545151362
model;0.011806852376568948
into;0.011972550753831227
python;0.014464715690973497
for;0.01173181471272979
use;0.011300020192223799
with;0.011327919338660496
PyTorch;0.019919903322875028
is;0.011776826182371681
using;0.011640859068980105
the;0.011046650364021502
Hugging;0.016766688793621577
Face;0.011789223411373891
Transformer's;0.019595908124630395
library,;0.015262719744311367
which;0.01183811420402835
has;0.011313846355870178
built;0.011781358339815464
in;0.011028717323179886
methods;0.011694728895559644
for;0.011085030115617546
pre-training,;0.018216182466728764
inference,;0.0140847347268471
and;0.011002840731990095
deploying;0.013723511074863425
BERT.;0.016506310140099522
â€˜**;0.015968165584877982
from;0.011438312991912896
transformers;0.013536847446888501
import;0.012217275425554265
AutoTokenizer,;0.018312990541820947
BertModel;0.013498979350215531
import;0.011916149646917245
torch;0.013575583482274603
tokenizer;0.012822476168387555
=;0.012010250924310528
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.11882399848210987
model;0.010853191283179036
=;0.01165846126929084
"BertModel.from_pretrained(""bert-base-uncased"")";0.08185940979027112
inputs;0.011131217270435827
=;0.011521067187602693
"tokenizer(""Hello,";0.01940436385949197
my;0.010628872129337726
dog;0.010840215104423392
is;0.010734188795059212
"cute"",";0.013745173058811028
"return_tensors=""pt"")";0.024709783101348217
outputs;0.010965125917640976
=;0.0113111654269715
model(**inputs);0.018720790504513994
last_hidden_states;0.014753743475914276
=;0.010691694420872895
outputs.last_hidden_state;0.015742121028856835
***;0.01011056576026518
