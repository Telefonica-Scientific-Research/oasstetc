text;attention
The;0.012700896068978713
easiest;0.012068685479355532
way;0.012856388926716067
to;0.01136240625997011
import;0.014669195841288049
the;0.011476417931721272
BERT;0.030831065455038827
language;0.011802598586439943
model;0.013386712424065158
into;0.01149035882152437
python;0.012137038137744893
for;0.011143558564103066
use;0.011617322820040521
with;0.011010922186616218
PyTorch;0.017750918201504186
is;0.01380054550805855
using;0.012091312048040427
the;0.011629923690957429
Hugging;0.014885356079631915
Face;0.01670116950181398
Transformer's;0.09155379387522752
library,;0.01500865612496345
which;0.01116561379600256
has;0.011142856684562793
built;0.011053362284092967
in;0.011060732496615327
methods;0.011623091811141035
for;0.01102022115364427
pre-training,;0.014582002156996683
inference,;0.013290953360887571
and;0.010721393828684502
deploying;0.011557073598691722
BERT.;0.021201072166805217
â€˜**;0.014622775765208103
from;0.012084329777012492
transformers;0.014301735394714389
import;0.01186676558868765
AutoTokenizer,;0.01769839692706737
BertModel;0.014214330439384236
import;0.014033628565091352
torch;0.01241748257589247
tokenizer;0.013488803578526377
=;0.01186207914214054
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.05366437361942808
model;0.012335705784937012
=;0.011175651888261336
"BertModel.from_pretrained(""bert-base-uncased"")";0.07567158500926952
inputs;0.012430295077408058
=;0.010968371191034505
"tokenizer(""Hello,";0.016641261665561593
my;0.010907429213667094
dog;0.01150395271071422
is;0.010756483826060054
"cute"",";0.012702589828982822
"return_tensors=""pt"")";0.017692635009003928
outputs;0.011219782529729484
=;0.010697148702073842
model(**inputs);0.015484590329907951
last_hidden_states;0.014547313783902872
=;0.01071760564098413
outputs.last_hidden_state;0.01330323931423908
***;0.010596041249184689
