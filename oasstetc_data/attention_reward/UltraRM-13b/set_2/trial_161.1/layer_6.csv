text;attention
The;0.012072715628213196
easiest;0.011791549514002395
way;0.01187033412200233
to;0.011509743133903233
import;0.015844786952374824
the;0.011472038162262728
BERT;0.0157558358849099
language;0.011903972336829641
model;0.012002732253625416
into;0.012253680395638974
python;0.01197998617352325
for;0.01262542857073517
use;0.011971711437846333
with;0.012035047135855802
PyTorch;0.01365565972472999
is;0.012803314503143079
using;0.012327055337132131
the;0.011442856704202633
Hugging;0.014044340132221729
Face;0.01692215668272288
Transformer's;0.01933992541396762
library,;0.013753855473346184
which;0.011844990886521136
has;0.011515369302544957
built;0.010910392227858898
in;0.011769609728427019
methods;0.012264237051475899
for;0.011704465697689018
pre-training,;0.017504156816531193
inference,;0.012758177432283144
and;0.011331743585045096
deploying;0.012546722139019915
BERT.;0.01491784819690056
â€˜**;0.014763708099533934
from;0.016775102317272274
transformers;0.01824815579616445
import;0.014671130082919256
AutoTokenizer,;0.02343107298060392
BertModel;0.015256672672712888
import;0.014792100600877378
torch;0.013241976324961092
tokenizer;0.015751334616714848
=;0.014559046621995102
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.12448272490463337
model;0.013579454845978821
=;0.013620290385567528
"BertModel.from_pretrained(""bert-base-uncased"")";0.03975425897423248
inputs;0.013828079533595713
=;0.0134631263711146
"tokenizer(""Hello,";0.025063836943862668
my;0.011357104395144495
dog;0.01282515749759243
is;0.011225995144233149
"cute"",";0.013559871392127084
"return_tensors=""pt"")";0.021944734725698443
outputs;0.012672068717286237
=;0.01190822549936758
model(**inputs);0.0182766018794766
last_hidden_states;0.016918027313681985
=;0.011450757198291176
outputs.last_hidden_state;0.013290600271529743
***;0.010842345155344615
