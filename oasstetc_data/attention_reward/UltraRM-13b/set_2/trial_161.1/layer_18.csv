text;attention
The;0.00980239659193055
easiest;0.009636926739197997
way;0.009948470707889906
to;0.009133735157505095
import;0.01150585518657313
the;0.00926741603195228
BERT;0.020371104961178128
language;0.010195774122229934
model;0.010809496086077265
into;0.009124501546970904
python;0.010036463259101234
for;0.009106885458415717
use;0.009413618626877347
with;0.009019871157616407
PyTorch;0.015195442831283974
is;0.011094575890333113
using;0.00997765831334463
the;0.009772479013487748
Hugging;0.013338164502494521
Face;0.016287741872003846
Transformer's;0.20314791164256493
library,;0.012588472952943597
which;0.009287576498764417
has;0.008985292905882422
built;0.008838374479255298
in;0.008991391982960926
methods;0.009497215401414115
for;0.008977407960924586
pre-training,;0.012725421073868265
inference,;0.011152593004737286
and;0.008760714574160645
deploying;0.009414827444236943
BERT.;0.01785341893949294
â€˜**;0.0108974420531711
from;0.009803810477269852
transformers;0.013595952014900225
import;0.010391164339859456
AutoTokenizer,;0.016051929306967184
BertModel;0.012266464819790615
import;0.010910656468628292
torch;0.010215892463724976
tokenizer;0.010802731483039662
=;0.009978553994002097
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04502637184485805
model;0.010650411749764286
=;0.009299225988819285
"BertModel.from_pretrained(""bert-base-uncased"")";0.1070174610256947
inputs;0.010157324024783547
=;0.009100041602905281
"tokenizer(""Hello,";0.014727339122265898
my;0.008825707145079038
dog;0.009458573702966785
is;0.008746479501878287
"cute"",";0.009824912507888651
"return_tensors=""pt"")";0.016418325903542925
outputs;0.009160821203491347
=;0.008869252034194853
model(**inputs);0.013789366958128687
last_hidden_states;0.011928670667796761
=;0.008737846193812307
outputs.last_hidden_state;0.011428979310323242
***;0.008659095172782759
