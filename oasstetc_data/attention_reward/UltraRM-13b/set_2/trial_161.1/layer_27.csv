text;attention
The;0.015473355218857267
easiest;0.015995721983790376
way;0.016129019852182475
to;0.013928109625520114
import;0.016746529593199545
the;0.013772192994918091
BERT;0.01712818256588605
language;0.01468446220069789
model;0.015208079509706035
into;0.013479004920544447
python;0.014716284402252597
for;0.013463596973862015
use;0.014732934615159003
with;0.012998430652264293
PyTorch;0.0194503597927524
is;0.01588468685155849
using;0.01399501628711767
the;0.013225132837875339
Hugging;0.018772280843356928
Face;0.015053482733883686
Transformer's;0.03667669340988469
library,;0.018159420089249447
which;0.013832887463845861
has;0.013372677555695758
built;0.013922189417497908
in;0.01353066852419188
methods;0.01417694638551228
for;0.012834865100202526
pre-training,;0.018644413714159452
inference,;0.015417360364880952
and;0.012882435864378965
deploying;0.014886799896228804
BERT.;0.01984211717840334
â€˜**;0.019086240677826365
from;0.014256882771088235
transformers;0.01608880895606916
import;0.013945332803222703
AutoTokenizer,;0.01999999748547267
BertModel;0.015292909472109307
import;0.014282910722135217
torch;0.015031178311769592
tokenizer;0.015779765525387426
=;0.013490723547576697
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.05339929194754761
model;0.013308027165023384
=;0.012772084591828967
"BertModel.from_pretrained(""bert-base-uncased"")";0.03896519390992819
inputs;0.013133667581540913
=;0.012626207843025723
"tokenizer(""Hello,";0.0163589822891649
my;0.012633353688996941
dog;0.012743262011469293
is;0.012522983270703626
"cute"",";0.01389339373968509
"return_tensors=""pt"")";0.016336271501798547
outputs;0.012520301455600964
=;0.01237311455507927
model(**inputs);0.014900402964832698
last_hidden_states;0.013819529063408425
=;0.012224046097206153
outputs.last_hidden_state;0.013151773366948437
***;0.012047023264036899
