text;attention
The;0.001636300770328748
easiest;0.0012501536586182794
way;0.0013835244361876673
to;0.0009374871765114711
import;0.001563881452968345
the;0.0009634670088217616
BERT;0.00785843471700027
language;0.0008887250433555911
model;0.0009460287871210795
into;0.000715277337514401
python;0.0008377501251051791
for;0.000729546227976401
use;0.0008447198054460672
with;0.0006286025364766727
PyTorch;0.018649580952598322
is;0.0009542212149548208
using;0.0008157174368849879
the;0.000706727931450823
Hugging;0.009998791493595905
Face;0.0008745103488806706
Transformer's;0.02023129187583225
library,;0.0016056242757907994
which;0.0006091828363173285
has;0.0005530202348755577
built;0.0005909478813297625
in;0.0005398410465439864
methods;0.0006059502563556787
for;0.0004917504417434932
pre-training,;0.003996986301375408
inference,;0.0009343947144980099
and;0.00047959326890702116
deploying;0.0008329412242433588
BERT.;0.0029821776816088548
â€˜**;0.0015028098377476858
from;0.0006549551107766376
transformers;0.0011638147267519201
import;0.0006066374551571228
AutoTokenizer,;0.004130080103335388
BertModel;0.00133251918663888
import;0.0006253108116322483
torch;0.0009940048762208777
tokenizer;0.0009653656946232969
=;0.0005286949871812993
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7753138447278627
model;0.0004924713365069134
=;0.00044738944113175116
"BertModel.from_pretrained(""bert-base-uncased"")";0.11473943831976649
inputs;0.000429040804380482
=;0.0004000792422376635
"tokenizer(""Hello,";0.0012142520168401034
my;0.0003981372551048146
dog;0.0004046727238191854
is;0.00038434180396663373
"cute"",";0.0006065124955243025
"return_tensors=""pt"")";0.0015005929394025258
outputs;0.00037663169169769336
=;0.00036609104201033957
model(**inputs);0.0008905149975463984
last_hidden_states;0.0006483318391795921
=;0.00034624121644690094
outputs.last_hidden_state;0.0005771385839390854
***;0.000322934231352179
