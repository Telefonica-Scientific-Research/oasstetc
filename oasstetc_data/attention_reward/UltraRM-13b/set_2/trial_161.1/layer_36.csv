text;attention
The;0.007435157952808081
easiest;0.006543415616538403
way;0.00715782473969987
to;0.005198864631785702
import;0.007988132432916947
the;0.005436894634875117
BERT;0.01967638863600181
language;0.005143506416506502
model;0.005942921060826886
into;0.004411285069580997
python;0.005067145296352215
for;0.004472675936470168
use;0.005009765862882423
with;0.003922066849484056
PyTorch;0.026329316401389354
is;0.005736318045908907
using;0.004620945399038105
the;0.004176253905381266
Hugging;0.017883663964649438
Face;0.005311512924670239
Transformer's;0.043683669661489184
library,;0.008369718724176979
which;0.00386179010309501
has;0.003608966190457108
built;0.003835971349369289
in;0.0035668432859941755
methods;0.00393432915920366
for;0.0033185788745794136
pre-training,;0.013357489189611898
inference,;0.005285253679953094
and;0.003196972364627987
deploying;0.0047490994790578825
BERT.;0.013266460358909186
â€˜**;0.0069416978551014785
from;0.004120643934827862
transformers;0.006285229252938458
import;0.003859035708639502
AutoTokenizer,;0.013524529306527765
BertModel;0.005976347352093742
import;0.003998650158205559
torch;0.005150342868116147
tokenizer;0.005513375904872952
=;0.0035464632833589567
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4755839890800516
model;0.003458080945084095
=;0.003122818625759784
"BertModel.from_pretrained(""bert-base-uncased"")";0.13334904515177207
inputs;0.003100433552887946
=;0.002874230597628811
"tokenizer(""Hello,";0.00650537176945942
my;0.0028488966059650907
dog;0.0028986829952433956
is;0.0027760553532020604
"cute"",";0.0038498676086660176
"return_tensors=""pt"")";0.006646510672207773
outputs;0.0027484610165634297
=;0.0026824277817230303
model(**inputs);0.004979107999634429
last_hidden_states;0.00382480773171146
=;0.002570300401162169
outputs.last_hidden_state;0.0033328776695756165
***;0.0024325206187280446
