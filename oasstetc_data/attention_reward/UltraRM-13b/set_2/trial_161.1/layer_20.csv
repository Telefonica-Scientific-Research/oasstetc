text;attention
The;0.01291288321990373
easiest;0.012476797800553755
way;0.01314808464225182
to;0.012281603577256005
import;0.015417286548787728
the;0.012714141698520484
BERT;0.0213104861353745
language;0.01289950024876695
model;0.01321834191811943
into;0.012306928976433191
python;0.01278080677971752
for;0.012396127790040573
use;0.012957765520296592
with;0.012462857502777789
PyTorch;0.017882887209691237
is;0.014391305404016062
using;0.01313616253182858
the;0.012464244470059347
Hugging;0.01630959131269857
Face;0.015061929235908034
Transformer's;0.07203213308664139
library,;0.015817433054868605
which;0.0122635983405219
has;0.01211301680605551
built;0.012058371833629799
in;0.012083941848629707
methods;0.012589573246708733
for;0.012288541254848793
pre-training,;0.014739891861710886
inference,;0.0137766638468465
and;0.011620178889614002
deploying;0.01264060886086286
BERT.;0.021490250417611205
â€˜**;0.01594786751142734
from;0.013241791720160597
transformers;0.014207389457679988
import;0.01300758642682286
AutoTokenizer,;0.01719973596169754
BertModel;0.015030292195998217
import;0.014761142160711987
torch;0.013322986616182522
tokenizer;0.014644388441216401
=;0.012341507501408791
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.05436446313940787
model;0.013239786331134798
=;0.012128824749674005
"BertModel.from_pretrained(""bert-base-uncased"")";0.062195611074926244
inputs;0.012879494400183473
=;0.011784027042040807
"tokenizer(""Hello,";0.01580077391100609
my;0.01186344965880071
dog;0.011738477544236544
is;0.011486419116001947
"cute"",";0.013601758629754579
"return_tensors=""pt"")";0.017967184327421108
outputs;0.012149487435844649
=;0.011483317498762474
model(**inputs);0.01753845099455201
last_hidden_states;0.014629263493632144
=;0.011531333848570554
outputs.last_hidden_state;0.014460073727964371
***;0.011409179211227676
