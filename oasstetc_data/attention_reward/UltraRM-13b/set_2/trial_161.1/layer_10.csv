text;attention
The;0.007959132046361072
easiest;0.007695226077458563
way;0.007014538959366941
to;0.006621547299379771
import;0.01525986151939288
the;0.00746081516994452
BERT;0.02953509960624536
language;0.007186753999446254
model;0.009642540908723116
into;0.007967254854060146
python;0.007959772039361103
for;0.007010484570234456
use;0.0075647128472451156
with;0.00688145264612146
PyTorch;0.012509039820783626
is;0.007943075313427715
using;0.007835030999068834
the;0.007310317573863973
Hugging;0.008061984312740474
Face;0.01687333326057227
Transformer's;0.057733391773299586
library,;0.011493118168201602
which;0.0068631563085988405
has;0.006715327170849399
built;0.005984915044795444
in;0.007784363294221349
methods;0.00726036386881125
for;0.007208203283450652
pre-training,;0.01693424693641418
inference,;0.010416672283631401
and;0.00640174082020014
deploying;0.008290931595179971
BERT.;0.015806986297157488
â€˜**;0.009468731559315978
from;0.009730776847689847
transformers;0.01128577400018673
import;0.011599571664412087
AutoTokenizer,;0.03205548100799135
BertModel;0.013467422284819366
import;0.010869517729937874
torch;0.009664687911078267
tokenizer;0.015299821086880569
=;0.01056390939788927
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.24133709895507965
model;0.008077856502009442
=;0.008518378758604476
"BertModel.from_pretrained(""bert-base-uncased"")";0.09702004354635299
inputs;0.008755521718914706
=;0.007628743408774772
"tokenizer(""Hello,";0.025322434963565572
my;0.0061373527975493525
dog;0.006353721080490434
is;0.006227805432271189
"cute"",";0.008023941460055257
"return_tensors=""pt"")";0.017170631093441597
outputs;0.00742606395288028
=;0.006816611975524736
model(**inputs);0.01585071600935248
last_hidden_states;0.009496709535434363
=;0.006829860315370032
outputs.last_hidden_state;0.007971047196811082
***;0.005844377138707466
