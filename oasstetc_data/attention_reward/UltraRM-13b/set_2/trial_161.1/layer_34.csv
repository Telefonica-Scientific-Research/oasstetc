text;attention
The;0.01611902069413216
easiest;0.015480233081802044
way;0.01603435895168258
to;0.01332320358564867
import;0.017804042504797144
the;0.013720587212214315
BERT;0.032275952590229286
language;0.014526989977247133
model;0.015204980160717927
into;0.012656787419109186
python;0.014354473683840201
for;0.012613178241670952
use;0.014124433881668627
with;0.011992063546158555
PyTorch;0.031924753821090855
is;0.014467276243114748
using;0.012938762815424453
the;0.01217369510943611
Hugging;0.023247890776328647
Face;0.01414899016650904
Transformer's;0.03711376594153493
library,;0.017283078677177606
which;0.011745692877835986
has;0.011271734839959018
built;0.011886687273251562
in;0.011374499023473818
methods;0.01216863576546995
for;0.010821556506052392
pre-training,;0.020422026781401256
inference,;0.013861336946900123
and;0.01070973575095174
deploying;0.012903590302422455
BERT.;0.0219584726822781
â€˜**;0.015732055423906502
from;0.012136598219799507
transformers;0.014897566026503052
import;0.011911347957041395
AutoTokenizer,;0.020936405623448014
BertModel;0.014825451088058827
import;0.012022123818910884
torch;0.013894987959742259
tokenizer;0.013893066862103943
=;0.011265149129455821
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08979557215765599
model;0.01122202357717638
=;0.01059907353616809
"BertModel.from_pretrained(""bert-base-uncased"")";0.049764348721849865
inputs;0.01065005344566752
=;0.010207019450070215
"tokenizer(""Hello,";0.014195056845353918
my;0.010172920465276696
dog;0.010306036121396439
is;0.010058122977356882
"cute"",";0.011400147564196137
"return_tensors=""pt"")";0.014059661032114923
outputs;0.01002066468431137
=;0.009919847625386596
model(**inputs);0.012369925543918142
last_hidden_states;0.011232937052603923
=;0.009747500174162304
outputs.last_hidden_state;0.010526798629206337
***;0.009585050455626487
