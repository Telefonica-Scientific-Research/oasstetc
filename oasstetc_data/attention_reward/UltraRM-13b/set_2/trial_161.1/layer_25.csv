text;attention
The;0.01656666535583339
easiest;0.01683107402580146
way;0.017053296696202813
to;0.015251881352157604
import;0.017163220077275883
the;0.015287893828151442
BERT;0.017505308902597713
language;0.015659654476182696
model;0.016213024152044137
into;0.015070268405148987
python;0.015934742467336012
for;0.015014007210864346
use;0.015923999870227704
with;0.014762723883031511
PyTorch;0.018333870783461586
is;0.016462889812989424
using;0.015456885215624447
the;0.0148616150159771
Hugging;0.01660387804595997
Face;0.015871597294909466
Transformer's;0.02409140989812374
library,;0.01750836912950402
which;0.015103008593124185
has;0.014865047582251545
built;0.015057402200301457
in;0.015099496496748471
methods;0.01563187968769647
for;0.014624857620134848
pre-training,;0.017019324767955576
inference,;0.015856568210752833
and;0.014612071557566999
deploying;0.015508187713519733
BERT.;0.01789747771254195
â€˜**;0.01970654238296589
from;0.015597147930884986
transformers;0.015923889769863073
import;0.01512865722901049
AutoTokenizer,;0.017272498973640388
BertModel;0.01578595138190668
import;0.015662573102834483
torch;0.015605377672200285
tokenizer;0.016280735542697555
=;0.01514809720779853
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.024973865236625502
model;0.01500411472846073
=;0.014660138471303728
"BertModel.from_pretrained(""bert-base-uncased"")";0.023315772250754128
inputs;0.014964659662720967
=;0.014655240233560208
"tokenizer(""Hello,";0.016471902404847836
my;0.01462437660787808
dog;0.01472333983672521
is;0.014515358951964086
"cute"",";0.015227927621517635
"return_tensors=""pt"")";0.01614663436289141
outputs;0.014578658581757982
=;0.014502544224879461
model(**inputs);0.015899808483778626
last_hidden_states;0.015229429992646325
=;0.014442203898925369
outputs.last_hidden_state;0.014913251398669627
***;0.014335703814291624
