text;attention
The;0.015770108452241544
easiest;0.015621509763089047
way;0.01608065570026722
to;0.014244769558308647
import;0.016814128144938933
the;0.01430239693729647
BERT;0.01977325092812197
language;0.015006216312598991
model;0.015475710083795686
into;0.013674737649049131
python;0.014570907296562545
for;0.013622293863259518
use;0.014770988619485661
with;0.013230702201215947
PyTorch;0.021733353899354504
is;0.015285919513070753
using;0.01400350034659275
the;0.013440534226844117
Hugging;0.02110344702816364
Face;0.014837619370962006
Transformer's;0.032660946096232636
library,;0.017432166858638454
which;0.01347799836662818
has;0.013094360548771895
built;0.013730554697175143
in;0.013163142833501708
methods;0.013668141592476717
for;0.01266796248619787
pre-training,;0.01874297583193412
inference,;0.014939451313305985
and;0.012656174733699567
deploying;0.014529009955148981
BERT.;0.019603451671725895
â€˜**;0.016367959896266947
from;0.013787406349425711
transformers;0.016052875844413606
import;0.013711616173959065
AutoTokenizer,;0.019532337388517146
BertModel;0.01528855877622815
import;0.013779555732529756
torch;0.014856958449792181
tokenizer;0.015299350528739219
=;0.013180541006950434
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.058550488915998795
model;0.013182506680212615
=;0.01261082594198659
"BertModel.from_pretrained(""bert-base-uncased"")";0.04259818112620063
inputs;0.012745551269723264
=;0.012374585375964557
"tokenizer(""Hello,";0.016350205263825097
my;0.012335834893229649
dog;0.012426937085374096
is;0.012228643677365796
"cute"",";0.013643394806040965
"return_tensors=""pt"")";0.01629069013544266
outputs;0.012217764323274165
=;0.012090338941341115
model(**inputs);0.014846835154259061
last_hidden_states;0.013488997769504012
=;0.011902200130041077
outputs.last_hidden_state;0.01281563454309289
***;0.011714136939644154
