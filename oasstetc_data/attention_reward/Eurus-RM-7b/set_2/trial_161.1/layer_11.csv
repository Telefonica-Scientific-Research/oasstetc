text;attention
The;0.008691487736829475
easiest;0.005854377019106698
way;0.005064194726860129
to;0.005273331809179433
import;0.007822948432853534
the;0.006676388987397807
BERT;0.02486562904804298
language;0.006306426437998246
model;0.007877523894456319
into;0.006389059128017225
python;0.007197186456473293
for;0.005506776881294191
use;0.005937708482921633
with;0.005294562332280782
PyTorch;0.011611701865253985
is;0.005776829585377327
using;0.0053915372272810595
the;0.005438052843172033
Hugging;0.011892721517325084
Face;0.00823339440258634
Transformer's;0.01714219583283325
library,;0.007908470667509741
which;0.005059463032606431
has;0.00495576890394269
built;0.004475948354263667
in;0.0056066254410572286
methods;0.0051609233086419605
for;0.005105147142960162
pre-training,;0.0131674278811754
inference,;0.008386593043738049
and;0.004980532590252885
deploying;0.005620825655788886
BERT.;0.010595071890944266
â€˜**;0.007102059297365216
from;0.011303755820503537
transformers;0.008720939624808908
import;0.008455600579436971
AutoTokenizer,;0.05508947245938808
BertModel;0.016914481496085643
import;0.00889496874898731
torch;0.007438327350185731
tokenizer;0.016705076550654688
=;0.009167357128711357
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.38810718468338923
model;0.006447731170303985
=;0.00598016442417521
"BertModel.from_pretrained(""bert-base-uncased"")";0.07073955125935452
inputs;0.006393876762826897
=;0.005771338821448468
"tokenizer(""Hello,";0.01536054681725747
my;0.004679802882789428
dog;0.005056621331108304
is;0.004699459631862444
"cute"",";0.006285068025404122
"return_tensors=""pt"")";0.022971029558125085
outputs;0.00535828025342665
=;0.0048928608108007665
model(**inputs);0.014754721190319677
last_hidden_states;0.010941683771770436
=;0.004987303807120279
outputs.last_hidden_state;0.007239769202481008
***;0.0042741339794865555
