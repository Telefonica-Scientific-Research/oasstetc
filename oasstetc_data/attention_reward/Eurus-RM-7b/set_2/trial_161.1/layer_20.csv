text;attention
The;0.006580727465409974
easiest;0.0064973960919492875
way;0.006038791757456683
to;0.006298661865736393
import;0.007311410542531141
the;0.0070172722632386485
BERT;0.02051763511111329
language;0.0058375679937512385
model;0.00667222685759139
into;0.005973430736884018
python;0.007228334603312403
for;0.006000907451263249
use;0.006261406545702232
with;0.006095250225725903
PyTorch;0.01295821763144077
is;0.005967963511084564
using;0.006980935627679944
the;0.006128298820769931
Hugging;0.012477790639541833
Face;0.00861160583167855
Transformer's;0.01829092696446327
library,;0.008534357512568872
which;0.005776927684262518
has;0.006354807291228257
built;0.005606615924885528
in;0.006237871920499768
methods;0.006133030319343892
for;0.006446190405665353
pre-training,;0.01432947463144675
inference,;0.008116582839118952
and;0.0054286594488659546
deploying;0.006391405404231471
BERT.;0.009911872076716045
â€˜**;0.007419249806326246
from;0.008863760758286855
transformers;0.008934486077003251
import;0.007164938766512036
AutoTokenizer,;0.025425712271961918
BertModel;0.009499292406253367
import;0.007353964331526364
torch;0.009560726648447402
tokenizer;0.01360108135200349
=;0.007304663208339397
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4214739287867787
model;0.006754523536569305
=;0.006273357075340883
"BertModel.from_pretrained(""bert-base-uncased"")";0.06655775575075754
inputs;0.008782302511550247
=;0.006774715309880445
"tokenizer(""Hello,";0.016858957896073362
my;0.005513553044391493
dog;0.005615584596250003
is;0.005344261352842067
"cute"",";0.006412081598244234
"return_tensors=""pt"")";0.01818332166607921
outputs;0.006861476410956866
=;0.005694403694884464
model(**inputs);0.010999892096060192
last_hidden_states;0.011303794830301145
=;0.005752385725221588
outputs.last_hidden_state;0.009436758026636403
***;0.005264516467363368
