text;attention
The;0.011992535211347183
easiest;0.012657242792063764
way;0.01201936898417723
to;0.011634001008172166
import;0.013080428332993808
the;0.011531620219111575
BERT;0.017786556475398823
language;0.01058821552856474
model;0.011572961240346923
into;0.011141964657479886
python;0.01142331404022925
for;0.011262966345502064
use;0.01156564768208413
with;0.010720081366140251
PyTorch;0.017178581637491877
is;0.011420179516761539
using;0.012236693906126549
the;0.01115910879067633
Hugging;0.02336740281308657
Face;0.014772291719461851
Transformer's;0.029481148254469392
library,;0.013679088635502061
which;0.01143275429041682
has;0.011693530890759338
built;0.010883137367044892
in;0.012703375965188479
methods;0.01226910646986266
for;0.011090061814372882
pre-training,;0.016738300326507666
inference,;0.014205076660084802
and;0.011002890989976423
deploying;0.01132193463279171
BERT.;0.01590639904717434
â€˜**;0.01507742836602986
from;0.013946825628499367
transformers;0.015563892091991102
import;0.012485174917292119
AutoTokenizer,;0.017991688972908284
BertModel;0.013414875718700297
import;0.013437550812670212
torch;0.013555129697979522
tokenizer;0.016000883320136834
=;0.011974771571805428
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.13042149950455467
model;0.011824744699104486
=;0.011507767483251382
"BertModel.from_pretrained(""bert-base-uncased"")";0.05743239925204063
inputs;0.012985599782586397
=;0.011120694027023285
"tokenizer(""Hello,";0.023984654809497633
my;0.010450611716592525
dog;0.010703079500558713
is;0.010529260941952983
"cute"",";0.012643031318151485
"return_tensors=""pt"")";0.02203860268500355
outputs;0.011175800013021344
=;0.010482015688498658
model(**inputs);0.01728671792631434
last_hidden_states;0.0151275975410143
=;0.010678907356565123
outputs.last_hidden_state;0.01452145558051235
***;0.010119371464375167
