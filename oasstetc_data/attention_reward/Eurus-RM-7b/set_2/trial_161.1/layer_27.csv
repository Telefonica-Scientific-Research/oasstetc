text;attention
The;0.00893780894189784
easiest;0.009029234940280698
way;0.01168102894170083
to;0.00863931096636165
import;0.010031144844530495
the;0.009650550486057738
BERT;0.011710202234759267
language;0.008439018694030628
model;0.008872293040616458
into;0.009316879122283647
python;0.009011562004672959
for;0.008749990583164103
use;0.008296031331343327
with;0.008598440748858734
PyTorch;0.01383244878801919
is;0.010353778949743434
using;0.009897100938432032
the;0.009123689455776499
Hugging;0.012006005027906332
Face;0.01070364733610349
Transformer's;0.026840310643591832
library,;0.01206037378141519
which;0.009414242651237682
has;0.009376208126219152
built;0.008142594866997407
in;0.008831404819543545
methods;0.008938943739326683
for;0.009161089780286352
pre-training,;0.01605827549022122
inference,;0.010417420371523304
and;0.008090315411263071
deploying;0.00901196989772077
BERT.;0.01838770731827265
â€˜**;0.016908030544070585
from;0.014128391217686075
transformers;0.011127413039698277
import;0.010598818342299771
AutoTokenizer,;0.013948643221748214
BertModel;0.011430017127121002
import;0.011468211478563646
torch;0.009934162729551778
tokenizer;0.011757110841631143
=;0.010322752251359929
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.20558761543416137
model;0.008345998164488149
=;0.008483208898424431
"BertModel.from_pretrained(""bert-base-uncased"")";0.13617955738071993
inputs;0.009457122723525214
=;0.009675978347251892
"tokenizer(""Hello,";0.020698998372252725
my;0.007797186053602156
dog;0.008039294042355415
is;0.007698864755647821
"cute"",";0.009948478485895564
"return_tensors=""pt"")";0.020826565241692033
outputs;0.007973125452948272
=;0.008236171122938032
model(**inputs);0.01570114634513564
last_hidden_states;0.013089697907264863
=;0.007832568445931547
outputs.last_hidden_state;0.013602114028739343
***;0.007591733729136666
