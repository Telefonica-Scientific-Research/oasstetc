text;attention
The;0.004674445129156918
easiest;0.002699573137375153
way;0.0023927981580997145
to;0.003629979398244328
import;0.00270571582487406
the;0.002948956766843729
BERT;0.016596987976936765
language;0.002652226611090184
model;0.0029710039740135775
into;0.0023736193742517485
python;0.002576658823643565
for;0.0025611557446933325
use;0.002630100256406075
with;0.00250482388752864
PyTorch;0.013615950825681082
is;0.0026169455431174454
using;0.0028830148604478472
the;0.0029955715881317415
Hugging;0.011239325518667248
Face;0.007036666757659101
Transformer's;0.02452029119278408
library,;0.004943347288685964
which;0.002359375935193902
has;0.0025400222948810123
built;0.0021458238013434515
in;0.0024403367961303257
methods;0.0023978197168072314
for;0.0023555566425026158
pre-training,;0.008947845841719414
inference,;0.004413305564902417
and;0.002230089514289546
deploying;0.00250023297099073
BERT.;0.004629753852257672
â€˜**;0.0031033134421136533
from;0.003413457300088575
transformers;0.004209392795851132
import;0.004731901612486148
AutoTokenizer,;0.07808612145832061
BertModel;0.008774073445110263
import;0.003815906639595758
torch;0.0037556633240747707
tokenizer;0.00403014781474769
=;0.003970461648392172
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.614941564014744
model;0.003302584064526345
=;0.002492078248345103
"BertModel.from_pretrained(""bert-base-uncased"")";0.03384424398115741
inputs;0.003924709627335839
=;0.0025987361214550096
"tokenizer(""Hello,";0.007729770790271547
my;0.0022235720048973586
dog;0.0022441488884597477
is;0.002128687246878024
"cute"",";0.002665711718491472
"return_tensors=""pt"")";0.019715603792522733
outputs;0.0027053899919322402
=;0.0023274335469766452
model(**inputs);0.005722060083446187
last_hidden_states;0.0053619970893083796
=;0.0023719925727823457
outputs.last_hidden_state;0.003019737284901446
***;0.0020602178814368
