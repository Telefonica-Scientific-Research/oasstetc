text;attention
The;0.012238809452883472
easiest;0.012374184412884593
way;0.013555648238393758
to;0.011872624325216144
import;0.01469699641195685
the;0.012330711424829149
BERT;0.01753803937446521
language;0.010496501869068467
model;0.013109765101530934
into;0.011154170451244698
python;0.011619355088494146
for;0.011034128117928843
use;0.011124174200519577
with;0.010522786982090178
PyTorch;0.014688775613142448
is;0.012064109865079034
using;0.013398028402973184
the;0.011557108922674557
Hugging;0.014772853079159215
Face;0.013387533921139137
Transformer's;0.029347893383097276
library,;0.016466616240324864
which;0.011733761500377408
has;0.012343228393304595
built;0.010733381257202447
in;0.01198321469038758
methods;0.011761352696915155
for;0.01133558392315839
pre-training,;0.016008090623244015
inference,;0.012626120949672944
and;0.010326595550735724
deploying;0.012513689803017187
BERT.;0.01683972431736911
â€˜**;0.013997047561862354
from;0.0192402590903771
transformers;0.014171398628338883
import;0.01332048112399174
AutoTokenizer,;0.017006230516068952
BertModel;0.011766672273130713
import;0.01375743308805677
torch;0.011849704025274693
tokenizer;0.01488269130784684
=;0.012204553585500924
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1007830793563
model;0.010865547754473927
=;0.011077537479182692
"BertModel.from_pretrained(""bert-base-uncased"")";0.09301613358013787
inputs;0.011587516788469974
=;0.011543838993180115
"tokenizer(""Hello,";0.029668439160531558
my;0.010067411000730475
dog;0.010802621353665009
is;0.010157660698823298
"cute"",";0.012305044807297468
"return_tensors=""pt"")";0.020863378113078763
outputs;0.010488683809512732
=;0.01057513913170184
model(**inputs);0.017703288972292466
last_hidden_states;0.014590494017411504
=;0.010275499231688977
outputs.last_hidden_state;0.014132093653735613
***;0.009744562312856654
