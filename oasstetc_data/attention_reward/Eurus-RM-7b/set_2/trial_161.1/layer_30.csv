text;attention
The;7.657681328619801e-05
easiest;6.558978567872325e-05
way;6.482965432387754e-05
to;6.740695889603561e-05
import;7.652092060540845e-05
the;8.318841802374937e-05
BERT;0.00017354244389509435
language;5.644587225679225e-05
model;5.565085561003349e-05
into;6.265578268291611e-05
python;6.494660961225413e-05
for;7.619507140445978e-05
use;5.842035769133524e-05
with;6.068203485992545e-05
PyTorch;0.00015446534657263111
is;6.052875515308211e-05
using;8.966026668310136e-05
the;8.316956826163298e-05
Hugging;0.00013913894384806835
Face;8.742022593299177e-05
Transformer's;0.0009650255988775983
library,;9.650950259274508e-05
which;5.782720786638814e-05
has;6.695779707419139e-05
built;5.819996943502111e-05
in;7.354765647655646e-05
methods;5.507636141181202e-05
for;6.841369801448489e-05
pre-training,;0.0003089790703220981
inference,;8.942753546034124e-05
and;5.387285586603761e-05
deploying;6.525414295254998e-05
BERT.;0.00014985388151166418
â€˜**;0.0001951289001940882
from;0.0001030525702250126
transformers;0.00010747793353726201
import;0.00016643209221029503
AutoTokenizer,;0.0008685138051309567
BertModel;0.0002219444404584537
import;8.666837613508336e-05
torch;7.990984620204219e-05
tokenizer;0.00019196028863686693
=;0.00012149761536474347
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.987700304747506
model;6.500495422013161e-05
=;5.790785815852494e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.004311379280660213
inputs;7.661165620587557e-05
=;6.621532081241941e-05
"tokenizer(""Hello,";0.00023509025301998054
my;5.292768374586001e-05
dog;5.114345878152687e-05
is;4.774371824670978e-05
"cute"",";8.804395005088755e-05
"return_tensors=""pt"")";0.0005074753381295701
outputs;5.382567377443102e-05
=;5.352806720476791e-05
model(**inputs);0.0001864671248228652
last_hidden_states;0.00019059276527870524
=;4.6450360028607415e-05
outputs.last_hidden_state;0.0002584763758350922
***;4.2245582283027974e-05
