text;attention
The;0.008393415515947193
easiest;0.008271389083345827
way;0.010425485756150198
to;0.008153862277624033
import;0.008711120931072193
the;0.0087949655859138
BERT;0.01209593075380809
language;0.007983885200431565
model;0.00843412757160448
into;0.008216295878041787
python;0.009826696505103368
for;0.008102929448483957
use;0.007963349502393356
with;0.008009640588565552
PyTorch;0.012314127678241856
is;0.008524213529499574
using;0.009512021163364304
the;0.008440862342600993
Hugging;0.011897514775765982
Face;0.009738539512978562
Transformer's;0.03715076973747239
library,;0.011911054597389035
which;0.008469702106145609
has;0.008736313527095599
built;0.008169417203525446
in;0.008925992240078081
methods;0.0094443735732879
for;0.008778418284721453
pre-training,;0.01706676259886442
inference,;0.010076262480159399
and;0.007851818768732834
deploying;0.008792039872149332
BERT.;0.016177998742786248
â€˜**;0.01621503244147549
from;0.01488091916571815
transformers;0.01308254443647087
import;0.010639747210276149
AutoTokenizer,;0.014721638952632938
BertModel;0.012592073454972139
import;0.01128838580278402
torch;0.010182897392966921
tokenizer;0.015863861010399317
=;0.010080440313559983
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2052092074242215
model;0.008826015213790703
=;0.008559471355092205
"BertModel.from_pretrained(""bert-base-uncased"")";0.14098175048885617
inputs;0.009198865952462545
=;0.008902661188539798
"tokenizer(""Hello,";0.018091646045061183
my;0.007594452906167115
dog;0.007907460643463109
is;0.00759208606518147
"cute"",";0.010677152436414015
"return_tensors=""pt"")";0.018749026511261126
outputs;0.008170656912890876
=;0.007955924711728539
model(**inputs);0.014842487491636042
last_hidden_states;0.01250683217773629
=;0.007752998429835694
outputs.last_hidden_state;0.013998166826409442
***;0.007574299704651711
