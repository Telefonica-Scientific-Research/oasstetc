text;attention
The;0.0038548268078026373
easiest;0.0030155333952798713
way;0.0027478233768669373
to;0.0032276286054714674
import;0.0032477015360027
the;0.003058430199106432
BERT;0.01178471388195808
language;0.0026186609242699293
model;0.0029049067159952094
into;0.002473056388344531
python;0.0028651669337391235
for;0.0024793771357384505
use;0.002543046766736892
with;0.0024300288853030777
PyTorch;0.011761799771707212
is;0.003009188972993395
using;0.0029480650706492644
the;0.00290307982195352
Hugging;0.008397954505110095
Face;0.005629717019223157
Transformer's;0.016813212447159012
library,;0.004924506961101845
which;0.0024250314760396953
has;0.002580854369177466
built;0.002225289073699688
in;0.0026760352687408466
methods;0.002649926474666702
for;0.0027681540042926124
pre-training,;0.009903714199055315
inference,;0.004612190457389765
and;0.0022185030432433873
deploying;0.0026000904757073283
BERT.;0.0049055384915492935
â€˜**;0.0031054539703017814
from;0.004151810860296309
transformers;0.004436354068444979
import;0.003971876568695784
AutoTokenizer,;0.06339734384384561
BertModel;0.006983799987560575
import;0.004329949382246733
torch;0.004668177543192548
tokenizer;0.0053932195318041635
=;0.005037684958231904
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6064924598575664
model;0.0033515530830615964
=;0.002841083841976752
"BertModel.from_pretrained(""bert-base-uncased"")";0.05155345162504385
inputs;0.004348537743865745
=;0.0029659716934461932
"tokenizer(""Hello,";0.011591473604486074
my;0.002310932399346442
dog;0.0024545497738844837
is;0.002228133412443601
"cute"",";0.0032344242728916932
"return_tensors=""pt"")";0.02527575474874833
outputs;0.002898103739380824
=;0.00255588431940341
model(**inputs);0.010081248372875566
last_hidden_states;0.007022147064536888
=;0.002483589515657406
outputs.last_hidden_state;0.0035290070213158057
***;0.00210226973937345
