text;attention
The;0.011256029638672864
easiest;0.010613963968289949
way;0.009632058379802539
to;0.009309259037025303
import;0.012585348874158117
the;0.009349599882032744
BERT;0.015228404706936904
language;0.00893355800551193
model;0.009984042170609596
into;0.010993021233196126
python;0.009821287595291007
for;0.010354977734608861
use;0.009874159530585723
with;0.010413352710507365
PyTorch;0.012401927986547103
is;0.012485677772795401
using;0.010628226640398397
the;0.009404857980608108
Hugging;0.014140807711696818
Face;0.012229170796517494
Transformer's;0.020096419686193818
library,;0.013120723324145935
which;0.009848983969905469
has;0.009749761677324999
built;0.008532382723222877
in;0.01043511363829691
methods;0.009510137068987694
for;0.010937500018150643
pre-training,;0.0158222013937305
inference,;0.011933508794939828
and;0.009001945589355877
deploying;0.011311495092768343
BERT.;0.013481678305101288
â€˜**;0.01640556776925099
from;0.017298541800273442
transformers;0.012473236041183616
import;0.012409495487656618
AutoTokenizer,;0.023050164433160783
BertModel;0.01447451057718232
import;0.012314949044804025
torch;0.009920779955958628
tokenizer;0.012792020212394152
=;0.014515749471172389
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1821790037457555
model;0.01067206605889551
=;0.010896275470364446
"BertModel.from_pretrained(""bert-base-uncased"")";0.08675784881102054
inputs;0.011079424813253955
=;0.009866918659724567
"tokenizer(""Hello,";0.02500657625243794
my;0.008776346336150232
dog;0.008989459971877885
is;0.008417378938805812
"cute"",";0.01048086933822356
"return_tensors=""pt"")";0.020758650253934266
outputs;0.010536156653245962
=;0.009347252915063737
model(**inputs);0.019218794981191174
last_hidden_states;0.018669518846521783
=;0.009526275456312154
outputs.last_hidden_state;0.011703804347401064
***;0.008040779718866301
