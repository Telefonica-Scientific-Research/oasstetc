text;attention
The;0.010526837092344999
easiest;0.012353377417356416
way;0.011263897457120486
to;0.010337340998349641
import;0.01312743954847022
the;0.009925501377311215
BERT;0.015219517297266418
language;0.009941727365950864
model;0.01114458633109385
into;0.010904937438434943
python;0.011066004978227838
for;0.01094961377202627
use;0.010499534007605523
with;0.010411617396886405
PyTorch;0.012196054229213056
is;0.012378703989779864
using;0.01115695191297913
the;0.010082162910877449
Hugging;0.015144454256533057
Face;0.02050801722029647
Transformer's;0.020526641497213536
library,;0.014094140589495967
which;0.010512299122081402
has;0.010534852030443992
built;0.010060636157518973
in;0.0117519136881341
methods;0.011408215697606618
for;0.011328863591160329
pre-training,;0.017420408206091104
inference,;0.01270653184386199
and;0.009719838953423598
deploying;0.011479171151562198
BERT.;0.013308578253136699
â€˜**;0.013454203398194644
from;0.014112112868607217
transformers;0.017195192751524854
import;0.012394106875648528
AutoTokenizer,;0.02084732387642218
BertModel;0.015724955823015557
import;0.012541662306375613
torch;0.011579274423457318
tokenizer;0.015010148064779403
=;0.015197725726421094
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.15098681345669457
model;0.01233196985068748
=;0.012142451394942109
"BertModel.from_pretrained(""bert-base-uncased"")";0.07419347391460861
inputs;0.012627512402486935
=;0.011055109708943713
"tokenizer(""Hello,";0.02550868408356462
my;0.00995366888072203
dog;0.010578568752009665
is;0.009567940321204674
"cute"",";0.011331779623959884
"return_tensors=""pt"")";0.020916318615415673
outputs;0.011499717757014116
=;0.010047052528269929
model(**inputs);0.016647384998821634
last_hidden_states;0.016989137507538644
=;0.009994659080201165
outputs.last_hidden_state;0.012307204472060006
***;0.009273478756553532
