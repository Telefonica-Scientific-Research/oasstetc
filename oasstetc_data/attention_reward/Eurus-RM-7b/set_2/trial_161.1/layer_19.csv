text;attention
The;0.0038240028585936974
easiest;0.0029653339390769717
way;0.0032189521139450094
to;0.0032132735266677944
import;0.0034572344442894355
the;0.003642592988625952
BERT;0.024862742794249694
language;0.002934782765832905
model;0.003476188760852653
into;0.0028746700805259693
python;0.003251399405950415
for;0.002897085373685047
use;0.0030309825321186068
with;0.002883147834760971
PyTorch;0.011647860644516247
is;0.003430968222658442
using;0.00370219160524135
the;0.0034501853831897453
Hugging;0.013291788363001319
Face;0.006286397497853762
Transformer's;0.01740596426210272
library,;0.005003050737447427
which;0.0031180999838545604
has;0.0032517548518389613
built;0.0027309854390726073
in;0.0031845111351013885
methods;0.0029886925677280493
for;0.003207998350794797
pre-training,;0.006647565288573551
inference,;0.0035735216074460167
and;0.00259290222866083
deploying;0.0031606365518037664
BERT.;0.006696124869146461
â€˜**;0.0040752341469726
from;0.006344489612455641
transformers;0.005655754165242448
import;0.004879310599424174
AutoTokenizer,;0.033037526882853975
BertModel;0.008566089184801302
import;0.003979527712668915
torch;0.005226026949614763
tokenizer;0.005622263982360937
=;0.005722205310449147
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5635333670717495
model;0.004026343769339802
=;0.0037418273369925754
"BertModel.from_pretrained(""bert-base-uncased"")";0.09615346086929025
inputs;0.003990839380722878
=;0.003671422602553684
"tokenizer(""Hello,";0.013081054702882016
my;0.002659420552161461
dog;0.0030475538708234387
is;0.002635563707285623
"cute"",";0.0035823306024483875
"return_tensors=""pt"")";0.013349243275076174
outputs;0.0032178065030744408
=;0.003052836600735941
model(**inputs);0.008660706424783525
last_hidden_states;0.006159552956082147
=;0.0033162950624409
outputs.last_hidden_state;0.004606893761518937
***;0.002501463393987444
