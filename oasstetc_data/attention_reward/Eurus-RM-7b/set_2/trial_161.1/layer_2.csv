text;attention
The;0.014380205067618597
easiest;0.014639742099092055
way;0.014089169362511344
to;0.013915539660910725
import;0.014609600960012953
the;0.01347386289383221
BERT;0.01492518748942353
language;0.014270529622411847
model;0.013494260415148
into;0.01405730661494092
python;0.013904701771521102
for;0.014346087165061368
use;0.0141401571038427
with;0.013680761735814567
PyTorch;0.016068037265500594
is;0.014478224817590283
using;0.014343758720859502
the;0.013428564191038255
Hugging;0.01605141429595597
Face;0.014064835313233461
Transformer's;0.017505361418646656
library,;0.01593737609988137
which;0.0137370899327742
has;0.013432307005175748
built;0.013922402819092036
in;0.014053598434371725
methods;0.014033390823275154
for;0.013970142392823142
pre-training,;0.019319540035156253
inference,;0.015229395771106234
and;0.013020322069878475
deploying;0.013975840111191094
BERT.;0.015693890914761664
â€˜**;0.01787852232445458
from;0.015342497000907644
transformers;0.015166911163675953
import;0.015309505701483362
AutoTokenizer,;0.01945853310245772
BertModel;0.015311359365169001
import;0.015167297862455723
torch;0.014177290235092255
tokenizer;0.015182086935859972
=;0.01457257408247189
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0701225981294703
model;0.013804362196519177
=;0.014404518077392882
"BertModel.from_pretrained(""bert-base-uncased"")";0.03638734078397974
inputs;0.013996603099640677
=;0.014462668448288776
"tokenizer(""Hello,";0.022988268382565947
my;0.013260194432930274
dog;0.013685109826769412
is;0.013110030193467242
"cute"",";0.01425797103584319
"return_tensors=""pt"")";0.01933289161311967
outputs;0.013411486317211492
=;0.013628283880961977
model(**inputs);0.01747599829657117
last_hidden_states;0.017309449819372796
=;0.013045471210641632
outputs.last_hidden_state;0.015134523746892926
***;0.012423048339878855
