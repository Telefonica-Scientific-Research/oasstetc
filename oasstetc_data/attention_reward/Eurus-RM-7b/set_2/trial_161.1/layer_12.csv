text;attention
The;0.006514160586346285
easiest;0.004549159992472251
way;0.003655986111208742
to;0.004268543799511257
import;0.006724266033775312
the;0.005650888994837149
BERT;0.022505872587256687
language;0.004236320857318357
model;0.005836551069772421
into;0.004915482823557495
python;0.004909890421616108
for;0.00429686311356736
use;0.004844174128605647
with;0.004003031871287647
PyTorch;0.02253286004005656
is;0.004401994102950537
using;0.004104474448676854
the;0.003986790052539643
Hugging;0.010720243111606532
Face;0.007703915129546434
Transformer's;0.01362667720798659
library,;0.006580026604836606
which;0.0038665463459570523
has;0.003755662106874569
built;0.003276507174705252
in;0.00407783543655963
methods;0.0041123548262499724
for;0.004240330250763124
pre-training,;0.01114346001393174
inference,;0.006203262637606053
and;0.0036056282689579354
deploying;0.004105141162465696
BERT.;0.009112271920764312
â€˜**;0.004630366391789442
from;0.005974618297253632
transformers;0.005264649801545306
import;0.0057755668210860076
AutoTokenizer,;0.06151745229477261
BertModel;0.01083169072132654
import;0.006050393892920589
torch;0.0055903633476955174
tokenizer;0.01028741593260919
=;0.005297001383481266
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5127197588737343
model;0.005300480246003977
=;0.004288703043962657
"BertModel.from_pretrained(""bert-base-uncased"")";0.04601958778190215
inputs;0.00530003321931777
=;0.004391815339906077
"tokenizer(""Hello,";0.014426543942288267
my;0.003308444973426993
dog;0.0034927813085536908
is;0.0032974469864104844
"cute"",";0.004262867728591485
"return_tensors=""pt"")";0.017247153461656207
outputs;0.003764338320119777
=;0.0035654422852784105
model(**inputs);0.011015993124843474
last_hidden_states;0.007017051125505711
=;0.0035811317101793073
outputs.last_hidden_state;0.0046147199594137826
***;0.0030990144502575276
