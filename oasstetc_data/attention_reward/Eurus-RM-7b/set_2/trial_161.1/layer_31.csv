text;attention
The;0.0005626973761686925
easiest;0.00048517221027144974
way;0.00043723613800359827
to;0.0005065302833427639
import;0.000602771604569491
the;0.0006725255337976738
BERT;0.0016958277459011282
language;0.00046066983618104707
model;0.0004870488683219054
into;0.0005001372695990958
python;0.000524056607065538
for;0.0005578248975064725
use;0.0004323345452297965
with;0.00047176022221409277
PyTorch;0.0014555719673993116
is;0.00047808747255509264
using;0.0006813797469499074
the;0.0005747498665118588
Hugging;0.0014201697381453294
Face;0.0006252067435372101
Transformer's;0.01705014667408909
library,;0.0009491323488904674
which;0.00044729301482197863
has;0.0004629795168486577
built;0.0004061766211572275
in;0.0005006563011284143
methods;0.00044335929263332463
for;0.0004986855521064139
pre-training,;0.0016447974875311272
inference,;0.0006194889415134932
and;0.0003799596370910469
deploying;0.0005609322730697108
BERT.;0.002007692182748201
â€˜**;0.002050352197578422
from;0.0006668307626479781
transformers;0.0008611991453638664
import;0.00115606989474535
AutoTokenizer,;0.0040035685634130245
BertModel;0.002094543979955302
import;0.0007780460512465541
torch;0.000879535527681825
tokenizer;0.0018216265311591362
=;0.0007604515729995296
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.8458015940169854
model;0.0006163966310992979
=;0.00046874894054890873
"BertModel.from_pretrained(""bert-base-uncased"")";0.08365626764990422
inputs;0.0005576156013604835
=;0.0004781383695380118
"tokenizer(""Hello,";0.0018388856750184515
my;0.00036676757097780144
dog;0.0003841014847436569
is;0.00035789223427937065
"cute"",";0.0006324713054147342
"return_tensors=""pt"")";0.0034837310186960515
outputs;0.0004323508058673325
=;0.00037527093142418586
model(**inputs);0.0020220551628030384
last_hidden_states;0.0011473717994300457
=;0.0003718214869104509
outputs.last_hidden_state;0.0019950714510800615
***;0.0003381651202270306
