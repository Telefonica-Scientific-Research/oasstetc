text;attention
The;0.007641405156417801
easiest;0.008329532108293236
way;0.007599366954622021
to;0.007030790986649079
import;0.0087090157036941
the;0.006578636693126982
BERT;0.009379450280151054
language;0.007128014904761767
model;0.006753263549246441
into;0.007371600889742325
python;0.007134398988723209
for;0.0071183488257740265
use;0.006760677312007129
with;0.006661067647485858
PyTorch;0.011227491693975335
is;0.007386447367545848
using;0.007405047133868738
the;0.006462361691725548
Hugging;0.009577822491451922
Face;0.007060353030263419
Transformer's;0.01568008225746842
library,;0.008707342163469061
which;0.006823727295910196
has;0.006616889512590968
built;0.00711441804705433
in;0.006552214039704428
methods;0.007086788849846697
for;0.0067789070226621515
pre-training,;0.014993897015391002
inference,;0.010441337898089908
and;0.006423465289421795
deploying;0.008017953984360358
BERT.;0.011702018152818088
â€˜**;0.012861707093405422
from;0.00740684904665065
transformers;0.008281471890915047
import;0.00788002116539047
AutoTokenizer,;0.013693545982880419
BertModel;0.00832008885307902
import;0.007330934245249853
torch;0.007410417747777338
tokenizer;0.008470697918349384
=;0.0077796936601231045
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.31709939622545136
model;0.006432038355076219
=;0.0075127840122063815
"BertModel.from_pretrained(""bert-base-uncased"")";0.1628152501008088
inputs;0.006610372448500552
=;0.007178096530790795
"tokenizer(""Hello,";0.017613250296782512
my;0.006297476873890699
dog;0.0064215558258716105
is;0.0062181263454045435
"cute"",";0.008677802101295212
"return_tensors=""pt"")";0.02260890301862563
outputs;0.006522761385663841
=;0.0070248881720456095
model(**inputs);0.013255585873462386
last_hidden_states;0.012672747717600633
=;0.006395384586716532
outputs.last_hidden_state;0.013412220847395845
***;0.0055417967402769655
