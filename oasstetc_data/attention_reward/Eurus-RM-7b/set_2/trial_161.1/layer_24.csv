text;attention
The;0.01166693324516293
easiest;0.012843958443407655
way;0.011682930309779786
to;0.011166828844172005
import;0.011544600502968882
the;0.012603832720409979
BERT;0.01758589967768916
language;0.011390685675445218
model;0.011376522510633787
into;0.010701646108377807
python;0.012083394333369685
for;0.011546051821998488
use;0.01123602276018023
with;0.010736151262322307
PyTorch;0.017545861321272452
is;0.011452051532726286
using;0.01199835714162245
the;0.011398630149478773
Hugging;0.014978156944906907
Face;0.013732966758883027
Transformer's;0.021395330596874828
library,;0.01384871265967493
which;0.011718491037046941
has;0.012775141282034837
built;0.011040690283390702
in;0.012222278861443335
methods;0.011851546864760705
for;0.012169789673605361
pre-training,;0.016943138205375786
inference,;0.012556740072764328
and;0.01119264328191031
deploying;0.01164498290992011
BERT.;0.01707072863981871
â€˜**;0.014737540901939317
from;0.016698153738679198
transformers;0.013086746121143753
import;0.01331702759829831
AutoTokenizer,;0.02034136078194273
BertModel;0.01389466111038901
import;0.015335862211851431
torch;0.015367217296621244
tokenizer;0.018066330109667305
=;0.013841101854926886
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.11556218299701627
model;0.011540058654469519
=;0.01176526015955233
"BertModel.from_pretrained(""bert-base-uncased"")";0.0644750404924468
inputs;0.012366438009920148
=;0.01210125238671763
"tokenizer(""Hello,";0.03161559536449313
my;0.010375856901002218
dog;0.010827544851843254
is;0.010334425621759622
"cute"",";0.014751841526970958
"return_tensors=""pt"")";0.022374064794310138
outputs;0.011400097772362186
=;0.011057211713461836
model(**inputs);0.01825702310098606
last_hidden_states;0.014868199237783847
=;0.010689024571765916
outputs.last_hidden_state;0.014809150875570238
***;0.010442032808680113
