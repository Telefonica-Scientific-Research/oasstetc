text;attention
The;0.0030655671227510944
easiest;0.0024404596564322587
way;0.002290651612200986
to;0.002340671146104472
import;0.0031961833192650753
the;0.0023595198935994805
BERT;0.006157500936046122
language;0.0019278209839100717
model;0.0022058275381968318
into;0.002015818125143583
python;0.002326659985998806
for;0.0021085580918742573
use;0.0019808698859769282
with;0.001932897817223485
PyTorch;0.00552712162424101
is;0.0026808363296413726
using;0.002823173445982092
the;0.002412500052069468
Hugging;0.0045627059015153956
Face;0.0030749131377640704
Transformer's;0.008813528028465845
library,;0.003594033114572363
which;0.002221498267935972
has;0.002508523283984296
built;0.001857799669967528
in;0.0022622596777994537
methods;0.0021040548966561137
for;0.0024910345152080334
pre-training,;0.009420086917489266
inference,;0.003228540252298479
and;0.0018251295217994131
deploying;0.0020935372422576115
BERT.;0.005703912275384837
â€˜**;0.002889058244239643
from;0.003040549684720117
transformers;0.0027913814059365976
import;0.0029292893268261853
AutoTokenizer,;0.012381612327586528
BertModel;0.003707272011019237
import;0.0032389261187599095
torch;0.0026687615668252672
tokenizer;0.004265340982876141
=;0.0029850981892561475
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6097942365609011
model;0.0023590579418626737
=;0.0023134862358959014
"BertModel.from_pretrained(""bert-base-uncased"")";0.17741013890744445
inputs;0.003107667520727807
=;0.002499270919099684
"tokenizer(""Hello,";0.00934716753425385
my;0.0017363894733417474
dog;0.0018808708914258218
is;0.0016858932409764707
"cute"",";0.002686630386243697
"return_tensors=""pt"")";0.012362424501184698
outputs;0.0022487099774871335
=;0.0020993367606079233
model(**inputs);0.007434070471584289
last_hidden_states;0.00673375025425796
=;0.002269027684646096
outputs.last_hidden_state;0.004029515770373311
***;0.0015508708398834932
