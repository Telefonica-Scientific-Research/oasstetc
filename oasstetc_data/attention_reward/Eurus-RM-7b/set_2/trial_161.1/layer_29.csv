text;attention
The;0.005543984026975852
easiest;0.005239687459821551
way;0.00560033797086522
to;0.005329008308884265
import;0.005920089313319239
the;0.007551434879668681
BERT;0.012736871899033008
language;0.00527390338208701
model;0.005227448305108078
into;0.004998523092695339
python;0.005258026449077088
for;0.004979051723764947
use;0.0051341627863983
with;0.005075013422149901
PyTorch;0.010264605063959509
is;0.005635337302341971
using;0.007789596430313417
the;0.006234862866052658
Hugging;0.009538398560215168
Face;0.008495765070802129
Transformer's;0.042397953941632115
library,;0.008033846216317926
which;0.005011913220863712
has;0.005430046776445169
built;0.0051472782348148186
in;0.005236545473482362
methods;0.0051492918612743325
for;0.005530997155826801
pre-training,;0.014112848127684856
inference,;0.006882429744472452
and;0.004598461716763859
deploying;0.006432834255356638
BERT.;0.012764355616641338
â€˜**;0.009093814926687148
from;0.006332009498502324
transformers;0.0074406101662090164
import;0.007281226397287228
AutoTokenizer,;0.017092548651350653
BertModel;0.007769760533456054
import;0.00656336105111859
torch;0.006444102866509362
tokenizer;0.009281320388081157
=;0.00592911629550085
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.37980293929103714
model;0.005526037964775986
=;0.005146859774319444
"BertModel.from_pretrained(""bert-base-uncased"")";0.14484555229538457
inputs;0.006003641291108919
=;0.005552093508935371
"tokenizer(""Hello,";0.014048666929478643
my;0.004358015861021318
dog;0.004764154293355915
is;0.004339331310477
"cute"",";0.007748153376859482
"return_tensors=""pt"")";0.018264470810447606
outputs;0.004723450812353591
=;0.004694941360545606
model(**inputs);0.011535189953427005
last_hidden_states;0.009068080555670474
=;0.0045654169903335
outputs.last_hidden_state;0.01906881861419932
***;0.004161403576457165
