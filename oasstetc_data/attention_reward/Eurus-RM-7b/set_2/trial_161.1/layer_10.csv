text;attention
The;0.005064501215095104
easiest;0.003910906103365639
way;0.0034956632892746823
to;0.003098414476174571
import;0.00546612483972308
the;0.004042620967629253
BERT;0.01602326096496913
language;0.003000544159020265
model;0.005287161414918011
into;0.004295845144038059
python;0.0044858364501849
for;0.003476352470848689
use;0.0035285417207731656
with;0.00292473452425857
PyTorch;0.006985298276712685
is;0.006673607775335209
using;0.004564146832854789
the;0.00327284630950118
Hugging;0.0065643838291885885
Face;0.005113220667647662
Transformer's;0.011177238928357465
library,;0.005945411511222606
which;0.003308200153428118
has;0.002996050792580638
built;0.0025068371220252897
in;0.003263385907007735
methods;0.003331106205622641
for;0.0034380045787091034
pre-training,;0.00895378549185434
inference,;0.005177625999116136
and;0.0028294442852620423
deploying;0.0032518838605933124
BERT.;0.010340654004926151
â€˜**;0.005691569348290509
from;0.005851511840338292
transformers;0.005505587452594813
import;0.004702503752638536
AutoTokenizer,;0.029233381500281208
BertModel;0.007441027490766459
import;0.006244046654627379
torch;0.003958864900653501
tokenizer;0.009488479260243623
=;0.006726545290397582
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5681731456258258
model;0.0037659683073142177
=;0.003877183812958536
"BertModel.from_pretrained(""bert-base-uncased"")";0.08862897839989374
inputs;0.0037263166150626184
=;0.0036554838741359864
"tokenizer(""Hello,";0.01934698005752632
my;0.0027821025012212145
dog;0.0029460715103763074
is;0.0026661923055114447
"cute"",";0.003596251445228157
"return_tensors=""pt"")";0.01383653388347879
outputs;0.0030715269087498373
=;0.0028768295816772346
model(**inputs);0.008904762458980467
last_hidden_states;0.0059098254313400675
=;0.0031106137281598357
outputs.last_hidden_state;0.004151325340665699
***;0.0023367504488429453
