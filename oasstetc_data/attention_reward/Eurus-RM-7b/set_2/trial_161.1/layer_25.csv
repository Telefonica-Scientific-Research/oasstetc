text;attention
The;0.011963593480152305
easiest;0.012771071685161478
way;0.0125651913381184
to;0.012480971713580738
import;0.012032872949082417
the;0.012351645892005797
BERT;0.017551626189859885
language;0.0115766147721714
model;0.012188818392812013
into;0.01125159097210264
python;0.011632899638106847
for;0.011589291855635037
use;0.011510851797727246
with;0.0111592532710353
PyTorch;0.014476886356904113
is;0.01234953166215496
using;0.01244505013667348
the;0.011676020083700073
Hugging;0.0166028083882833
Face;0.013132140791261928
Transformer's;0.0291977772612848
library,;0.014632076269995326
which;0.012310868118434256
has;0.012550026147432692
built;0.011725651939094937
in;0.012012613276590714
methods;0.012273924940365393
for;0.012647923279229865
pre-training,;0.019498340909286184
inference,;0.01400335270847062
and;0.011123694322924793
deploying;0.011725034168517076
BERT.;0.018336898608285867
â€˜**;0.016177019030292433
from;0.016806631786363116
transformers;0.014579572385337925
import;0.014547500391898227
AutoTokenizer,;0.02101458740452736
BertModel;0.01476424322383673
import;0.015421020878301253
torch;0.014741551388887357
tokenizer;0.01652526353813963
=;0.013195194352270614
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.11785420535540223
model;0.01213228085743628
=;0.011211986613612269
"BertModel.from_pretrained(""bert-base-uncased"")";0.054406101438079925
inputs;0.011963998519708767
=;0.01167920498846799
"tokenizer(""Hello,";0.019491777519743445
my;0.010964913902018494
dog;0.011059776116037132
is;0.010947271416543504
"cute"",";0.013728182442932037
"return_tensors=""pt"")";0.019032455013229668
outputs;0.01150164304387712
=;0.011069155927603443
model(**inputs);0.017591004744414614
last_hidden_states;0.01492219685988748
=;0.01085075672871014
outputs.last_hidden_state;0.015039687530163619
***;0.011433903285835422
