text;attention
The;0.0065429388730468725
easiest;0.0062917097718803786
way;0.005720010426248194
to;0.0051554572609668016
import;0.010002794932087023
the;0.006411947929787558
BERT;0.015494682480113237
language;0.00571931444123828
model;0.0075362375089248
into;0.006733003165506271
python;0.007890234392782863
for;0.005569949752029565
use;0.005804337977843917
with;0.0053649443802301095
PyTorch;0.010355982834917931
is;0.006945406978115768
using;0.006004265616728273
the;0.005687452374569021
Hugging;0.011453340251751983
Face;0.009954995440241797
Transformer's;0.020413462149484612
library,;0.009971903150978657
which;0.005559987537580964
has;0.005220910487474356
built;0.004708877999305061
in;0.005950818130203477
methods;0.005797826583492258
for;0.005560254487560911
pre-training,;0.01460332585724343
inference,;0.008433571546306844
and;0.004916803037892818
deploying;0.006964593494701668
BERT.;0.01003135712345926
â€˜**;0.009080712901805984
from;0.00894001555481474
transformers;0.01008927499305838
import;0.008436495388775066
AutoTokenizer,;0.02755398754018463
BertModel;0.010825400485242758
import;0.007921393517470735
torch;0.007795699016667046
tokenizer;0.010158290797617296
=;0.007645510312873919
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.38659478286138477
model;0.006338854482562466
=;0.006232994454748496
"BertModel.from_pretrained(""bert-base-uncased"")";0.09623671152331227
inputs;0.0072925692570362235
=;0.006216097842900602
"tokenizer(""Hello,";0.025713289174219285
my;0.005386170353735213
dog;0.005576102522915171
is;0.005032429484949416
"cute"",";0.006820680369604947
"return_tensors=""pt"")";0.024348015146600406
outputs;0.005682984095458352
=;0.005164489248508058
model(**inputs);0.01208094643284925
last_hidden_states;0.0106551826294478
=;0.005197873227743135
outputs.last_hidden_state;0.007734923470362602
***;0.004475424538455997
