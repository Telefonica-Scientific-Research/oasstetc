text;attention
The;0.0030367039662337275
easiest;0.002554907307872842
way;0.00250531762545658
to;0.002622023424522441
import;0.0029695212009681277
the;0.0028588692845458036
BERT;0.01599736305646945
language;0.002864916674124936
model;0.0029265146139236128
into;0.002314482576138166
python;0.002800572459590313
for;0.0024696094299613866
use;0.0025696326083037174
with;0.0023505046728801516
PyTorch;0.00881495116221313
is;0.002605231566334559
using;0.002785163563451399
the;0.0026501581976666935
Hugging;0.00771965607440539
Face;0.004963060983376348
Transformer's;0.019865653982787414
library,;0.0044098643488531325
which;0.002334290158575893
has;0.0023808153231794082
built;0.002155382461771606
in;0.0025201415526161846
methods;0.002516755569124873
for;0.002446453461539848
pre-training,;0.008094445099641053
inference,;0.0034534822272513786
and;0.0020650295460515056
deploying;0.0024581839503627244
BERT.;0.005154642834477023
â€˜**;0.0032981409598734387
from;0.003668306234888196
transformers;0.006201843041236032
import;0.003748876631609404
AutoTokenizer,;0.03514065395878393
BertModel;0.007721310409495824
import;0.0036429541039552323
torch;0.004403855395148181
tokenizer;0.004776706046634794
=;0.003284104406903186
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6612363766191328
model;0.002608655184122584
=;0.002285128685328245
"BertModel.from_pretrained(""bert-base-uncased"")";0.057144923358960924
inputs;0.002940070205567097
=;0.002437669397293683
"tokenizer(""Hello,";0.008178962736079544
my;0.002143350832592478
dog;0.0024650175254528374
is;0.0020868750210686483
"cute"",";0.0026131506145359233
"return_tensors=""pt"")";0.0134467587009255
outputs;0.002474070012883944
=;0.0021302041368051
model(**inputs);0.006003221383289305
last_hidden_states;0.006293254107650844
=;0.0020980148100527957
outputs.last_hidden_state;0.0033339611410763454
***;0.0019592833739824566
