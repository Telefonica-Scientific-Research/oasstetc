text;attention
The;0.009147163061605335
easiest;0.009572613892959396
way;0.008957197020428834
to;0.008802157004233863
import;0.009566299456388207
the;0.010144984701207907
BERT;0.025006297226924035
language;0.009063662437099962
model;0.009463221157246503
into;0.008492414017408934
python;0.009228716008688444
for;0.00852037783205645
use;0.00847919094090011
with;0.008251467619302929
PyTorch;0.013612698677338657
is;0.00806100530150431
using;0.009090618098281792
the;0.008436559769450269
Hugging;0.013222144323045509
Face;0.011354439234725539
Transformer's;0.0215381496671966
library,;0.011918119809376798
which;0.00792581198005591
has;0.008819257359851762
built;0.007824607994513063
in;0.008957172194552821
methods;0.008589641314534342
for;0.008979013755266559
pre-training,;0.018002312511791897
inference,;0.010772304549954214
and;0.007522098248138458
deploying;0.008806759382637894
BERT.;0.013391580261453187
â€˜**;0.010145459846094882
from;0.011745932657185217
transformers;0.01131682413171956
import;0.009897875710832189
AutoTokenizer,;0.028970321530114012
BertModel;0.013331691107087438
import;0.010276191860798568
torch;0.010295678291842084
tokenizer;0.014833852039208255
=;0.00999468050439383
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.26911198319900353
model;0.008775827842449657
=;0.008459461162428818
"BertModel.from_pretrained(""bert-base-uncased"")";0.07197083031137219
inputs;0.009587721431857288
=;0.008986402428374065
"tokenizer(""Hello,";0.02133210893363157
my;0.007472654398858373
dog;0.007906540003611125
is;0.007662860042186582
"cute"",";0.009738883301982054
"return_tensors=""pt"")";0.01868041378764771
outputs;0.007961380207995685
=;0.007845799934293114
model(**inputs);0.01449423719419873
last_hidden_states;0.013467030834726114
=;0.007569591223211466
outputs.last_hidden_state;0.011487758673339942
***;0.007159950599435303
