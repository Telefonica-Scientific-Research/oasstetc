text;attention
The;0.012611905549643244
easiest;0.0133666300964446
way;0.01195142350271193
to;0.011806707468413495
import;0.01462101130123731
the;0.011895895029412759
BERT;0.013881105319554414
language;0.012795699053995854
model;0.012019223879460033
into;0.01243386744655976
python;0.012358872155220673
for;0.012090501792119466
use;0.012483531110445012
with;0.012528651028928077
PyTorch;0.015052521314839648
is;0.012653819803926939
using;0.012411145598585801
the;0.011748039534820813
Hugging;0.017246899360418975
Face;0.015385930441770649
Transformer's;0.017340400194803453
library,;0.014093107679023501
which;0.011775133078564044
has;0.011858500368507562
built;0.012661682605143422
in;0.01317902983610083
methods;0.013296618279234841
for;0.012240620485830972
pre-training,;0.01942949673683362
inference,;0.014776881378645971
and;0.01163270119509521
deploying;0.013505573536676864
BERT.;0.013782471318746601
â€˜**;0.015701954260898727
from;0.013704625326092875
transformers;0.01555795046455203
import;0.013390267544158903
AutoTokenizer,;0.0201277683696242
BertModel;0.014455937181326017
import;0.012673557826232347
torch;0.01406004144753329
tokenizer;0.01464842410345923
=;0.012698359277675257
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09764768121732748
model;0.012381944428883842
=;0.01235574714477567
"BertModel.from_pretrained(""bert-base-uncased"")";0.055602389158435156
inputs;0.01316456632643569
=;0.012335522115073817
"tokenizer(""Hello,";0.027070686477666774
my;0.011709428992500593
dog;0.01226268076172524
is;0.01133742323024105
"cute"",";0.013563142638019603
"return_tensors=""pt"")";0.027645523223436882
outputs;0.01299084679970599
=;0.011960048221520884
model(**inputs);0.01989587472670422
last_hidden_states;0.019318162469593864
=;0.011697056498248523
outputs.last_hidden_state;0.018205691015920344
***;0.010921101270515246
