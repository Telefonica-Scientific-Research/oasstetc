text;attention
The;0.0017595263461964786
easiest;0.0014946975889446052
way;0.0011441598305868832
to;0.0012030221444659188
import;0.0017411111090952878
the;0.0015280849632825044
BERT;0.00871199140242834
language;0.0011327910940960788
model;0.0017171113028687447
into;0.0012725739108721571
python;0.0017432394491058993
for;0.001133470984207967
use;0.001299259539717664
with;0.0009957770071938557
PyTorch;0.00543270669245132
is;0.0013362826411218076
using;0.0012106744686734
the;0.001143782250684472
Hugging;0.004728035467586431
Face;0.0026101960671287743
Transformer's;0.010035481937257801
library,;0.0026159772713181643
which;0.0011524359612763962
has;0.0011828161443347698
built;0.0008953700545167569
in;0.0011978582938720228
methods;0.0011276346447890986
for;0.001095877580918415
pre-training,;0.005412719828107223
inference,;0.0018972992807042852
and;0.0009650422781696258
deploying;0.00108992773835568
BERT.;0.003112943241106316
â€˜**;0.0015681612214330522
from;0.002091051841466096
transformers;0.0024589008716662973
import;0.0024331651694695796
AutoTokenizer,;0.011382219143896971
BertModel;0.002478584310587027
import;0.0019239594546304217
torch;0.001399973779148494
tokenizer;0.0029955480676389134
=;0.0023032315627188387
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7734204328483291
model;0.0015116063760025336
=;0.0012363784792998411
"BertModel.from_pretrained(""bert-base-uncased"")";0.07118108153597248
inputs;0.00190782808510216
=;0.0015651333935066195
"tokenizer(""Hello,";0.013479431713937111
my;0.0009565684475772016
dog;0.001037780771373797
is;0.0008896885649843893
"cute"",";0.0016263397814953054
"return_tensors=""pt"")";0.010303761480869665
outputs;0.0013099138438188759
=;0.0011154662954304076
model(**inputs);0.006385732297244599
last_hidden_states;0.0023905051515228557
=;0.0010197768827385338
outputs.last_hidden_state;0.0016891261738694926
***;0.0008187739388343654
