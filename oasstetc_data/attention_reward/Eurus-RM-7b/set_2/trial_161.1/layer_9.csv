text;attention
The;0.007211843814070401
easiest;0.006467199227464935
way;0.006015502706133915
to;0.005682969787978191
import;0.008432634482958237
the;0.007485829209465743
BERT;0.039866762324733976
language;0.007366972353559292
model;0.012557608479842684
into;0.007304044358502448
python;0.007089023299382494
for;0.006435938979669699
use;0.006731589574678285
with;0.006138077750919535
PyTorch;0.015284284963446864
is;0.007955522319756301
using;0.006495898314734548
the;0.005879377584272004
Hugging;0.016620561686982106
Face;0.012591029621050279
Transformer's;0.034418105665949536
library,;0.013831441557250222
which;0.006072390104708456
has;0.005920939991450115
built;0.0052381289275272615
in;0.006362513831903076
methods;0.006056534858973085
for;0.006082670639419885
pre-training,;0.016738066921320984
inference,;0.01018202118070825
and;0.00559493053772568
deploying;0.007178387628125497
BERT.;0.01829988461515082
â€˜**;0.007817539532040733
from;0.007793937114809588
transformers;0.01126102176043041
import;0.009271247953230465
AutoTokenizer,;0.08296735915787282
BertModel;0.020708164030791096
import;0.010068323936146253
torch;0.00798703853019323
tokenizer;0.018390906028913124
=;0.008363517948818048
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2842751191594109
model;0.007504096401405409
=;0.006389597903194175
"BertModel.from_pretrained(""bert-base-uncased"")";0.04262238787235033
inputs;0.008560648265958553
=;0.006204511655858175
"tokenizer(""Hello,";0.017103892347218163
my;0.0054682469075137225
dog;0.005943940469603387
is;0.005375578053791073
"cute"",";0.006249998652576024
"return_tensors=""pt"")";0.025296797573324876
outputs;0.006187034970167575
=;0.005637631612241582
model(**inputs);0.012323695914390044
last_hidden_states;0.01051463616458571
=;0.006140955623697957
outputs.last_hidden_state;0.006890855226448119
***;0.005092631933203724
