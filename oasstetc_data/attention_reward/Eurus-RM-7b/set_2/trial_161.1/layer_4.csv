text;attention
The;0.012639699307769705
easiest;0.013055424909700701
way;0.011728645650030617
to;0.011526828869834868
import;0.013533103076832684
the;0.011058566866547371
BERT;0.01376185005765682
language;0.011913901490424275
model;0.011622180510760008
into;0.01234665187797838
python;0.011283132303701732
for;0.012335330009437432
use;0.011626693695559534
with;0.011928009168372904
PyTorch;0.012507250288738711
is;0.012350333303571187
using;0.01238106840647446
the;0.010941731812734917
Hugging;0.016578510384892196
Face;0.01839342310598917
Transformer's;0.018034037801813733
library,;0.014503486434661464
which;0.011542790694780992
has;0.011572274683151732
built;0.011541421992098533
in;0.012180777320968561
methods;0.011672047970524582
for;0.011968093375974362
pre-training,;0.017065599518713517
inference,;0.012633869972104635
and;0.010446574156787099
deploying;0.011437080275640677
BERT.;0.012969203611012222
â€˜**;0.01690693672530937
from;0.014384156229191246
transformers;0.014106705332044127
import;0.01263816211959203
AutoTokenizer,;0.020396996999126837
BertModel;0.016059125324828876
import;0.013552568681828645
torch;0.012596495501594378
tokenizer;0.014957077642033913
=;0.014445653037378771
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.13674387005639857
model;0.013783037113519993
=;0.013836394362400105
"BertModel.from_pretrained(""bert-base-uncased"")";0.05523210431035125
inputs;0.012745666135467357
=;0.013011501572585885
"tokenizer(""Hello,";0.025041983111545894
my;0.010766877886286637
dog;0.011077989041200744
is;0.010455526065901445
"cute"",";0.011590457956679737
"return_tensors=""pt"")";0.019251298155842325
outputs;0.011516567844130571
=;0.011153948987388139
model(**inputs);0.01821251679967506
last_hidden_states;0.018669825106387452
=;0.011845300294340153
outputs.last_hidden_state;0.013883690006913451
***;0.010057974694817338
