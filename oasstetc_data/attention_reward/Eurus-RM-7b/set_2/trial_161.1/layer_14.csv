text;attention
The;0.0014686099435480455
easiest;0.0010604738450852738
way;0.0009291966275290948
to;0.0010230553618022981
import;0.0012001645240083151
the;0.0010616448404558447
BERT;0.004924493783827613
language;0.001027443117250686
model;0.0011616353228405714
into;0.0009528461421185561
python;0.0010247208288237768
for;0.0009559462801023777
use;0.0009351075604631305
with;0.0008980560162959232
PyTorch;0.0032329612287020203
is;0.0010283734121235458
using;0.0010808051032183525
the;0.0010165263630848484
Hugging;0.002892469783861345
Face;0.0016768099683862568
Transformer's;0.006925384954765908
library,;0.0017724773485501655
which;0.000913231204151842
has;0.0009769728910953498
built;0.0008065818415096836
in;0.0010112634897776166
methods;0.000938195335656187
for;0.0009052795727889581
pre-training,;0.0031236410037664697
inference,;0.0016741494912886773
and;0.0008665543299379128
deploying;0.001014231759180344
BERT.;0.0018416255152745422
â€˜**;0.0011726237496535075
from;0.0013257720254070448
transformers;0.0017856579877604068
import;0.0016336413474848262
AutoTokenizer,;0.012463355366383294
BertModel;0.0023714492409569465
import;0.0015939975873587521
torch;0.001143441634247137
tokenizer;0.002229688442881202
=;0.0019183986836589593
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.8691987312423527
model;0.0012057262274930087
=;0.0009558712704045842
"BertModel.from_pretrained(""bert-base-uncased"")";0.021849685347003638
inputs;0.0012704123954888125
=;0.0011008859918146864
"tokenizer(""Hello,";0.005128251701278655
my;0.0008325571500392322
dog;0.0008804024929086875
is;0.0007894095800900378
"cute"",";0.001195603505801774
"return_tensors=""pt"")";0.005972312046900409
outputs;0.000982437127766207
=;0.0008856921925877743
model(**inputs);0.0030323254532087464
last_hidden_states;0.00182606045602293
=;0.0008766928332465247
outputs.last_hidden_state;0.001301235007333148
***;0.000756753121194921
