text;attention
The;0.008948195924565293
easiest;0.009050318211642734
way;0.008740572286012297
to;0.007849290441500975
import;0.0114115698961904
the;0.008399081685759332
BERT;0.01770284809255496
language;0.007598858623315068
model;0.010994936655733046
into;0.009371749103426426
python;0.011055080416555512
for;0.008942392781759883
use;0.009365010066142548
with;0.008648697241887842
PyTorch;0.012925238866180156
is;0.009672329697022683
using;0.009046219658068963
the;0.008091426956525583
Hugging;0.013969917511268337
Face;0.01351153614948169
Transformer's;0.032965916907510086
library,;0.016230153504372374
which;0.00888947939500536
has;0.008376821690548758
built;0.007364442759836573
in;0.008649193556185795
methods;0.009131193859045358
for;0.008447146457687264
pre-training,;0.014550722070542524
inference,;0.010575009239559357
and;0.0075267061204852504
deploying;0.009399933026102873
BERT.;0.012691995320137182
â€˜**;0.011656175432378204
from;0.013622934702322448
transformers;0.011995238011249193
import;0.012359461861663007
AutoTokenizer,;0.030091748460834136
BertModel;0.016443561984988135
import;0.010483246861184684
torch;0.00952273454293783
tokenizer;0.013836297285276675
=;0.012779011387469645
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.21464432952231957
model;0.010062602439523768
=;0.009367878806364971
"BertModel.from_pretrained(""bert-base-uncased"")";0.07354959256384834
inputs;0.011662317705829156
=;0.008915544239097023
"tokenizer(""Hello,";0.029881806596943575
my;0.007461256150851239
dog;0.00819619032680508
is;0.007458562045420735
"cute"",";0.010911180897574934
"return_tensors=""pt"")";0.029996667823563777
outputs;0.009254491611299001
=;0.008185940277092357
model(**inputs);0.015821278389251674
last_hidden_states;0.017008464548252514
=;0.007979231604503959
outputs.last_hidden_state;0.009628325561960787
***;0.007129944186580998
