text;attention
The;0.004484571170922303
easiest;0.0034955315336207835
way;0.002791522399219
to;0.0031517715675152733
import;0.004865793344819313
the;0.015085286813657882
BERT;0.0048312082341760145
language;0.0029691967076091834
model;0.003170086090616854
into;0.0028744829337528474
python;0.010807988560857752
for;0.0030873820211149766
use;0.0028372686960956017
with;0.0032301319987148767
PyTorch;0.008624235681526263
is;0.0027066409313705733
using;0.003021087448081692
the;0.01134830470679741
Hugging;0.004536237283897593
Face;0.0028198610021297533
Transformer's;0.010603959759058442
library,;0.005362730793205821
which;0.0026835109785410685
has;0.0026311674686749966
built;0.0027778170223631974
in;0.00261821996008079
methods;0.0027619415215145047
for;0.002786161965129
pre-training,;0.012655807463344534
inference,;0.006908844415243023
and;0.0027294308880264307
deploying;0.004398430067776966
BERT.;0.006366418687606879
â€˜**;0.004245731901532881
from;0.002563587620002012
transformers;0.0036512467777708875
import;0.0034123208285183424
AutoTokenizer,;0.008826048578702079
BertModel;0.003914075125012294
import;0.003293182028560655
torch;0.003092585583597367
tokenizer;0.0034112155819724706
=;0.0027934243591192713
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5655890631072751
model;0.002310734554251672
=;0.0026044978873216115
"BertModel.from_pretrained(""bert-base-uncased"")";0.17291576432259537
inputs;0.002247813424760386
=;0.0024721595281266273
"tokenizer(""Hello,";0.007753951044335542
my;0.0021853882814313473
dog;0.0021160734913727853
is;0.002128870965462809
"cute"",";0.0032155093619816695
"return_tensors=""pt"")";0.008141532980392722
outputs;0.0020929459904576245
=;0.0022786422686564625
model(**inputs);0.00510564931450535
last_hidden_states;0.0038023608221198313
=;0.0021167871482341656
outputs.last_hidden_state;0.003914824955245034
***;0.001780982049623947
