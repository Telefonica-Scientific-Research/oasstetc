text;attention
The;0.004182718446512547
easiest;0.003455461531861731
way;0.0038390840471841246
to;0.0036521708473592814
import;0.003922356001944868
the;0.004510280109372347
BERT;0.00775127299015243
language;0.0034106496749361347
model;0.0033938627302442692
into;0.0036136895371183006
python;0.0036003877337998113
for;0.0037814146766707297
use;0.003193022298253082
with;0.0034578708858934177
PyTorch;0.005650353636326677
is;0.0037105278169511638
using;0.004790752253023501
the;0.004513786255662682
Hugging;0.006035165575776337
Face;0.005020773286265625
Transformer's;0.01895806096979137
library,;0.005133557620285016
which;0.003588139085555185
has;0.0038046446617191173
built;0.003269934499901288
in;0.003787884499558974
methods;0.003431871570122817
for;0.0037778234430059783
pre-training,;0.00916827267873684
inference,;0.0048303791882800965
and;0.0032896456724294974
deploying;0.003608267920424226
BERT.;0.008798378489529609
â€˜**;0.011069366528855002
from;0.005632365723370013
transformers;0.004942532889244636
import;0.005773618681357297
AutoTokenizer,;0.01317339477540921
BertModel;0.0058742958841933035
import;0.004329792493158262
torch;0.005482983943250777
tokenizer;0.005423849340235231
=;0.004908720089916646
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6096503990941436
model;0.003111231775479816
=;0.0035602538445273857
"BertModel.from_pretrained(""bert-base-uncased"")";0.07372710913176152
inputs;0.0038160377151398865
=;0.0037871598294998265
"tokenizer(""Hello,";0.013372616665743154
my;0.0030983086344472826
dog;0.003081546122825388
is;0.0029802440378881665
"cute"",";0.0045651345984033176
"return_tensors=""pt"")";0.012466913022113123
outputs;0.0032048752793872354
=;0.0032169800116906796
model(**inputs);0.00737715159071415
last_hidden_states;0.007249550871762309
=;0.003109661416028161
outputs.last_hidden_state;0.008247316473192313
***;0.00283412890161311
