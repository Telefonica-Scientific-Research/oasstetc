text;attention
The;1.477353064755427e-17
easiest;1.5356909848704084e-17
way;1.2913110157354196e-17
to;1.5097676528307952e-17
import;1.6925122046841252e-17
the;1.7531134358837618e-17
BERT;2.040334434593567e-17
language;2.3552454594837373e-17
model;2.328059108949586e-17
into;1.5257740497720584e-17
python;2.1638617608105722e-17
for;1.3851979778747474e-17
use;1.3570688618313172e-17
with;1.4637513207222205e-17
PyTorch;3.0611883674978366e-17
is;1.2829280747283121e-17
using;1.333053209913946e-17
the;1.5706363073153786e-17
Hugging;2.6500727451553855e-17
Face;1.929764334357729e-17
Transformer's;3.963526471399819e-17
library,;2.003750159627061e-17
which;1.4263855741211166e-17
has;1.3950437467611046e-17
built;1.3276883284356428e-17
in;1.2995295468822046e-17
methods;1.383543347606539e-17
for;1.5559372179997304e-17
pre-training,;3.2249331260798875e-17
inference,;1.6644273161273543e-17
and;1.5962072546653904e-17
deploying;1.5472778706734348e-17
BERT.;0.9999999999999976
â€˜**;2.5977405172593374e-17
from;1.3901187326560136e-17
transformers;2.9439317874693896e-17
import;1.636891728038236e-17
AutoTokenizer,;4.584137546536607e-17
BertModel;3.299377761739959e-17
import;1.6857545087558947e-17
torch;3.329288749748426e-17
tokenizer;3.093505298960934e-17
=;1.2392881060929924e-17
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";7.49291169031344e-16
model;1.7326800090522522e-17
=;1.2776826712539372e-17
"BertModel.from_pretrained(""bert-base-uncased"")";5.705079207289987e-16
inputs;2.1685325089844575e-17
=;1.2940198794711609e-17
"tokenizer(""Hello,";4.58180149693464e-17
my;1.901404248211937e-17
dog;2.2435760413404545e-17
is;1.5737999128426373e-17
"cute"",";2.0016149137583694e-17
"return_tensors=""pt"")";1.5810947864060322e-16
outputs;1.552679439401933e-17
=;1.286389761551603e-17
model(**inputs);5.448978639274879e-17
last_hidden_states;4.9716255992853494e-17
=;1.238468597573385e-17
outputs.last_hidden_state;7.635587772004936e-17
***;1.7588993223391424e-17
