text;attention
The;0.00023553847472612962
easiest;0.0001995809014296092
way;0.0002002538631093707
to;0.00020019982392063208
import;0.0002952678200856363
the;0.00022211830858240276
BERT;0.0002990270965069892
language;0.0002625178043548322
model;0.00031239754260028925
into;0.0002423382215542458
python;0.00030378165947411685
for;0.0002502244528448161
use;0.00023692054417017198
with;0.0003037177492012589
PyTorch;0.0004950902236692612
is;0.00043844046226828475
using;0.0003073702911042697
the;0.00028804005649230843
Hugging;0.0003324912265494132
Face;0.00020888677801622561
Transformer's;0.0008857964557196286
library,;0.0006022006445280674
which;0.0002326345067061419
has;0.0003146621807882254
built;0.0002506184170156493
in;0.00019547204092546915
methods;0.00031922246502800555
for;0.0002686452523123809
pre-training,;0.001137704311639424
inference,;0.0004118223724754805
and;0.0002782101795809908
deploying;0.0003081666407143224
BERT.;0.00121955256101878
â€˜**;0.000669242608529911
from;0.0003104612429086781
transformers;0.0003726225551935797
import;0.0005975556319447364
AutoTokenizer,;0.00115999917701279
BertModel;0.0003547067202671301
import;0.0005340025436591795
torch;0.00021235257097118257
tokenizer;0.00031238749708591775
=;0.0006225597746779597
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.45142704169302283
model;0.0003356937650466117
=;0.0005190051851065342
"BertModel.from_pretrained(""bert-base-uncased"")";0.5036360964088092
inputs;0.000320432080243053
=;0.00046215908343838133
"tokenizer(""Hello,";0.002176530205473036
my;0.0002159743690405365
dog;0.0002105663851897596
is;0.00024137684537527354
"cute"",";0.000430508164797212
"return_tensors=""pt"")";0.013128475435077116
outputs;0.00028779480556640695
=;0.0003884508445423888
model(**inputs);0.003006525913127921
last_hidden_states;0.0014346946291166702
=;0.00045455094154502084
outputs.last_hidden_state;0.003877220103059939
***;0.00024210149105809492
