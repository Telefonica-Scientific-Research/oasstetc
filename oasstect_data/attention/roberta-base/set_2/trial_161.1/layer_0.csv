text;attention
The;2.0889249634371233e-07
easiest;2.945720994192897e-07
way;1.9027631870102603e-07
to;1.988158237376894e-07
import;2.2897328967814708e-07
the;2.0349611856616377e-07
BERT;4.355494012172035e-07
language;2.3916446142024445e-07
model;2.0381931793430663e-07
into;1.9836875141402575e-07
python;2.8625534807079013e-07
for;2.0024198438614186e-07
use;2.2782291671309798e-07
with;2.0342508957387235e-07
PyTorch;9.213019449072818e-07
is;1.997677704110191e-07
using;2.2439159275202475e-07
the;1.8143845650519335e-07
Hugging;4.497725378643568e-07
Face;1.9351433228437978e-07
Transformer's;1.7927729602867837e-06
library,;5.03075522610432e-07
which;2.588366303223915e-07
has;2.06241949013712e-07
built;2.308579682542289e-07
in;1.9363726984988798e-07
methods;2.2806576253096425e-07
for;1.8196858126090295e-07
pre-training,;4.6282746145477625e-06
inference,;6.051588823110821e-07
and;2.0092361214084805e-07
deploying;2.489733420230357e-07
BERT.;1.2099853167508487e-06
â€˜**;1.9031556991236536e-06
from;1.8247976218590953e-07
transformers;4.2187173515286255e-07
import;2.132570675429882e-07
AutoTokenizer,;3.1982842238634637e-06
BertModel;4.3870556519369234e-07
import;2.1444245785502052e-07
torch;2.3845827149701115e-07
tokenizer;4.480793426422138e-07
=;2.790862606068147e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7649878660350059
model;1.836230923249236e-07
=;2.908902766716684e-07
"BertModel.from_pretrained(""bert-base-uncased"")";0.23475359778973723
inputs;2.1870449130035773e-07
=;2.433166859622641e-07
"tokenizer(""Hello,";1.3446881427281853e-05
my;1.7349839116544134e-07
dog;2.1019724405282418e-07
is;1.8656322344162102e-07
"cute"",";7.258711980404953e-07
"return_tensors=""pt"")";0.00012889570959888003
outputs;2.332200210244163e-07
=;2.556182506468932e-07
model(**inputs);3.319281259012682e-05
last_hidden_states;7.99497068805016e-06
=;2.5925317809288323e-07
outputs.last_hidden_state;4.829511341479702e-05
***;2.1347863555377336e-07
