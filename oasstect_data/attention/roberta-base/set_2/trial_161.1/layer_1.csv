text;attention
The;4.310488024157989e-05
easiest;4.305538636380696e-05
way;3.8886101225138176e-05
to;5.0726811315588044e-05
import;5.79100456248043e-05
the;5.029571097215832e-05
BERT;5.751523134000477e-05
language;4.182813481233039e-05
model;3.991635124171243e-05
into;4.0780067130370265e-05
python;6.164965539389899e-05
for;4.160591181214675e-05
use;4.072120209232834e-05
with;4.696205018765153e-05
PyTorch;0.0001071072956561637
is;5.717083680926233e-05
using;4.5894443943586345e-05
the;5.477211145930384e-05
Hugging;7.029682290539719e-05
Face;3.720782924325259e-05
Transformer's;0.00018873311075785936
library,;9.36355430294695e-05
which;4.254796541717225e-05
has;4.118604169917383e-05
built;4.310838998142766e-05
in;4.0462644189065385e-05
methods;4.858889786110286e-05
for;4.913181567472363e-05
pre-training,;0.0002968763978579337
inference,;8.27101889103716e-05
and;4.55726001290335e-05
deploying;4.131334432554187e-05
BERT.;0.00028725849768243615
â€˜**;0.00010723221895222215
from;4.458507461750009e-05
transformers;7.884425309244852e-05
import;5.128596864583116e-05
AutoTokenizer,;0.0002051766722157683
BertModel;5.4969382730233e-05
import;4.9268616643664804e-05
torch;3.18989369174927e-05
tokenizer;5.303268034092218e-05
=;6.141724948625469e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6846782900343016
model;3.768203295980691e-05
=;5.556822544785055e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.3058800087796219
inputs;3.863403208282626e-05
=;4.8440042566877815e-05
"tokenizer(""Hello,";0.0007774577052164844
my;3.582076020457786e-05
dog;4.6104505232720804e-05
is;4.2931873288385026e-05
"cute"",";0.00011168095579043555
"return_tensors=""pt"")";0.002622524807754909
outputs;3.86538340502864e-05
=;4.813947768641444e-05
model(**inputs);0.0006330624743755104
last_hidden_states;0.0002665416401514324
=;5.239842221816106e-05
outputs.last_hidden_state;0.00147426070753389
***;4.555631858798713e-05
