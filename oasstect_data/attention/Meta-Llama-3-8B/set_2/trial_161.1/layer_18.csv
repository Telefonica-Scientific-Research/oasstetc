text;attention
The;0.013324205405661305
easiest;0.011262187053390391
way;0.012000851438245376
to;0.01085480268130748
import;0.014668812990091016
the;0.011773204056761372
BERT;0.04846161694860427
language;0.012311548889595996
model;0.019145490574710627
into;0.013333377024129405
python;0.015707845444734494
for;0.012755673011846807
use;0.011159038541454887
with;0.011477477143742181
PyTorch;0.025281333367088155
is;0.011949532729872802
using;0.012204409440127389
the;0.012364569347779564
Hugging;0.013336424471101919
Face;0.015205459587362628
Transformer's;0.021867718853872634
library,;0.013821071191424254
which;0.011391299040572933
has;0.011228240905030712
built;0.010374005518428589
in;0.011620904705513378
methods;0.011007254069554938
for;0.011385912992449468
pre-training,;0.015101125522338985
inference,;0.012378233607199784
and;0.010246595799895732
deploying;0.010268912898568251
BERT.;0.017835472548146335
â€˜**;0.01393676778142031
from;0.012502382315515141
transformers;0.01571923824760738
import;0.012336185831305718
AutoTokenizer,;0.031184221388840815
BertModel;0.028029930529949877
import;0.011735697804961349
torch;0.01415490366929677
tokenizer;0.014888327842509094
=;0.011973370653596861
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09752919196505817
model;0.012546743599956863
=;0.01115235116205368
"BertModel.from_pretrained(""bert-base-uncased"")";0.03329034628725533
inputs;0.014632353811334967
=;0.010760410020808889
"tokenizer(""Hello,";0.024031197558300996
my;0.01075090790661223
dog;0.011826483510118122
is;0.009993043426254464
"cute"",";0.011416985336329157
"return_tensors=""pt"")";0.02255942972800199
outputs;0.013430787678606823
=;0.010436424421126514
model(**inputs);0.014726098101748766
last_hidden_states;0.014784465541582168
=;0.010402267096935338
outputs.last_hidden_state;0.012107937163918791
***;0.010056941818389321
