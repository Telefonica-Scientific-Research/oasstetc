text;attention
The;0.0150414799356435
easiest;0.013096020143914425
way;0.01343282716010252
to;0.012849318931796229
import;0.016195972502673107
the;0.01323598348477172
BERT;0.026192969248707204
language;0.012999768614151343
model;0.014465193414611853
into;0.013831727400338768
python;0.014803396577759576
for;0.01328155458665793
use;0.012534188401105944
with;0.013199835625451274
PyTorch;0.020954690998631727
is;0.013661663618634537
using;0.013637047995753469
the;0.013334485877067181
Hugging;0.01633078460121645
Face;0.014969639097758147
Transformer's;0.01665600170681529
library,;0.01449867602259001
which;0.01277137840121417
has;0.012518841529166096
built;0.01248102205847588
in;0.013266995956808154
methods;0.013145638520731664
for;0.012619522667807331
pre-training,;0.015141196812488055
inference,;0.013373820923578698
and;0.012238740092470467
deploying;0.013003113282231131
BERT.;0.01913621106659793
â€˜**;0.020660778479056535
from;0.01442182452241897
transformers;0.014426900969485897
import;0.014858926904777628
AutoTokenizer,;0.020369597124617975
BertModel;0.020421967975785597
import;0.014062502264844505
torch;0.01516854349838217
tokenizer;0.013815169057517411
=;0.013471868266600762
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07079176543155967
model;0.013283756920320298
=;0.013352553118638314
"BertModel.from_pretrained(""bert-base-uncased"")";0.04057111663848952
inputs;0.01545051670508654
=;0.013464335554916941
"tokenizer(""Hello,";0.02348387521683826
my;0.012252901305694935
dog;0.01247440007677696
is;0.012141712182449587
"cute"",";0.013476799904098153
"return_tensors=""pt"")";0.01946255770898893
outputs;0.01467865222945278
=;0.012654024107296918
model(**inputs);0.017404320023090843
last_hidden_states;0.015186552071242254
=;0.012422627203395252
outputs.last_hidden_state;0.014528041136549295
***;0.012341706143905251
