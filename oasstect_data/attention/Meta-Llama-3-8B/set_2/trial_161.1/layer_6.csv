text;attention
The;0.011745303638310165
easiest;0.012782631396797564
way;0.010696661002383645
to;0.011060290148195357
import;0.015776958975379427
the;0.011716919161181642
BERT;0.024470871541456194
language;0.012417582602928245
model;0.01735599607506098
into;0.017877741085192845
python;0.01646774633008759
for;0.013606942561973662
use;0.010617476623454334
with;0.010360983608966699
PyTorch;0.016289442922852625
is;0.01776002073411324
using;0.011440364581249172
the;0.010408156983560122
Hugging;0.011622617966550963
Face;0.014406732617404177
Transformer's;0.020868273072910597
library,;0.015664367990753943
which;0.010503961989060577
has;0.010762391970385646
built;0.008921855645476439
in;0.011242994631953095
methods;0.010445862462036425
for;0.010793638925494007
pre-training,;0.017750670115341066
inference,;0.011797232898098673
and;0.009160805548245322
deploying;0.009614977931455066
BERT.;0.018436672127545933
â€˜**;0.016814499892536108
from;0.012139544984718711
transformers;0.01393378467210065
import;0.013734860736061208
AutoTokenizer,;0.022902430694086595
BertModel;0.013980929721690967
import;0.016609860814847086
torch;0.010447180947158161
tokenizer;0.012653976644978573
=;0.014504522958457847
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.12920446665415491
model;0.012117819624375293
=;0.010639887660291739
"BertModel.from_pretrained(""bert-base-uncased"")";0.05617894048591366
inputs;0.012262572549305914
=;0.00988842407544554
"tokenizer(""Hello,";0.03674805156673131
my;0.009601299526723527
dog;0.009874526609776467
is;0.009178593229648034
"cute"",";0.011522479859893062
"return_tensors=""pt"")";0.021719132927834974
outputs;0.011632906049802057
=;0.009166448695566348
model(**inputs);0.015067469087187126
last_hidden_states;0.013517698434751382
=;0.009150133537407885
outputs.last_hidden_state;0.011394538816262206
***;0.008567872676437221
