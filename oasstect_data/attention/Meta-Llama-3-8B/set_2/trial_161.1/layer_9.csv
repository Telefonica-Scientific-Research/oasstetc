text;attention
The;0.011414604041085267
easiest;0.007535128887996392
way;0.007342642265017391
to;0.007107401333446803
import;0.00963711878063931
the;0.007758537623233853
BERT;0.02968068170905121
language;0.00924347709292058
model;0.018038087292024312
into;0.014155608190121637
python;0.021317065513227824
for;0.011161121105348903
use;0.007837608257196628
with;0.007423390207759806
PyTorch;0.03030988731088147
is;0.014319449144854592
using;0.00906216004004715
the;0.007763701381824737
Hugging;0.01155788054793561
Face;0.012183269296966562
Transformer's;0.020878853729935656
library,;0.013658264809782066
which;0.007529817119678252
has;0.006554304858921752
built;0.005760356126209291
in;0.0073352819636309605
methods;0.007702352216294296
for;0.007043374698081792
pre-training,;0.013876829215651955
inference,;0.009306479912006031
and;0.006321683449283403
deploying;0.00703645907746291
BERT.;0.018076151824445625
â€˜**;0.01634222382448542
from;0.009225265636007347
transformers;0.013629260914306997
import;0.010020981514403423
AutoTokenizer,;0.034529863659506804
BertModel;0.024440986773702853
import;0.01152399871659732
torch;0.011624176784633233
tokenizer;0.008442864077881258
=;0.010178802358521834
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.22999179575471398
model;0.008321366071220705
=;0.0076966041851558075
"BertModel.from_pretrained(""bert-base-uncased"")";0.06731224090469987
inputs;0.008192489532401741
=;0.006892642996416933
"tokenizer(""Hello,";0.028445246078742385
my;0.006221522059944351
dog;0.0067580442392642616
is;0.005952593313473395
"cute"",";0.0076607236528158885
"return_tensors=""pt"")";0.023355030525121497
outputs;0.007601292279639856
=;0.006379330089686928
model(**inputs);0.012779811137527129
last_hidden_states;0.010513878216340946
=;0.0064130438535579815
outputs.last_hidden_state;0.007748268553175571
***;0.005876623273090377
