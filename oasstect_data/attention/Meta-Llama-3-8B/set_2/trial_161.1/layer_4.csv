text;attention
The;0.013990804994018724
easiest;0.01315190543358482
way;0.012398319146415836
to;0.012167386833485483
import;0.016008815637281546
the;0.013006379086303121
BERT;0.01971839794332742
language;0.01391372443152263
model;0.014461644665459213
into;0.015922271669161995
python;0.015780140803495216
for;0.013517779179528188
use;0.012242371947036321
with;0.01248337888278525
PyTorch;0.01777074853872249
is;0.016082071576115714
using;0.013243143968158226
the;0.011836929815044552
Hugging;0.01403416965419716
Face;0.015297539365389566
Transformer's;0.0194921893376519
library,;0.01640586421464708
which;0.012257857257345723
has;0.011611115993934567
built;0.011142215938900007
in;0.012761734199441632
methods;0.012491918114909366
for;0.011983390456241994
pre-training,;0.018121507886404285
inference,;0.01344407404689543
and;0.011172705931270697
deploying;0.012040648610167943
BERT.;0.017875728115818376
â€˜**;0.020973627207071476
from;0.013486759864684589
transformers;0.015268724341277377
import;0.013956989730380264
AutoTokenizer,;0.020576742327848955
BertModel;0.015050439988141179
import;0.013612440649499145
torch;0.011670036637300754
tokenizer;0.01405157235106698
=;0.013790108582198299
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08602486322443412
model;0.014452922753991312
=;0.012921380412531215
"BertModel.from_pretrained(""bert-base-uncased"")";0.050143414894290805
inputs;0.015068680067552164
=;0.012454988214679401
"tokenizer(""Hello,";0.025929962585427915
my;0.011394681333265249
dog;0.011684300561913262
is;0.010943135433922752
"cute"",";0.012944205312721124
"return_tensors=""pt"")";0.023216397695597628
outputs;0.012979033159260434
=;0.011374829439402672
model(**inputs);0.01602471106892149
last_hidden_states;0.017055868499853695
=;0.011219482756038448
outputs.last_hidden_state;0.013288858881034885
***;0.010611968351030015
