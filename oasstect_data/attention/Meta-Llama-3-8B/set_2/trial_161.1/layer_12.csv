text;attention
The;0.007300151604654698
easiest;0.004562871447449865
way;0.004311364580839854
to;0.003901942994169151
import;0.005734088673785866
the;0.004744839345651406
BERT;0.019317884064186948
language;0.004641155031908394
model;0.007672560943231687
into;0.0073175602768782025
python;0.011623635687969034
for;0.006136919299181145
use;0.004535307825376922
with;0.004003025257899757
PyTorch;0.014506332775114714
is;0.007365326353428959
using;0.005611701532653139
the;0.004661999072875887
Hugging;0.006350933075087796
Face;0.006760020119823832
Transformer's;0.014026898824582393
library,;0.006859672708854387
which;0.004621665265485088
has;0.004146003690807114
built;0.0036309558910042443
in;0.004675966173235608
methods;0.0041997022773700805
for;0.004230097546353981
pre-training,;0.011494324977346915
inference,;0.0066081753435055695
and;0.004020970718699638
deploying;0.004072287535591704
BERT.;0.009199907075740128
â€˜**;0.012346852411123996
from;0.006584442703665528
transformers;0.007431572866866754
import;0.006938090169244626
AutoTokenizer,;0.027281424029927847
BertModel;0.011322800935313342
import;0.006432419173412005
torch;0.005248195400856568
tokenizer;0.007811977604701476
=;0.006219353542146957
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.46507689120061296
model;0.005602183858047383
=;0.005106769886559534
"BertModel.from_pretrained(""bert-base-uncased"")";0.08519036794785442
inputs;0.005530293812144794
=;0.0046349321671754155
"tokenizer(""Hello,";0.03048411612990789
my;0.0037212554754255845
dog;0.004427552002455094
is;0.003670912654758568
"cute"",";0.005236650240474302
"return_tensors=""pt"")";0.018326156152514782
outputs;0.0053089977020466285
=;0.004129728692485063
model(**inputs);0.01188151954043976
last_hidden_states;0.008452067711016953
=;0.004297359369463626
outputs.last_hidden_state;0.0050739459279091
***;0.0033849227007049214
