text;attention
The;0.014635922022201812
easiest;0.017025000214145432
way;0.015816466925686216
to;0.01485731071064041
import;0.020052919275562856
the;0.014598071418194355
BERT;0.02109303924792851
language;0.015318774303256752
model;0.016117464319870108
into;0.01513046486780694
python;0.0163199050439716
for;0.014363730546665746
use;0.013641392707261429
with;0.013644564026628076
PyTorch;0.018501826166108282
is;0.015110325632411256
using;0.01432659155442892
the;0.013331259931410431
Hugging;0.014902668582964764
Face;0.01750124214149946
Transformer's;0.019330461799087234
library,;0.01806912164616751
which;0.01384334181021631
has;0.013294829978321503
built;0.01358121364148251
in;0.014141662123246643
methods;0.014780355431576055
for;0.013597392899318628
pre-training,;0.017436217406985064
inference,;0.014211164394319001
and;0.012783319544985138
deploying;0.01340089800502173
BERT.;0.016829517801363533
â€˜**;0.017214373131165195
from;0.014965883427099223
transformers;0.018589567203568948
import;0.015653811969602135
AutoTokenizer,;0.01882027629431178
BertModel;0.015004607668306913
import;0.01387941172281128
torch;0.013614871730513818
tokenizer;0.0145434929805188
=;0.015416441343869951
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.044208320442996586
model;0.013715855660472087
=;0.014259495234964522
"BertModel.from_pretrained(""bert-base-uncased"")";0.03370120057316316
inputs;0.013748449238721035
=;0.013667949051851607
"tokenizer(""Hello,";0.023955932673186367
my;0.013053742403074373
dog;0.014038930381091882
is;0.012626621591451965
"cute"",";0.013723265084277717
"return_tensors=""pt"")";0.021301245774514203
outputs;0.013561437011550916
=;0.013338370462786943
model(**inputs);0.016517211525835247
last_hidden_states;0.015638160940162477
=;0.013037559187042486
outputs.last_hidden_state;0.014083250767932792
***;0.012531828402421312
