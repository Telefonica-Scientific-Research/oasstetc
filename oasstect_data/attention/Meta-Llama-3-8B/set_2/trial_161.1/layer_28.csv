text;attention
The;0.010758408696154015
easiest;0.009727846536893637
way;0.010108245947294436
to;0.009672241268852614
import;0.01147098450805555
the;0.010622246496420705
BERT;0.019065297862282927
language;0.010051618778253333
model;0.011865577234552607
into;0.011372696055857794
python;0.011665156166578504
for;0.010744053935429002
use;0.009493187905217552
with;0.010377633141020992
PyTorch;0.0231688133074511
is;0.010490484571226536
using;0.01151264440775445
the;0.010814697616334551
Hugging;0.010942161299945947
Face;0.012789156429709235
Transformer's;0.015971326356738858
library,;0.012068989640443065
which;0.009752340421777526
has;0.010060876646126458
built;0.009309726320555753
in;0.010374041967484413
methods;0.009805556097329181
for;0.011055374875422559
pre-training,;0.017277530849935316
inference,;0.011477959843165152
and;0.00889425233965278
deploying;0.009773504098792839
BERT.;0.01629678713831636
â€˜**;0.016582252613859167
from;0.010447644356765689
transformers;0.011112935700846394
import;0.01096453017859622
AutoTokenizer,;0.01995518315845544
BertModel;0.015455865197059432
import;0.009825759633217579
torch;0.01104495648968927
tokenizer;0.010541621308420044
=;0.009972694426896354
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.18575306561022734
model;0.010003344046467057
=;0.009617743051946782
"BertModel.from_pretrained(""bert-base-uncased"")";0.09888090977038481
inputs;0.010413506095563926
=;0.009453486512708824
"tokenizer(""Hello,";0.02186311657521709
my;0.009370040287747737
dog;0.01053575968109066
is;0.009396520556682896
"cute"",";0.011966575743875664
"return_tensors=""pt"")";0.019261207250657233
outputs;0.010730784176109757
=;0.008988680461621209
model(**inputs);0.015149902377616143
last_hidden_states;0.01344365743221873
=;0.00900501924630579
outputs.last_hidden_state;0.01225578037535219
***;0.009176038923375039
