text;attention
The;0.009455033012318278
easiest;0.004824171770701231
way;0.00503822214895015
to;0.004467418214242195
import;0.00661275360538362
the;0.005526038355665327
BERT;0.02915866433864294
language;0.005093964655447911
model;0.011193605968983892
into;0.010286960236715733
python;0.010036795349972497
for;0.008263957419969457
use;0.005190785676694473
with;0.004604071554297828
PyTorch;0.019013754254626352
is;0.010723697215020927
using;0.00648196890660255
the;0.006271621446253099
Hugging;0.006433446113519285
Face;0.007264680155310397
Transformer's;0.01231446201770315
library,;0.008265214554848044
which;0.005470058566259928
has;0.004545393721950402
built;0.0039992304792490435
in;0.004890610052373765
methods;0.004742716670252739
for;0.004702899392422644
pre-training,;0.008531575909368174
inference,;0.006533020129516093
and;0.004309701687686699
deploying;0.004459825407650166
BERT.;0.01547599278286867
â€˜**;0.011646891538336765
from;0.006687958694519233
transformers;0.00689251709567964
import;0.007379322371135134
AutoTokenizer,;0.0338495565116037
BertModel;0.013504489547555726
import;0.007766301929737247
torch;0.006848801460005825
tokenizer;0.008752504314005491
=;0.00752619553673929
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.46223303673114563
model;0.005241188381231661
=;0.005356441128257103
"BertModel.from_pretrained(""bert-base-uncased"")";0.0442520000220709
inputs;0.0053356146338984665
=;0.00503590600602506
"tokenizer(""Hello,";0.020934004233857804
my;0.00409048767144338
dog;0.004451910682062907
is;0.003936441803871978
"cute"",";0.005247456549875461
"return_tensors=""pt"")";0.019011340178378328
outputs;0.005692365091325272
=;0.004425745173748317
model(**inputs);0.009070963329493007
last_hidden_states;0.007871914249028259
=;0.004276052961805527
outputs.last_hidden_state;0.004721585153762718
***;0.003778695247932459
