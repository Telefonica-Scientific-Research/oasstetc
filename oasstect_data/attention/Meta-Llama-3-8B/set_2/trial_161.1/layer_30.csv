text;attention
The;0.004372435087913114
easiest;0.003721580254716191
way;0.0035080361434656985
to;0.003467730044958706
import;0.004398179182599496
the;0.004061707943956467
BERT;0.012893886177102128
language;0.0040607820297876625
model;0.004368059687415266
into;0.0042712076922623125
python;0.005374503228161263
for;0.0042529353115287934
use;0.0034324692840321204
with;0.0040966783919740065
PyTorch;0.013315143651430033
is;0.004210655960768857
using;0.00455040148609659
the;0.004519465147183449
Hugging;0.004927210056462265
Face;0.005368640584239251
Transformer's;0.007679768011092171
library,;0.004744059675511811
which;0.003587950459348193
has;0.0034842656013406963
built;0.0032567323109686766
in;0.003642879969964411
methods;0.0036106432327696046
for;0.003823907754237338
pre-training,;0.006111983313592618
inference,;0.004284068541829903
and;0.0030552101169433743
deploying;0.003445003371359817
BERT.;0.009959904371287798
â€˜**;0.012695827883416836
from;0.0043177868415210955
transformers;0.003903618164529738
import;0.004954384907731867
AutoTokenizer,;0.014658960842923499
BertModel;0.008469708884959263
import;0.004547943223917281
torch;0.004638403874251148
tokenizer;0.004886145147519252
=;0.003730285678781912
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4534537604159814
model;0.0037605956884478966
=;0.0034613935754734242
"BertModel.from_pretrained(""bert-base-uncased"")";0.2427858496395976
inputs;0.004076937674619231
=;0.003512390324650896
"tokenizer(""Hello,";0.008862506521526451
my;0.0034336151343288793
dog;0.003420363747493862
is;0.003246224987300034
"cute"",";0.0043731614890515375
"return_tensors=""pt"")";0.007865234544585533
outputs;0.0037676924323597825
=;0.00314730056965023
model(**inputs);0.0052187889534765575
last_hidden_states;0.004770923332830718
=;0.003043778558997826
outputs.last_hidden_state;0.00402100839427184
***;0.003117324489504356
