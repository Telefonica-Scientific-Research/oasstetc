text;attention
The;0.014304265958575906
easiest;0.012602928830992401
way;0.012150981275391907
to;0.012121224819680934
import;0.015186535601030384
the;0.012343655279227502
BERT;0.030426927856293907
language;0.015491300475482639
model;0.01472315406092948
into;0.017705134846292624
python;0.01799769750569698
for;0.015379134191120442
use;0.01131505994040845
with;0.011504475905291274
PyTorch;0.018070689157040846
is;0.01735108922133597
using;0.013138327514291793
the;0.011612497732574294
Hugging;0.013772682570389294
Face;0.014045535739835987
Transformer's;0.020759864155661854
library,;0.017894304506165512
which;0.012570993699485137
has;0.0111801063778286
built;0.010457881527180198
in;0.011649171760885169
methods;0.011569757904212163
for;0.012301450367591135
pre-training,;0.021241981129588944
inference,;0.013061528764329491
and;0.010485824635710317
deploying;0.011202368154379755
BERT.;0.02041468580847884
â€˜**;0.01795012786171725
from;0.01522604768127226
transformers;0.014667733999742945
import;0.012807862805419054
AutoTokenizer,;0.02326917471210901
BertModel;0.016151140755435923
import;0.013801864986372783
torch;0.011127298925002554
tokenizer;0.01361267423755729
=;0.014102658462246778
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08459253130159056
model;0.012719843760100418
=;0.012549405741746407
"BertModel.from_pretrained(""bert-base-uncased"")";0.046015095429355654
inputs;0.013530388483494745
=;0.012529556369919601
"tokenizer(""Hello,";0.02992877896022408
my;0.010906917856927297
dog;0.010964168733111332
is;0.010328538199520253
"cute"",";0.01164068544746596
"return_tensors=""pt"")";0.01993749633923055
outputs;0.011668067170418715
=;0.011069898595974934
model(**inputs);0.015497433102285714
last_hidden_states;0.014692759305048927
=;0.011147654076784037
outputs.last_hidden_state;0.011422819715778468
***;0.010108159710766527
