text;attention
The;0.009345879299375599
easiest;0.00809205315062903
way;0.008248219586263423
to;0.007884514720725905
import;0.012691736852969412
the;0.008869785410815217
BERT;0.03110461440794666
language;0.009400123921232857
model;0.019701660944274585
into;0.015635098372349245
python;0.018785255428324724
for;0.010850136196841548
use;0.007855938461915037
with;0.007571670578665565
PyTorch;0.02076330627460717
is;0.016162354647074238
using;0.009359573726514497
the;0.008063462897723292
Hugging;0.00907449830857514
Face;0.011506596203294184
Transformer's;0.02024625430985566
library,;0.015425400532010436
which;0.008728407094814514
has;0.0078144099151983
built;0.0064698474291314334
in;0.00815760533207887
methods;0.008473607825107463
for;0.007906496137495715
pre-training,;0.013275899482123408
inference,;0.010154979474766753
and;0.006739736516324582
deploying;0.007211228899248223
BERT.;0.024231971333074333
â€˜**;0.01827762318046762
from;0.010226100976536404
transformers;0.013450858054528975
import;0.011410771788041634
AutoTokenizer,;0.02661897863266715
BertModel;0.014811693858729066
import;0.012780904769483332
torch;0.009473807373243116
tokenizer;0.009994895963212708
=;0.011993938681359492
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.20452457047296718
model;0.009588309305822627
=;0.008414965642774318
"BertModel.from_pretrained(""bert-base-uncased"")";0.0630776877911164
inputs;0.009693105442036372
=;0.008358746347479724
"tokenizer(""Hello,";0.04451544345860985
my;0.007105577673533441
dog;0.0077908012743531565
is;0.0067351848134187525
"cute"",";0.008563872500446486
"return_tensors=""pt"")";0.021397298572846578
outputs;0.01006671390800704
=;0.007327868829834838
model(**inputs);0.015100156861263944
last_hidden_states;0.011228645254052358
=;0.006867440239034967
outputs.last_hidden_state;0.008468645202345476
***;0.006333069460440069
