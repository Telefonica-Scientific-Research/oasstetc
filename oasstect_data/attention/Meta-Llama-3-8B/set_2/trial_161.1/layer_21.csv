text;attention
The;0.013409768969886006
easiest;0.01297700564446528
way;0.012482435402903795
to;0.011881927307371996
import;0.016485343618753704
the;0.012835684321497752
BERT;0.02645206514330648
language;0.013777044627652462
model;0.014635333123829334
into;0.01534594433635482
python;0.016001727215338397
for;0.013484618072561974
use;0.012070370805952194
with;0.013830644947432873
PyTorch;0.024493426672563563
is;0.012876957779106107
using;0.014331211154109938
the;0.013356977507744052
Hugging;0.013938964674661502
Face;0.015299542912086928
Transformer's;0.019715760961894757
library,;0.01425800734051279
which;0.013221416086107067
has;0.013530834314478535
built;0.01186698852482523
in;0.013305323505602406
methods;0.012693967939956507
for;0.013981511324646584
pre-training,;0.018589201204344274
inference,;0.014522513806488776
and;0.011563632799232665
deploying;0.012862879067282507
BERT.;0.01684820922157747
â€˜**;0.022868446613683084
from;0.012904933379996686
transformers;0.013195532478837601
import;0.013093047714899535
AutoTokenizer,;0.02309207764296729
BertModel;0.019820725353351828
import;0.013154810902007617
torch;0.014593842786173226
tokenizer;0.013996610008836034
=;0.013012129113943605
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08014802674443558
model;0.012588606694242633
=;0.012323876121304742
"BertModel.from_pretrained(""bert-base-uncased"")";0.03679146587173293
inputs;0.013922592880107514
=;0.012340435226119073
"tokenizer(""Hello,";0.023035735109751092
my;0.011361720602658684
dog;0.01171771214107193
is;0.011266612736864589
"cute"",";0.012990200542808018
"return_tensors=""pt"")";0.020199625536390306
outputs;0.013096923982153614
=;0.011620245871593065
model(**inputs);0.015995752975892886
last_hidden_states;0.01587936004175274
=;0.011637022246381676
outputs.last_hidden_state;0.01472862314437292
***;0.01169606520114096
