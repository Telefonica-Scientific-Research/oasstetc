text;attention
The;0.011438802111794953
easiest;0.010352803819926211
way;0.010645057215886454
to;0.009850292176409646
import;0.010952932648024376
the;0.010694336189594367
BERT;0.019761257731332416
language;0.010752536000590663
model;0.011901581691362274
into;0.010648240476558369
python;0.012022429803553317
for;0.01075883549951112
use;0.009875915281407507
with;0.010460250565565163
PyTorch;0.02492694371108037
is;0.010725138155588107
using;0.011314717089312141
the;0.010982609688604822
Hugging;0.011832308418357798
Face;0.012376544353677472
Transformer's;0.013323484514111476
library,;0.01104759897714088
which;0.01019788695562466
has;0.010109889435007825
built;0.009529310352692038
in;0.010293869666434858
methods;0.0097683931938239
for;0.010642690178792748
pre-training,;0.014239234519054816
inference,;0.01108579667841127
and;0.009352653122689408
deploying;0.010268036337625889
BERT.;0.018443561932120545
â€˜**;0.01918810350089462
from;0.010517965404312041
transformers;0.01118402600615012
import;0.011055636361030663
AutoTokenizer,;0.017660109173503233
BertModel;0.014942300542886812
import;0.010315918604476722
torch;0.011366276831827445
tokenizer;0.01089974398125427
=;0.010069197466646024
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.158864146774723
model;0.010039214592864111
=;0.0099692609506217
"BertModel.from_pretrained(""bert-base-uncased"")";0.1188396988893284
inputs;0.012569011898942394
=;0.009975070195580603
"tokenizer(""Hello,";0.017602619914300746
my;0.009616442087601878
dog;0.01171059072406075
is;0.009986385297954741
"cute"",";0.012193503096700363
"return_tensors=""pt"")";0.022391484176673974
outputs;0.01233075799191993
=;0.009545072917712312
model(**inputs);0.013113767459614211
last_hidden_states;0.012670289291591861
=;0.009526081586076913
outputs.last_hidden_state;0.011852491965471229
***;0.009428893823611164
