text;attention
The;0.00781739628477378
easiest;0.004637261037404587
way;0.004656318919238982
to;0.004330485954867219
import;0.007071162867448799
the;0.004918700214413456
BERT;0.07738550334873563
language;0.006317745322421836
model;0.011290752093897485
into;0.007065338808555104
python;0.013656269838887652
for;0.006485938568079516
use;0.005121911131415994
with;0.004934445974276348
PyTorch;0.03613958975087195
is;0.006476699419057326
using;0.005916426008633006
the;0.006540485162553587
Hugging;0.007767381038140919
Face;0.00785598687333459
Transformer's;0.016746302076665925
library,;0.0067715851960728915
which;0.004819641068328697
has;0.004513478705236578
built;0.00406237565854523
in;0.004464778504032518
methods;0.004516785955898994
for;0.004327331365039297
pre-training,;0.01091841752557554
inference,;0.0068019370744409205
and;0.004157701147331409
deploying;0.004732869998923803
BERT.;0.011303212819661427
â€˜**;0.009633126711861036
from;0.006929413801702653
transformers;0.006713777457707424
import;0.007017633191319655
AutoTokenizer,;0.03603002373167429
BertModel;0.02788453379943931
import;0.006463760672503885
torch;0.007083804470007451
tokenizer;0.006801139139469846
=;0.005792190395375123
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3903492265808151
model;0.005668086932181553
=;0.004848103330450793
"BertModel.from_pretrained(""bert-base-uncased"")";0.050047500414508166
inputs;0.0065504657410097675
=;0.004450852836669846
"tokenizer(""Hello,";0.016742426430539803
my;0.004104438743582033
dog;0.004518708807946107
is;0.003953908507324824
"cute"",";0.004910919986088313
"return_tensors=""pt"")";0.014488610518976442
outputs;0.005761919939254045
=;0.004315638566307755
model(**inputs);0.009563574197704346
last_hidden_states;0.011155598387674805
=;0.004415193901397057
outputs.last_hidden_state;0.0054848301110906675
***;0.003798376982656805
