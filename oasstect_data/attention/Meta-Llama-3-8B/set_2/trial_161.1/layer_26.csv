text;attention
The;0.013107621393305848
easiest;0.012250807864102407
way;0.012667365374007304
to;0.011442334671166812
import;0.013181777281139357
the;0.012095170486335738
BERT;0.024905914954510652
language;0.01251548459119247
model;0.013687127288340789
into;0.012898428859046228
python;0.014783983096985422
for;0.012209873201829913
use;0.011838244502794608
with;0.01219919737234077
PyTorch;0.02330948823144407
is;0.013372137872065568
using;0.013510139090496066
the;0.012774308970154974
Hugging;0.013284396745823804
Face;0.014955131802412059
Transformer's;0.01681009895840415
library,;0.014193125571618618
which;0.012521437069803564
has;0.01192328466118491
built;0.011335623020613054
in;0.0124566796718004
methods;0.012891906034508254
for;0.012386430343362783
pre-training,;0.015213873564206193
inference,;0.01301919074675406
and;0.010829664364808124
deploying;0.011631003843433003
BERT.;0.0211158584642895
â€˜**;0.024093868943260047
from;0.012758831312546627
transformers;0.013014786988646353
import;0.013638179156092231
AutoTokenizer,;0.02359238771350476
BertModel;0.01974920393057489
import;0.01234621694178041
torch;0.013750018629389418
tokenizer;0.013312319003972246
=;0.011999703082445372
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09787909339910333
model;0.01205108389529472
=;0.011407390132721051
"BertModel.from_pretrained(""bert-base-uncased"")";0.055198900531511275
inputs;0.013201008023725053
=;0.01140168751199097
"tokenizer(""Hello,";0.022506485329580692
my;0.011208914000643128
dog;0.012262444458346905
is;0.011076995815302509
"cute"",";0.012954208775952523
"return_tensors=""pt"")";0.020318742773509847
outputs;0.013670020528772236
=;0.010912729670698621
model(**inputs);0.01570530723150655
last_hidden_states;0.016221256477375372
=;0.01099337281729364
outputs.last_hidden_state;0.01390318527596799
***;0.011554547684209639
