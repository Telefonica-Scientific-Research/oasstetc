text;attention
The;0.015313189862411536
easiest;0.013704713099959328
way;0.013676027311117285
to;0.012757207877667947
import;0.01758198499608721
the;0.012866426513461473
BERT;0.026123815715110573
language;0.012718872987686359
model;0.015392648735893816
into;0.014382535461502338
python;0.016707752479935795
for;0.013805407298363291
use;0.0130627860670248
with;0.012977567474021201
PyTorch;0.018239699149470236
is;0.013765217253993604
using;0.013451638851343236
the;0.013328935633020216
Hugging;0.013585262385972885
Face;0.014460583639849691
Transformer's;0.017146692928960587
library,;0.014641976551373713
which;0.01302864542839224
has;0.013045364196234711
built;0.012126067059401027
in;0.013200433842370261
methods;0.013422293481771775
for;0.013102381651026878
pre-training,;0.016204489851611287
inference,;0.013938932555789313
and;0.012081242969762087
deploying;0.012329986672966553
BERT.;0.0161403276314701
â€˜**;0.01627908182630334
from;0.015401447525570012
transformers;0.014416219960725082
import;0.014965780638925069
AutoTokenizer,;0.02723143113330413
BertModel;0.021403249735282924
import;0.014365642891831931
torch;0.01487012350926386
tokenizer;0.015556769979162829
=;0.013254971021217773
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07775214944735347
model;0.01415749703288396
=;0.013054589563652325
"BertModel.from_pretrained(""bert-base-uncased"")";0.028719142852283497
inputs;0.015878473534141464
=;0.01275749749405627
"tokenizer(""Hello,";0.02020239667201602
my;0.012097030177185131
dog;0.012568013207950313
is;0.011869787549472608
"cute"",";0.012984699045407435
"return_tensors=""pt"")";0.01900775981353087
outputs;0.015180864884714347
=;0.01297084536429956
model(**inputs);0.017308436694209416
last_hidden_states;0.018822059605225025
=;0.012570838239340973
outputs.last_hidden_state;0.014069008715626078
***;0.011973084270041123
