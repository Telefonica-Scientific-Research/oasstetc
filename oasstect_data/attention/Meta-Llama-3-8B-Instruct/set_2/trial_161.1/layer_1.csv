text;attention
The;0.014140914065886717
easiest;0.014645135614859648
way;0.01268819138726571
to;0.01233913196060594
import;0.014539677135714529
the;0.012282160560162231
BERT;0.014702834512784469
language;0.012237253261729086
model;0.011604474150136912
into;0.012167643586068309
python;0.011600038213246615
for;0.012354993722702073
use;0.011543257319907333
with;0.011341312110661583
PyTorch;0.019086727041930172
is;0.01219262825221377
using;0.011721010993956229
the;0.011994350533125924
Hugging;0.013971757776865086
Face;0.012454353517986379
Transformer's;0.014270487822039993
library,;0.014836744438576495
which;0.011409363037664388
has;0.01103455010015205
built;0.011327022307955568
in;0.011174735183259944
methods;0.011452505571947179
for;0.011359085252891471
pre-training,;0.01833488969755453
inference,;0.013210976729885023
and;0.01067290761557596
deploying;0.011428665424408882
BERT.;0.01715994778917254
â€˜**;0.01587469226880293
from;0.011515584364951125
transformers;0.01366402208058587
import;0.013306888759497808
AutoTokenizer,;0.018057397426593708
BertModel;0.01340199344761854
import;0.011054772321136332
torch;0.012024124124060469
tokenizer;0.012855909717488598
=;0.011759621701231108
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.11468294351384681
model;0.010578898508282562
=;0.011243020759549485
"BertModel.from_pretrained(""bert-base-uncased"")";0.09128879388171375
inputs;0.010857155583221733
=;0.011072122985469836
"tokenizer(""Hello,";0.026248920984373248
my;0.010824923524821395
dog;0.01067117805064697
is;0.010548981812245851
"cute"",";0.012731434322885123
"return_tensors=""pt"")";0.028900400389697024
outputs;0.010926389634007744
=;0.010809370975653845
model(**inputs);0.018186467134561044
last_hidden_states;0.014872741275160545
=;0.010304907813938982
outputs.last_hidden_state;0.014905957985867878
***;0.009550657963196959
