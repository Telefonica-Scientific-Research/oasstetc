text;attention
The;0.01738903810670047
easiest;0.015188184887411485
way;0.014558743235957752
to;0.01451861162959307
import;0.016606005903086325
the;0.014434096551964865
BERT;0.02453008398610156
language;0.014349816211137269
model;0.01588625892795848
into;0.01520915242636068
python;0.015619617362253476
for;0.014763444667115717
use;0.013894284567945624
with;0.01439740001999146
PyTorch;0.017272799141750176
is;0.014588184031832109
using;0.014751604143314756
the;0.015074433974783438
Hugging;0.014998263971945779
Face;0.01656345280888278
Transformer's;0.016033659911146688
library,;0.01598420937257253
which;0.014096459730992593
has;0.01407131922311383
built;0.013468873456413151
in;0.014127723124611408
methods;0.014223355195573753
for;0.014682861937665865
pre-training,;0.016195747630052874
inference,;0.0157083255808135
and;0.01344422381107191
deploying;0.014283413292014213
BERT.;0.025226273825741965
â€˜**;0.017940767045048354
from;0.014945988477919586
transformers;0.014484015671188432
import;0.014857931559593222
AutoTokenizer,;0.02215707229929315
BertModel;0.01870821707314613
import;0.014415203726127488
torch;0.01524552982903219
tokenizer;0.016051720014403827
=;0.01410433820731715
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04021657698755594
model;0.01424906045352818
=;0.014014458971204352
"BertModel.from_pretrained(""bert-base-uncased"")";0.026542714542798794
inputs;0.015932630104248134
=;0.014150589815740702
"tokenizer(""Hello,";0.020342310550178537
my;0.013545928578584679
dog;0.014491411092793147
is;0.013349729799212957
"cute"",";0.015303868727923348
"return_tensors=""pt"")";0.01805117563507776
outputs;0.014339726500156625
=;0.013598896045316937
model(**inputs);0.015829090850173543
last_hidden_states;0.01593208928276473
=;0.013480946313827083
outputs.last_hidden_state;0.014328016116020772
***;0.013250073081952796
