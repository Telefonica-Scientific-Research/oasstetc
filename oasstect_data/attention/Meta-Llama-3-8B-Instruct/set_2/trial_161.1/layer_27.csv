text;attention
The;0.014380461921435514
easiest;0.011748242427177381
way;0.011241415601957414
to;0.011007525407231626
import;0.012789564926374468
the;0.01147913798891304
BERT;0.020026838106151506
language;0.01188322218385402
model;0.012477079399776783
into;0.011606344472637628
python;0.014991144719847707
for;0.01129970798572814
use;0.0109336011316095
with;0.01123254207333417
PyTorch;0.018670767567663272
is;0.012837555721470127
using;0.01167681038674261
the;0.01198456446848937
Hugging;0.015507326346988294
Face;0.01282421527624744
Transformer's;0.014197473960981321
library,;0.0147663492946876
which;0.011068264309281107
has;0.010770084592482044
built;0.01059559152261814
in;0.011198302145183733
methods;0.011628596687028335
for;0.011410424686515389
pre-training,;0.015502418698023516
inference,;0.013563233683555257
and;0.010461048605680744
deploying;0.010647925077420796
BERT.;0.01954152068582642
â€˜**;0.016667380927445512
from;0.012775354843202238
transformers;0.012460824340403177
import;0.011965076821914465
AutoTokenizer,;0.024857975606896548
BertModel;0.018819370132902254
import;0.01169984745363661
torch;0.01348864434230199
tokenizer;0.01367129782097456
=;0.011180154817430132
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.15679260980466297
model;0.011397425081050195
=;0.010742824283209261
"BertModel.from_pretrained(""bert-base-uncased"")";0.06085734323644711
inputs;0.011970807121627038
=;0.011435519895599322
"tokenizer(""Hello,";0.019044665686158348
my;0.010360861206934685
dog;0.011720730190276179
is;0.010319205408096279
"cute"",";0.013036217935389588
"return_tensors=""pt"")";0.017023169226437207
outputs;0.011251809383278868
=;0.010354783619904284
model(**inputs);0.01459870623476252
last_hidden_states;0.01343627333328319
=;0.010175344232398133
outputs.last_hidden_state;0.011648361208300515
***;0.010298113742162477
