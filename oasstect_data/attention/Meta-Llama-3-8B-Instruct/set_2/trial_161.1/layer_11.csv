text;attention
The;0.00918007116965221
easiest;0.004910233758330493
way;0.0048845165798491125
to;0.004298591621753673
import;0.006672755713190681
the;0.0052011146743581186
BERT;0.030387902354320137
language;0.004869134166607823
model;0.00935811931303304
into;0.009398180883277717
python;0.010845821653121916
for;0.0072869204758810886
use;0.005417952624742117
with;0.004585410723244015
PyTorch;0.02223447143389871
is;0.009455487727076172
using;0.006129129511452825
the;0.006304338099059818
Hugging;0.006449223500923812
Face;0.007480458859610325
Transformer's;0.010329128695550322
library,;0.007882462809593725
which;0.0051560322869457186
has;0.004486594810311688
built;0.0058711284261937086
in;0.00501406374730353
methods;0.004797912989152759
for;0.004766810758699408
pre-training,;0.009501399758771522
inference,;0.006763950668711341
and;0.004408290167814613
deploying;0.004360390571908538
BERT.;0.015108561052668501
â€˜**;0.013676696264614393
from;0.006487688653477599
transformers;0.006761528295268542
import;0.006684904599066817
AutoTokenizer,;0.03358599871523765
BertModel;0.012740423081072923
import;0.007986783682614193
torch;0.007327656771323612
tokenizer;0.00666386488106815
=;0.009415775070292086
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4754444472585938
model;0.005628179293069076
=;0.00601171684409019
"BertModel.from_pretrained(""bert-base-uncased"")";0.036943969133642826
inputs;0.005757958327043023
=;0.005825791384527172
"tokenizer(""Hello,";0.02176693064939529
my;0.0042984136193547524
dog;0.004418475115459809
is;0.003955086561101989
"cute"",";0.004959443148417389
"return_tensors=""pt"")";0.011179729974515964
outputs;0.005483354617901922
=;0.004390747541991467
model(**inputs);0.009617239857478097
last_hidden_states;0.0067996015450346075
=;0.004188900818786644
outputs.last_hidden_state;0.004494144667372019
***;0.003707988041178684
