text;attention
The;0.01412013075038032
easiest;0.013947650537275943
way;0.012480518584308658
to;0.01220602857653748
import;0.016594932640797443
the;0.012952911379182504
BERT;0.021098108465860504
language;0.013934357022930373
model;0.014007248167376713
into;0.016080511029811173
python;0.015497679416304928
for;0.013712089435791034
use;0.012080260709764151
with;0.012546754559288283
PyTorch;0.016782885243407923
is;0.015754903343428318
using;0.013030431563978703
the;0.011650687980456407
Hugging;0.014159199929028681
Face;0.0156719723623998
Transformer's;0.018151712341124785
library,;0.016274905309088924
which;0.012095450127030897
has;0.011558519272856461
built;0.011007756168273698
in;0.012552380361285954
methods;0.01236502548004165
for;0.011887431504591714
pre-training,;0.018128963148347832
inference,;0.01336340665126489
and;0.011079330678162927
deploying;0.01229685833261769
BERT.;0.017144189800929438
â€˜**;0.02080157867338145
from;0.014674556188904573
transformers;0.014798801527154803
import;0.014038955820630934
AutoTokenizer,;0.021161155509835904
BertModel;0.015444832166046068
import;0.01368326117919921
torch;0.011685815358575423
tokenizer;0.013608987043605093
=;0.013723815815519411
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08890719398690343
model;0.014545598216333189
=;0.012885841101588025
"BertModel.from_pretrained(""bert-base-uncased"")";0.05244735663916405
inputs;0.014790737128260975
=;0.012164485646679837
"tokenizer(""Hello,";0.025699964383878397
my;0.011390650234273192
dog;0.011600785932216614
is;0.0110959449827619
"cute"",";0.012828519922970662
"return_tensors=""pt"")";0.020415074768682023
outputs;0.012679120196683982
=;0.011302030226676757
model(**inputs);0.016248089141036398
last_hidden_states;0.016518437918390163
=;0.011126517095246567
outputs.last_hidden_state;0.013028829213642991
***;0.010487873107831823
