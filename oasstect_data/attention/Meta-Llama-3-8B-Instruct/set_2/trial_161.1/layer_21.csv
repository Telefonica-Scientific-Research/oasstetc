text;attention
The;0.014063160751993575
easiest;0.013413006744037118
way;0.012651796725420876
to;0.012155062798695463
import;0.017208216129630714
the;0.013140077015734896
BERT;0.02910544606983095
language;0.013395314992072284
model;0.014910263795984166
into;0.014903118414558951
python;0.015665929643988066
for;0.013839835901023058
use;0.01257213859690458
with;0.01379532470583334
PyTorch;0.02024262797815516
is;0.01289530482046687
using;0.013532352315659205
the;0.013686389210822835
Hugging;0.014570899647065413
Face;0.01559921984695234
Transformer's;0.018712240182473455
library,;0.01430842365830577
which;0.013515471223855373
has;0.01415105253012241
built;0.011957822986506415
in;0.013201353463607693
methods;0.012793488961120007
for;0.013910011676807244
pre-training,;0.017664142261728334
inference,;0.01448132871926781
and;0.012017861692631327
deploying;0.012746004581319536
BERT.;0.016268300667440783
â€˜**;0.020418502273388525
from;0.0134602731940124
transformers;0.013616972916517374
import;0.013277122396215741
AutoTokenizer,;0.02312687412174536
BertModel;0.019113040441919703
import;0.01367871474139015
torch;0.013862574052346574
tokenizer;0.014438486966303064
=;0.013086965938808046
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07520925203204668
model;0.012885618083102006
=;0.012580413480304873
"BertModel.from_pretrained(""bert-base-uncased"")";0.039771871686175866
inputs;0.01431570211898375
=;0.012570226381096542
"tokenizer(""Hello,";0.022573529438847165
my;0.011679175846145765
dog;0.012287876843473338
is;0.011624715892820648
"cute"",";0.013233137857966922
"return_tensors=""pt"")";0.01882538559040145
outputs;0.013190262977387542
=;0.011908520849519391
model(**inputs);0.01672680212470103
last_hidden_states;0.01664391641344303
=;0.011878811008933964
outputs.last_hidden_state;0.014980785978386792
***;0.011961479643600122
