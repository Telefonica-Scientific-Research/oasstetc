text;attention
The;0.013042037649004615
easiest;0.012097222996071416
way;0.012311095441403309
to;0.01118881164562735
import;0.012900861818748274
the;0.012067843647115984
BERT;0.020488950687678795
language;0.011967755089212521
model;0.013150113590040969
into;0.012272272802405688
python;0.014125919929478923
for;0.011976148263017222
use;0.011402397230593808
with;0.01175465601330292
PyTorch;0.02310728270332089
is;0.012464282751476572
using;0.012346392486772986
the;0.013098884412634812
Hugging;0.013475481880732805
Face;0.013801391299661967
Transformer's;0.01411722568299004
library,;0.012545307495898491
which;0.011675917461652053
has;0.011565871175441165
built;0.011084581302462412
in;0.011552204205057658
methods;0.011371063614117265
for;0.012058872462881601
pre-training,;0.01639452472753896
inference,;0.012926199390895938
and;0.010821349011346714
deploying;0.011562527822808654
BERT.;0.019411221866010107
â€˜**;0.018377270820825874
from;0.012080298447336263
transformers;0.012627560030528326
import;0.012292113881680222
AutoTokenizer,;0.01845734906480649
BertModel;0.01583810504630884
import;0.011575622595057747
torch;0.01240638706289952
tokenizer;0.01193122211175085
=;0.011295536665397704
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.12549856495903994
model;0.011323509277382486
=;0.011204044615744826
"BertModel.from_pretrained(""bert-base-uncased"")";0.0793193908185094
inputs;0.014769761341482668
=;0.011437985714253052
"tokenizer(""Hello,";0.01829143805439357
my;0.010874446067943766
dog;0.012858110299049498
is;0.010958302429530387
"cute"",";0.012732656272067096
"return_tensors=""pt"")";0.021336966514809607
outputs;0.012631641430992269
=;0.010702317618846146
model(**inputs);0.01397260525710978
last_hidden_states;0.014405023069629468
=;0.010770415359566077
outputs.last_hidden_state;0.013136586982513947
***;0.010768099633141289
