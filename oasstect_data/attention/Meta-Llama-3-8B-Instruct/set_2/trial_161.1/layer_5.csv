text;attention
The;0.014981076363159596
easiest;0.013538783243674848
way;0.0126289201987176
to;0.012475665941745389
import;0.01679363064377325
the;0.0126258858827075
BERT;0.029747488974401173
language;0.01576545206924353
model;0.013831016067528902
into;0.018654388434642045
python;0.020275360736615483
for;0.015930216537073598
use;0.011622425098434972
with;0.012043984575776458
PyTorch;0.01716533832574647
is;0.018260091148024397
using;0.013832578380114688
the;0.011643418096030925
Hugging;0.014057743753942307
Face;0.01390539959410354
Transformer's;0.018836672841649037
library,;0.018137504862854415
which;0.012579238764682903
has;0.011472603965270578
built;0.010621173966448988
in;0.011729075293405812
methods;0.01198554139528553
for;0.012365368165068813
pre-training,;0.022401201737860862
inference,;0.013609581399385843
and;0.010696921273449529
deploying;0.011651570959199659
BERT.;0.020551007946235555
â€˜**;0.01830893156205907
from;0.01515532784729348
transformers;0.013664640135470422
import;0.012899639088792325
AutoTokenizer,;0.02287833565229177
BertModel;0.016243827823287434
import;0.013888518367254006
torch;0.011189168894382448
tokenizer;0.013203217142640158
=;0.015291576304706173
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07563287002901087
model;0.013461276563867076
=;0.01352978242046716
"BertModel.from_pretrained(""bert-base-uncased"")";0.04371764475999052
inputs;0.013661769816237464
=;0.01292697858054571
"tokenizer(""Hello,";0.029006966996325528
my;0.011058174071844128
dog;0.01103716718021881
is;0.010670511953227723
"cute"",";0.011946385802311768
"return_tensors=""pt"")";0.01760396750353811
outputs;0.011893208060491631
=;0.011302390777361128
model(**inputs);0.015537560151837446
last_hidden_states;0.014574297548405175
=;0.011412017486685992
outputs.last_hidden_state;0.011674522721025572
***;0.010212998122176678
