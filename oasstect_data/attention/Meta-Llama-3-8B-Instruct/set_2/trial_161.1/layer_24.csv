text;attention
The;0.01583070243840917
easiest;0.01370244927879182
way;0.013926296382550402
to;0.013111428647693508
import;0.01690677639492481
the;0.013283384192869433
BERT;0.026738232585921527
language;0.012960123526998888
model;0.014319614873018985
into;0.013894911088361675
python;0.015446490489767439
for;0.013559726451986312
use;0.013019685591833934
with;0.013393422389125633
PyTorch;0.02016128505023856
is;0.01413825080900242
using;0.013577322663158743
the;0.014039559396431978
Hugging;0.016948690312222417
Face;0.015089870236794927
Transformer's;0.015991709141479737
library,;0.014703276911066036
which;0.01320784236594994
has;0.012906901230295068
built;0.012962117557009168
in;0.013397717088231908
methods;0.013488452592143839
for;0.01287912699760798
pre-training,;0.015210033945305495
inference,;0.013935245201295278
and;0.01253252402380983
deploying;0.013027564717386553
BERT.;0.018414320763674233
â€˜**;0.019889954469311445
from;0.0152795048996522
transformers;0.014612065566517944
import;0.015348264400192577
AutoTokenizer,;0.020372792786132146
BertModel;0.02063273151736857
import;0.014702825251806806
torch;0.014701141428879476
tokenizer;0.013834638575762384
=;0.013582384183343966
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06292599702156629
model;0.013701236284813654
=;0.013514063562687641
"BertModel.from_pretrained(""bert-base-uncased"")";0.034918863616414796
inputs;0.016068241061641453
=;0.013756606166600755
"tokenizer(""Hello,";0.023159520207145252
my;0.012456531536739328
dog;0.012783575084990168
is;0.012368027816974892
"cute"",";0.013816821245347286
"return_tensors=""pt"")";0.019048322852490154
outputs;0.01503282451422126
=;0.012885306189438655
model(**inputs);0.017795295489740386
last_hidden_states;0.01587040446291244
=;0.012668792880859888
outputs.last_hidden_state;0.014989191842798726
***;0.012579019748291873
