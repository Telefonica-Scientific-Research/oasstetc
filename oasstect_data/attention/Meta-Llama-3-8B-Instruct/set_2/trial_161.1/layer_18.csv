text;attention
The;0.0141261310030345
easiest;0.011820949827642604
way;0.01203583396325075
to;0.011209987098574542
import;0.015746862158580958
the;0.011809289264901542
BERT;0.04647021243078722
language;0.011953964268122603
model;0.0183648520780676
into;0.013265349512644135
python;0.01721603157493492
for;0.012345440588250895
use;0.011619194309855642
with;0.011559634101676014
PyTorch;0.023483092148815204
is;0.012083334660915525
using;0.011777825381722135
the;0.01246540142125569
Hugging;0.013692551182745342
Face;0.016286636928110967
Transformer's;0.018986984196088644
library,;0.014380731547688657
which;0.011811076397021966
has;0.011966998400168764
built;0.01129528314175617
in;0.011682422082961948
methods;0.0113820407483731
for;0.0115746437117597
pre-training,;0.015010607234658519
inference,;0.012606322146021061
and;0.010723640209339951
deploying;0.010617780702085435
BERT.;0.016008222153992457
â€˜**;0.013633454986807823
from;0.013023411814273066
transformers;0.01661113154378338
import;0.012675833063439373
AutoTokenizer,;0.030076941247136706
BertModel;0.027189851764911815
import;0.012619774599212517
torch;0.013860165027772578
tokenizer;0.01535736259993942
=;0.012141625243075362
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.089843232363065
model;0.013070650163270688
=;0.011791300327925046
"BertModel.from_pretrained(""bert-base-uncased"")";0.03348318557863715
inputs;0.015867171240039363
=;0.011316488474281707
"tokenizer(""Hello,";0.024549622039459096
my;0.0111626536976308
dog;0.01198418833107332
is;0.01041187942039411
"cute"",";0.011915301643206127
"return_tensors=""pt"")";0.021002485901418
outputs;0.013302059135702399
=;0.010897351077333577
model(**inputs);0.015262592457582087
last_hidden_states;0.015360318271252726
=;0.010871424609779294
outputs.last_hidden_state;0.012765914812651661
***;0.01057330198914069
