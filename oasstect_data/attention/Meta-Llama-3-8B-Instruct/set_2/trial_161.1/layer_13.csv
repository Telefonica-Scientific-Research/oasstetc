text;attention
The;0.008411893970456403
easiest;0.004584619566615836
way;0.00429509628325138
to;0.0042200988422138345
import;0.005886856296037796
the;0.004731066558178496
BERT;0.10085115898598498
language;0.005585803439826063
model;0.013003156781065256
into;0.008024356173427574
python;0.012035192950328172
for;0.006261466835427945
use;0.005189690188590084
with;0.004947175051164889
PyTorch;0.044483055547090325
is;0.00629378769053954
using;0.005590435886820051
the;0.007634115623113254
Hugging;0.007600697273116842
Face;0.009654351849060758
Transformer's;0.013305438708621569
library,;0.005908551682605169
which;0.0041918047795677284
has;0.004152852782563958
built;0.005276513461811457
in;0.004388895438110782
methods;0.004336321946784187
for;0.004337792538457595
pre-training,;0.008666150214086411
inference,;0.006717507656294199
and;0.004328901249155597
deploying;0.004264195842280795
BERT.;0.008137115570495922
â€˜**;0.007510539185504059
from;0.005759595301507966
transformers;0.007730386930028369
import;0.005917180682191053
AutoTokenizer,;0.04755351931686783
BertModel;0.05049828459667845
import;0.006848131759392322
torch;0.00659255291179672
tokenizer;0.006402740897314158
=;0.007538406972808467
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.34081761291955226
model;0.006869554670676238
=;0.004678718446584571
"BertModel.from_pretrained(""bert-base-uncased"")";0.042296102822515426
inputs;0.008120986688745287
=;0.005053062753288513
"tokenizer(""Hello,";0.019070284382636585
my;0.004144322166128264
dog;0.0045473627368112475
is;0.003924385209667029
"cute"",";0.004911243121023444
"return_tensors=""pt"")";0.0146967044130579
outputs;0.006577460624172615
=;0.0041128135107406615
model(**inputs);0.0085070696092368
last_hidden_states;0.008913726805097095
=;0.004238812721414792
outputs.last_hidden_state;0.005159956124603565
***;0.0037123640568137817
