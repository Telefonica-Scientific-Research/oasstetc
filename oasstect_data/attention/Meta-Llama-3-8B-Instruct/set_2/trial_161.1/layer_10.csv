text;attention
The;0.00945495720317466
easiest;0.006967398656649395
way;0.0069682338535302256
to;0.006366984715846057
import;0.009984692464157742
the;0.007776229761558237
BERT;0.04487124385625353
language;0.007328918531721872
model;0.013100181099580992
into;0.014098255262121207
python;0.015957539856411913
for;0.0116727106542411
use;0.007510438730250232
with;0.006706293803911438
PyTorch;0.02178304479615768
is;0.011617223486631606
using;0.00817698390118248
the;0.008132833259808384
Hugging;0.009269327179994277
Face;0.01153579323930188
Transformer's;0.014475395757657307
library,;0.010599289227367335
which;0.006849203459501001
has;0.006169084452386545
built;0.00787140798723319
in;0.007113715343340451
methods;0.006938537434834072
for;0.006968016842227875
pre-training,;0.01348794990039953
inference,;0.009942691831658922
and;0.006294441441388165
deploying;0.006498443519984151
BERT.;0.014307139015761318
â€˜**;0.011748519889639389
from;0.009378730828538587
transformers;0.01056539648653135
import;0.008779347463452726
AutoTokenizer,;0.062143195143925904
BertModel;0.020584184125347675
import;0.009571013028143581
torch;0.009196772247329692
tokenizer;0.009120109610514493
=;0.010325367455084009
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2790950253453449
model;0.009447614059830177
=;0.008396959914883494
"BertModel.from_pretrained(""bert-base-uncased"")";0.039746711286630806
inputs;0.008934843226066915
=;0.00814084923225803
"tokenizer(""Hello,";0.02464243003892231
my;0.006499870918276389
dog;0.006814038671423861
is;0.0060293593506208585
"cute"",";0.00714726109886697
"return_tensors=""pt"")";0.016147771036516555
outputs;0.0083593875210589
=;0.00691723592993199
model(**inputs);0.013111445336658128
last_hidden_states;0.01210892841002059
=;0.006864643184251255
outputs.last_hidden_state;0.007553309744808645
***;0.005835078888897054
