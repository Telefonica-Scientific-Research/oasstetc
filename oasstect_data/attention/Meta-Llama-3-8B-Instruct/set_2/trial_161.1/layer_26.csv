text;attention
The;0.013845779059237786
easiest;0.013141289670800171
way;0.013342518395281408
to;0.011864949895240246
import;0.013928167147762962
the;0.012385176429704518
BERT;0.0247255989260894
language;0.012499893988400465
model;0.013701717784028539
into;0.012884992330430502
python;0.015030720870176968
for;0.012417033248081388
use;0.012298589955795169
with;0.012430345758134991
PyTorch;0.019872231779857086
is;0.013839896939664835
using;0.013365828393111389
the;0.013637414246755745
Hugging;0.013868825081021418
Face;0.01569587900393603
Transformer's;0.016116374253822917
library,;0.01429994075200051
which;0.01262718273691353
has;0.01232182950888623
built;0.011936244794487107
in;0.012440964515422532
methods;0.01317264399739194
for;0.012486996722988745
pre-training,;0.015603075325978665
inference,;0.013970573315647815
and;0.011372041057558289
deploying;0.011847493608647302
BERT.;0.020060844462969702
â€˜**;0.02411927054514365
from;0.013548809709360617
transformers;0.013373319943763702
import;0.013973908825648538
AutoTokenizer,;0.022375492957522086
BertModel;0.019358257523894994
import;0.013030277879209023
torch;0.013225009207665157
tokenizer;0.013312015112953884
=;0.012352673695447798
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09236767830718605
model;0.012671414896479322
=;0.011751402964914417
"BertModel.from_pretrained(""bert-base-uncased"")";0.04810439301875792
inputs;0.01396185016501944
=;0.012071282991712203
"tokenizer(""Hello,";0.02151461527326617
my;0.011409061367394675
dog;0.012388298361185844
is;0.011279477953830186
"cute"",";0.013437593091237343
"return_tensors=""pt"")";0.020124020926960386
outputs;0.0139533452666481
=;0.01130366176369143
model(**inputs);0.01710834373273323
last_hidden_states;0.016985766847557586
=;0.011514180897950083
outputs.last_hidden_state;0.014349349130946591
***;0.012002173685693377
