text;attention
The;0.014882475098584787
easiest;0.017964286538348737
way;0.015898445500259456
to;0.014677307958404343
import;0.020463673083801325
the;0.014848459619323913
BERT;0.021176220059617855
language;0.015802062999684084
model;0.01610948084050356
into;0.015269745889057395
python;0.016510077064317463
for;0.014572045351891971
use;0.01359452164738535
with;0.013689794166349498
PyTorch;0.017465675742743006
is;0.015035461059339937
using;0.014290057431684457
the;0.013230866688485322
Hugging;0.014935615547193644
Face;0.017216344469426922
Transformer's;0.01867727879590005
library,;0.01775924629373959
which;0.013776379685454468
has;0.013426197217109711
built;0.013597389792261287
in;0.013956017217645816
methods;0.01475231074050294
for;0.013735394738612681
pre-training,;0.017509678910650483
inference,;0.014306417960913373
and;0.012832601607239789
deploying;0.01371517460757624
BERT.;0.016116998978950824
â€˜**;0.016785740590589186
from;0.01545351480341204
transformers;0.01825776357187316
import;0.015955024635625606
AutoTokenizer,;0.018363325450998776
BertModel;0.015180231860688492
import;0.01403266418685904
torch;0.013639890029375172
tokenizer;0.014447068100979675
=;0.015210945301599046
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0432772079781963
model;0.013735144732414073
=;0.01442677991363659
"BertModel.from_pretrained(""bert-base-uncased"")";0.03439464519887843
inputs;0.013973537887066486
=;0.013792249475702865
"tokenizer(""Hello,";0.02499398011407847
my;0.013143318050813492
dog;0.01425159298530374
is;0.012794080674522935
"cute"",";0.013851647902349228
"return_tensors=""pt"")";0.01906908412929926
outputs;0.013687470363897622
=;0.013446281638013733
model(**inputs);0.016611789767208163
last_hidden_states;0.015524129997058066
=;0.013096604304446514
outputs.last_hidden_state;0.01422460844770732
***;0.012585974604446187
