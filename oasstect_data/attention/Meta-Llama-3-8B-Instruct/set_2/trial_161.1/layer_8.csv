text;attention
The;0.012026453081063756
easiest;0.007510024203693502
way;0.007128773021717971
to;0.0065960038214592925
import;0.009908033843559002
the;0.007170358491401669
BERT;0.05198398424767548
language;0.008718895730160419
model;0.01429647778324358
into;0.016090497465129944
python;0.02194673272651675
for;0.009246681039057696
use;0.007998295766523039
with;0.007051668990638774
PyTorch;0.03100483299564356
is;0.011981766536387975
using;0.007812611234441021
the;0.007918990421617401
Hugging;0.009178310326660308
Face;0.013439686419200864
Transformer's;0.016123186904673636
library,;0.01302174337919248
which;0.006805393295223865
has;0.0062022171944468895
built;0.006847288135935851
in;0.006684002617348975
methods;0.0071382994446686976
for;0.006398519169306449
pre-training,;0.013629201416968611
inference,;0.009094209347013554
and;0.006234590804716304
deploying;0.006721121321771901
BERT.;0.018869541359890902
â€˜**;0.021435289815718326
from;0.010133134315157437
transformers;0.01049415895164791
import;0.008617564321908553
AutoTokenizer,;0.052965732178231834
BertModel;0.02231890726194014
import;0.010862113473209208
torch;0.008048268670940823
tokenizer;0.009073423294173232
=;0.008715076350153863
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.23476097920119704
model;0.00835002077434975
=;0.00788830057161041
"BertModel.from_pretrained(""bert-base-uncased"")";0.05215815937627869
inputs;0.01009879345349298
=;0.007418742098156034
"tokenizer(""Hello,";0.023394950506594108
my;0.006308873698948607
dog;0.00654694521784575
is;0.0060321046725458375
"cute"",";0.007766024347544573
"return_tensors=""pt"")";0.017466453787542215
outputs;0.008131113766317486
=;0.006615026853041769
model(**inputs);0.01267253596383851
last_hidden_states;0.00966722250934641
=;0.006332285170717883
outputs.last_hidden_state;0.007394853715156589
***;0.0055545531454439
