text;attention
The;0.011528034992890672
easiest;0.010463551905869659
way;0.0105608946987266
to;0.010198445541128335
import;0.012675449784691989
the;0.01103097910977039
BERT;0.019350730947140297
language;0.01055225470992767
model;0.012286573338259887
into;0.011905641113715363
python;0.012598551882913473
for;0.011137456300945463
use;0.010040247869789149
with;0.010654088600017037
PyTorch;0.01698196830397339
is;0.01073351435991231
using;0.011314209567383512
the;0.011574168171856289
Hugging;0.01210901242727369
Face;0.013747724189613435
Transformer's;0.015664912734018797
library,;0.012526223253576895
which;0.010228229675060476
has;0.010576931020626918
built;0.009959895449751193
in;0.010517922423208198
methods;0.010314569616800587
for;0.011433111091409116
pre-training,;0.019440447382413746
inference,;0.012751974665860978
and;0.009585187759430017
deploying;0.010056051773056178
BERT.;0.01563307218440561
â€˜**;0.0179257246304186
from;0.010935930376445928
transformers;0.011752177885426473
import;0.011522608674514446
AutoTokenizer,;0.019770303950484163
BertModel;0.015402174991039042
import;0.010353109240036844
torch;0.011233858334363578
tokenizer;0.010905138719792238
=;0.010329816909765411
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1893641289201808
model;0.010477031833384028
=;0.009869097055898014
"BertModel.from_pretrained(""bert-base-uncased"")";0.07405228595028314
inputs;0.010968520751842301
=;0.009958836275301542
"tokenizer(""Hello,";0.022175349096424712
my;0.009895391382217457
dog;0.011320273863187177
is;0.009713474902112326
"cute"",";0.01202178753201978
"return_tensors=""pt"")";0.018472194747936856
outputs;0.010766094915750994
=;0.0093066589100706
model(**inputs);0.015394649810415457
last_hidden_states;0.014469483248876074
=;0.00944213192001903
outputs.last_hidden_state;0.01250388641769621
***;0.009565851908679392
