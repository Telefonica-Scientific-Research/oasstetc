text;attention
The;0.01603994141522118
easiest;0.01423622065447678
way;0.014349449302271512
to;0.013637396105593997
import;0.01627881189934415
the;0.014186223717039564
BERT;0.019441665265238908
language;0.01366089189929547
model;0.014602014720909124
into;0.014116322850586002
python;0.015549186932517096
for;0.014015694072855418
use;0.013509925346711464
with;0.013970477601755021
PyTorch;0.020027429462128554
is;0.014960726412812634
using;0.013888114891226289
the;0.014646460613272835
Hugging;0.01549546266528655
Face;0.015426497140739555
Transformer's;0.015772341506273897
library,;0.015394958933816257
which;0.013486676644636195
has;0.013463173541795137
built;0.013444039169784632
in;0.013547071164534236
methods;0.014527195691247519
for;0.013637007567800985
pre-training,;0.01579308120958948
inference,;0.014671767174424135
and;0.013113651980419064
deploying;0.013830672895362064
BERT.;0.020400226962761992
â€˜**;0.018674813288585126
from;0.015180185186578507
transformers;0.01426027620117266
import;0.014580030415142645
AutoTokenizer,;0.019942620445203618
BertModel;0.016542298287899656
import;0.014768683194352327
torch;0.015021837107883584
tokenizer;0.014796327886783181
=;0.01410865958154649
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.048744358799703125
model;0.01399020425386555
=;0.013653774845083122
"BertModel.from_pretrained(""bert-base-uncased"")";0.039914417205680244
inputs;0.01567455862855098
=;0.014185424258024633
"tokenizer(""Hello,";0.024089009922789124
my;0.013187465912468293
dog;0.014455134165914764
is;0.013053016602240597
"cute"",";0.015245871296462031
"return_tensors=""pt"")";0.020203800690499055
outputs;0.014472988130499851
=;0.01343359072033312
model(**inputs);0.016293134381406293
last_hidden_states;0.015964718361263064
=;0.013215275165205232
outputs.last_hidden_state;0.016224612141616458
***;0.013002135511519112
