text;attention
The;0.014414393313843685
easiest;0.013416836229273692
way;0.013065918614883909
to;0.01228620910326057
import;0.01544292914335922
the;0.012700841249948747
BERT;0.03336715457685814
language;0.012825235878043538
model;0.015590433103658879
into;0.013333838243149555
python;0.015426501509107786
for;0.01319749175688625
use;0.012629492840120672
with;0.012486572625051639
PyTorch;0.020308198952327442
is;0.013605184806529025
using;0.013037069105362984
the;0.013000807541758655
Hugging;0.015151882921802468
Face;0.014984959799480628
Transformer's;0.016674013442455733
library,;0.013941759049279282
which;0.012603328722906633
has;0.012606793811643558
built;0.011965374455732656
in;0.012517823230062758
methods;0.012960820920915219
for;0.012437334511813593
pre-training,;0.016338142549431712
inference,;0.01402060499997464
and;0.011958336208199886
deploying;0.012258963730857407
BERT.;0.016621282390885405
â€˜**;0.01524777439921083
from;0.014457981581582294
transformers;0.015492590416046063
import;0.014186624309754207
AutoTokenizer,;0.026412106223632333
BertModel;0.02060102000797295
import;0.014685888813118745
torch;0.013708517612540782
tokenizer;0.017148019884350277
=;0.013504106424131518
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07651248502548894
model;0.013604188410067152
=;0.013033033046517716
"BertModel.from_pretrained(""bert-base-uncased"")";0.030488317179872333
inputs;0.01804734439422141
=;0.01287295145592155
"tokenizer(""Hello,";0.022307536080629714
my;0.01204250747961602
dog;0.012763996676127607
is;0.011922761079770255
"cute"",";0.013789316683997008
"return_tensors=""pt"")";0.022480450492939886
outputs;0.013565528929803472
=;0.012374675218341841
model(**inputs);0.016731111556802885
last_hidden_states;0.017049192441163395
=;0.012307113593145931
outputs.last_hidden_state;0.013726364101337356
***;0.011759967143029695
