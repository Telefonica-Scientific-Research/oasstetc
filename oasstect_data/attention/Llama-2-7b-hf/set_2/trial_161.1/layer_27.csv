text;attention
The;1.1936651908519716e-14
easiest;1.00804571020052e-14
way;1.1217868222984045e-14
to;9.526652727798257e-15
import;1.0485123712994777e-14
the;9.161580174448604e-15
BERT;1.41697603302411e-14
language;9.339667637191399e-15
model;1.011649784734209e-14
into;9.634564673204406e-15
python;1.0482745220913988e-14
for;9.364079687644783e-15
use;9.15831926726807e-15
with;9.134264167612289e-15
PyTorch;1.3450454049709935e-14
is;1.0053451542322806e-14
using;9.868163728449327e-15
the;9.342737647332264e-15
Hugging;1.8372771512374726e-14
Face;1.0160720295351207e-14
Transformer's;2.2518937491065898e-13
library,;1.1805562406001292e-14
which;9.275469960779724e-15
has;9.24118546374403e-15
built;8.857691219315192e-15
in;9.111083346907448e-15
methods;9.582773039265021e-15
for;9.4262870992159e-15
pre-training,;1.1907587200755795e-14
inference,;1.0135871306260888e-14
and;8.721100437320928e-15
deploying;9.514045076092195e-15
BERT.;0.999999999998938
â€˜**;1.4198411348477425e-14
from;9.646653346645323e-15
transformers;1.1160078351905489e-14
import;1.0040908749928587e-14
AutoTokenizer,;1.4351576926422322e-14
BertModel;1.223764549169029e-14
import;9.16742781672174e-15
torch;1.0280962954652056e-14
tokenizer;1.0524135385840271e-14
=;9.211565037427346e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.4681848828110587e-13
model;8.961690367178947e-15
=;8.788101270570462e-15
"BertModel.from_pretrained(""bert-base-uncased"")";5.610907858865892e-14
inputs;9.193038841209179e-15
=;8.989065243556325e-15
"tokenizer(""Hello,";2.3622428267508327e-14
my;8.850470819368446e-15
dog;9.361351395559876e-15
is;8.932749425821675e-15
"cute"",";1.0602216891448835e-14
"return_tensors=""pt"")";2.4220122016128854e-14
outputs;9.352338222142833e-15
=;8.701308276572331e-15
model(**inputs);1.711317844426978e-14
last_hidden_states;1.2986221323585545e-14
=;8.75848061270656e-15
outputs.last_hidden_state;1.3705393030834438e-14
***;8.710180540792706e-15
