text;attention
The;2.5877682007976667e-16
easiest;2.165465170843236e-16
way;2.2901799470835126e-16
to;1.9931516979396517e-16
import;2.7084314487140734e-16
the;2.0636611361220782e-16
BERT;7.352029737692479e-16
language;2.8008701892031746e-16
model;2.913717512018349e-16
into;2.288863901772374e-16
python;2.571934361473495e-16
for;2.12836955858261e-16
use;2.0143478727517814e-16
with;2.1915950007140275e-16
PyTorch;5.626106579836881e-16
is;2.25617118564817e-16
using;2.2141126453269113e-16
the;2.1286823883697243e-16
Hugging;3.9052472591318417e-16
Face;2.5710502834733114e-16
Transformer's;5.516106017132716e-15
library,;2.4098590017530175e-16
which;2.0023743300843815e-16
has;1.9970981925431797e-16
built;2.1028737671336902e-16
in;2.059876417111105e-16
methods;2.1577019319069653e-16
for;2.066824509573616e-16
pre-training,;2.592212765763545e-16
inference,;2.482231219923277e-16
and;1.987034923912901e-16
deploying;2.266581129166988e-16
BERT.;0.9999999999999767
â€˜**;3.0675570301533356e-16
from;2.4667356205211645e-16
transformers;2.856348149943221e-16
import;2.42025531519824e-16
AutoTokenizer,;5.016492097954509e-16
BertModel;4.671431644406735e-16
import;2.238188015377814e-16
torch;3.1278078510568397e-16
tokenizer;3.156603448133946e-16
=;2.1884773694643321e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.0140980384211616e-15
model;2.292064905891279e-16
=;2.0911196994599485e-16
"BertModel.from_pretrained(""bert-base-uncased"")";7.300853445562065e-16
inputs;2.1881528503079108e-16
=;2.1403504833257204e-16
"tokenizer(""Hello,";3.2772397634217307e-16
my;2.0024135222903765e-16
dog;2.179692855952636e-16
is;2.003353738501601e-16
"cute"",";2.226819028732963e-16
"return_tensors=""pt"")";4.160180370782604e-16
outputs;2.142370275533917e-16
=;2.0685338784180934e-16
model(**inputs);3.0037509870706946e-16
last_hidden_states;2.980753073813218e-16
=;2.0051201756969846e-16
outputs.last_hidden_state;2.5251693730578384e-16
***;2.0421840819626975e-16
