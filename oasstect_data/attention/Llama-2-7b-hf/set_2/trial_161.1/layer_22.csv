text;attention
The;3.798973701628981e-16
easiest;3.6113277313329744e-16
way;3.4522386661760855e-16
to;3.2761241053897306e-16
import;4.710947294621026e-16
the;3.507310614038991e-16
BERT;1.2277508206723238e-15
language;3.6705063565611933e-16
model;4.1027226747368813e-16
into;3.8684373100332454e-16
python;3.700570554719142e-16
for;3.4853595715931063e-16
use;3.284862771916991e-16
with;3.638597278250707e-16
PyTorch;5.140730198954046e-16
is;3.306032736888896e-16
using;3.8902057540941033e-16
the;3.538913161476745e-16
Hugging;6.074216756950893e-16
Face;4.527030856288054e-16
Transformer's;1.0452094454458491e-14
library,;3.6407939321372486e-16
which;3.114325807072487e-16
has;3.181243739306507e-16
built;3.296982198565669e-16
in;3.3051446804546493e-16
methods;3.3051638635775964e-16
for;3.3466083560420316e-16
pre-training,;4.623352943859893e-16
inference,;4.3438717651790856e-16
and;3.1299252800132335e-16
deploying;3.949543538301676e-16
BERT.;0.9999999999999625
â€˜**;4.129860013115208e-16
from;3.932589145960785e-16
transformers;3.8034507507447567e-16
import;3.7974193068572763e-16
AutoTokenizer,;5.946939389651527e-16
BertModel;5.289304144447795e-16
import;3.554416621385185e-16
torch;3.768915855329266e-16
tokenizer;4.522212130818341e-16
=;3.3403469161811787e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.4855613807440227e-15
model;3.4691538905263245e-16
=;3.2323485812760905e-16
"BertModel.from_pretrained(""bert-base-uncased"")";1.1241341050524072e-15
inputs;3.865586817077528e-16
=;3.320585303352026e-16
"tokenizer(""Hello,";5.684064941368584e-16
my;3.1544207311944556e-16
dog;3.4541867206488926e-16
is;3.332039951509745e-16
"cute"",";3.6397221845486596e-16
"return_tensors=""pt"")";9.537627956021589e-16
outputs;3.432973953249427e-16
=;3.200071784342741e-16
model(**inputs);5.126930901165942e-16
last_hidden_states;4.964565369833943e-16
=;3.2080463269247394e-16
outputs.last_hidden_state;4.575112189912711e-16
***;3.1242685698767597e-16
