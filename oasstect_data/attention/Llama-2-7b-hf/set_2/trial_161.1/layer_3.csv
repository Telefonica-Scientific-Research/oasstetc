text;attention
The;1.9200397267279618e-19
easiest;1.7996952563994218e-19
way;1.701602003480761e-19
to;1.651473956069903e-19
import;1.8977240315748523e-19
the;1.6943094358266127e-19
BERT;2.231434580604316e-19
language;1.8711988010196806e-19
model;1.9090042668313694e-19
into;1.8045945065145884e-19
python;1.8978619215345095e-19
for;1.7017768917356093e-19
use;1.6572577219030964e-19
with;1.64751561963253e-19
PyTorch;2.444438086327049e-19
is;1.808549648368056e-19
using;1.7150502853097366e-19
the;1.6557841314730735e-19
Hugging;2.0268377924528988e-19
Face;1.6973474730310261e-19
Transformer's;2.2805311200756607e-19
library,;1.8777450605724157e-19
which;1.6321482722866643e-19
has;1.6068976022789112e-19
built;1.6516937772111916e-19
in;1.6281380324096895e-19
methods;1.638901625487499e-19
for;1.6365160726092902e-19
pre-training,;2.297162883363702e-19
inference,;1.7271137092652601e-19
and;1.5414243911442466e-19
deploying;1.616153969268035e-19
BERT.;1.0
â€˜**;2.286369853334944e-19
from;1.7765459933442451e-19
transformers;1.7383949664036545e-19
import;1.8593437309749748e-19
AutoTokenizer,;2.3145167646364433e-19
BertModel;1.7632533200091197e-19
import;1.6361577475307157e-19
torch;1.8722751865168524e-19
tokenizer;1.726678002285108e-19
=;1.7531268949420492e-19
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";9.124129706561692e-19
model;1.5970073492215702e-19
=;1.6576419228533832e-19
"BertModel.from_pretrained(""bert-base-uncased"")";3.679948079417529e-19
inputs;1.6653241018711396e-19
=;1.6543151299298398e-19
"tokenizer(""Hello,";2.31508279812232e-19
my;1.5734851487213868e-19
dog;1.564864920077741e-19
is;1.5709347571313315e-19
"cute"",";1.760174190862672e-19
"return_tensors=""pt"")";2.485976587000529e-19
outputs;1.6147355386373408e-19
=;1.6524631107901428e-19
model(**inputs);2.1870604200192645e-19
last_hidden_states;2.1922728937227917e-19
=;1.544030209318278e-19
outputs.last_hidden_state;1.9117635774013096e-19
***;1.4742134072886576e-19
