text;attention
The;1.3584017263902959e-13
easiest;1.0922179715554561e-13
way;1.0708323749057398e-13
to;9.866360087795854e-14
import;1.4409987928323301e-13
the;1.0250593693716241e-13
BERT;2.2792450856408257e-13
language;1.0734828252390227e-13
model;1.7805788560615627e-13
into;1.7266989815745466e-13
python;1.8484929864975714e-13
for;1.1639121052830638e-13
use;1.0406800320061475e-13
with;1.0088646470939869e-13
PyTorch;1.7417601153531244e-13
is;1.5829052046596202e-13
using;1.2050231670323504e-13
the;9.995712082673901e-14
Hugging;1.2750162260512183e-13
Face;1.593855633212243e-13
Transformer's;1.0427138218095172e-12
library,;1.7691769564189211e-13
which;1.022417117351322e-13
has;9.621717344669778e-14
built;8.758477923456265e-14
in;1.1306647296973672e-13
methods;1.0825012095834258e-13
for;1.0684262350693137e-13
pre-training,;2.0775824074385797e-13
inference,;1.3329267729872425e-13
and;9.237126436681503e-14
deploying;1.1023094392641128e-13
BERT.;0.9999999999795726
â€˜**;1.858837309960861e-13
from;1.2673992159923172e-13
transformers;1.5787039552791963e-13
import;1.3403378693234173e-13
AutoTokenizer,;3.3483666365448353e-13
BertModel;1.9565122841463649e-13
import;1.353786043404309e-13
torch;1.2882371301145147e-13
tokenizer;1.5887493457737428e-13
=;1.3243939499543907e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";9.106336033586894e-12
model;1.3440918519840886e-13
=;1.1080546504732315e-13
"BertModel.from_pretrained(""bert-base-uncased"")";1.6922143144007653e-12
inputs;1.4029224748613818e-13
=;1.0838063099412293e-13
"tokenizer(""Hello,";4.1858996214300913e-13
my;9.993452801172751e-14
dog;1.1265921513068331e-13
is;9.344654055569268e-14
"cute"",";1.203425909807985e-13
"return_tensors=""pt"")";3.5479815301515725e-13
outputs;1.2513194088472214e-13
=;9.812603280494685e-14
model(**inputs);3.2502569127095764e-13
last_hidden_states;2.3309442920311896e-13
=;9.933594366695155e-14
outputs.last_hidden_state;1.70794945119255e-13
***;8.607090307213141e-14
