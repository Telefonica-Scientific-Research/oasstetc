text;attention
The;1.1810671820113843e-08
easiest;1.913044632002671e-09
way;1.4979487597491195e-09
to;1.2364958496768887e-09
import;1.911974452116308e-09
the;1.559695062295783e-09
BERT;4.448566280314488e-09
language;1.4433038011543822e-09
model;1.645256549835632e-09
into;1.4345385465513668e-09
python;1.6746210960414156e-09
for;1.2500497861148182e-09
use;1.1597108715035648e-09
with;1.2731524919779512e-09
PyTorch;3.965353803512752e-09
is;1.3675255785273558e-09
using;1.330601315700838e-09
the;1.2936132878922896e-09
Hugging;3.7225043916587894e-09
Face;1.4197350371148574e-09
Transformer's;0.9999989922140973
library,;1.5229759206923914e-09
which;1.1598378595275208e-09
has;1.0638020936757714e-09
built;1.1215876379263786e-09
in;1.105778480768402e-09
methods;1.161244159347159e-09
for;1.2130833214680824e-09
pre-training,;2.684543865408244e-09
inference,;1.448614914708171e-09
and;1.0034113325118285e-09
deploying;1.2825904637354461e-09
BERT.;3.49261649263779e-09
â€˜**;3.860437437362298e-09
from;1.8357985942002163e-09
transformers;2.3888680726273243e-09
import;1.6266105476510258e-09
AutoTokenizer,;5.018626719375616e-09
BertModel;3.048755859591693e-09
import;1.377535543829274e-09
torch;1.7435260014071401e-09
tokenizer;1.945321895020502e-09
=;1.3366908708414025e-09
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";8.405865153084927e-07
model;1.2304360980777484e-09
=;1.1427452787366647e-09
"BertModel.from_pretrained(""bert-base-uncased"")";3.831849918442689e-08
inputs;1.2397119098827104e-09
=;1.1409673152802645e-09
"tokenizer(""Hello,";5.529890612924492e-09
my;1.11049421772139e-09
dog;1.1594808145769031e-09
is;1.17526460398974e-09
"cute"",";1.8138848593592234e-09
"return_tensors=""pt"")";1.1343100121302753e-08
outputs;1.082708586590675e-09
=;1.0288997297694624e-09
model(**inputs);3.1080437678244426e-09
last_hidden_states;2.8972743888727648e-09
=;9.820323465101915e-10
outputs.last_hidden_state;2.1227984527163983e-09
***;9.725334884828114e-10
