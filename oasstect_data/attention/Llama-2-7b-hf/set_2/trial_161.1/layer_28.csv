text;attention
The;6.397227800022956e-14
easiest;4.946248924912591e-14
way;5.06296782767253e-14
to;4.80417287342354e-14
import;5.397214930541022e-14
the;5.0687001443480454e-14
BERT;8.91205885866353e-14
language;5.151295538102828e-14
model;5.601798155739341e-14
into;6.020514902107107e-14
python;5.5408528601469263e-14
for;5.299901142136817e-14
use;4.952816735516905e-14
with;5.641305497401457e-14
PyTorch;1.04296650007295e-13
is;5.072337893278033e-14
using;5.713454835582959e-14
the;5.70960172297505e-14
Hugging;1.0241641434700024e-13
Face;6.377133982134402e-14
Transformer's;2.187754093289498e-12
library,;5.51701024287121e-14
which;4.909253441011566e-14
has;5.0448601296101126e-14
built;4.770672895485944e-14
in;4.9159381165741916e-14
methods;5.1600467271814546e-14
for;5.134010552785435e-14
pre-training,;6.871036230789181e-14
inference,;5.224227450884715e-14
and;4.726809990282557e-14
deploying;5.10039516779357e-14
BERT.;0.9999999999930798
â€˜**;9.719411546947373e-14
from;5.763263142025979e-14
transformers;7.610388676879398e-14
import;6.608703012533917e-14
AutoTokenizer,;9.582485976099923e-14
BertModel;6.975373703694069e-14
import;5.2753382760161356e-14
torch;6.152713135709508e-14
tokenizer;5.688907987863478e-14
=;5.326571910657442e-14
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";8.967738317461021e-13
model;4.7027936709337953e-14
=;4.95969216756375e-14
"BertModel.from_pretrained(""bert-base-uncased"")";2.9724351159888286e-13
inputs;4.906343047727181e-14
=;4.8633028745183596e-14
"tokenizer(""Hello,";1.1935802813333613e-13
my;4.94734444773431e-14
dog;4.994730038767915e-14
is;5.0009407597463616e-14
"cute"",";5.978576590575823e-14
"return_tensors=""pt"")";1.4609975304585247e-13
outputs;4.671436311131167e-14
=;4.63437228342901e-14
model(**inputs);7.104787917567408e-14
last_hidden_states;6.6905339629904e-14
=;4.6114656408533335e-14
outputs.last_hidden_state;6.153333714750369e-14
***;4.652770863586967e-14
