text;attention
The;8.35573467230544e-16
easiest;6.010666613055956e-16
way;5.877580716689217e-16
to;6.016116122444056e-16
import;7.3548259448290345e-16
the;5.866071186710617e-16
BERT;1.042270744437155e-15
language;7.953603984102409e-16
model;8.634296761152006e-16
into;8.156720702839105e-16
python;7.952990084892042e-16
for;6.815148933043699e-16
use;5.791672782252038e-16
with;5.725930825323747e-16
PyTorch;9.163254088632864e-16
is;7.602918503097777e-16
using;6.359160181464607e-16
the;5.754032522173433e-16
Hugging;7.367838805737227e-16
Face;8.175108733474227e-16
Transformer's;2.1904258183516802e-15
library,;8.045460520647384e-16
which;6.173100250004091e-16
has;5.823051597918688e-16
built;5.386827521051865e-16
in;6.0105262649103e-16
methods;5.970722675321777e-16
for;6.071256688885886e-16
pre-training,;1.1454808938058928e-15
inference,;6.849875018828441e-16
and;5.440071094127105e-16
deploying;6.139272248336717e-16
BERT.;0.999999999999936
â€˜**;8.686217603946125e-16
from;7.65285312234741e-16
transformers;7.040788545206843e-16
import;7.331304779774571e-16
AutoTokenizer,;1.4521816813569803e-15
BertModel;8.746236965305416e-16
import;7.606235292401029e-16
torch;6.364226776145593e-16
tokenizer;7.605387430116627e-16
=;7.832662121802152e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.5063437231471038e-14
model;7.461070759315831e-16
=;7.466374529477824e-16
"BertModel.from_pretrained(""bert-base-uncased"")";3.302940330701759e-15
inputs;7.3854319473913305e-16
=;6.850242076308823e-16
"tokenizer(""Hello,";1.3667061763425231e-15
my;5.592317003050741e-16
dog;5.579612032815797e-16
is;5.475776118403139e-16
"cute"",";6.625597058622776e-16
"return_tensors=""pt"")";1.2225245084116329e-15
outputs;6.561751757847048e-16
=;6.09361686286914e-16
model(**inputs);9.801829001979423e-16
last_hidden_states;1.3082355413300703e-15
=;6.033142553036725e-16
outputs.last_hidden_state;8.100885073119049e-16
***;5.251761272877503e-16
