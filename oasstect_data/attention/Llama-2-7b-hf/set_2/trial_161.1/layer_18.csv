text;attention
The;1.5840279419943571e-15
easiest;1.4095514116696805e-15
way;1.385558919176267e-15
to;1.2666744894330976e-15
import;1.6941689179648788e-15
the;1.3217799218788142e-15
BERT;3.745196108764317e-15
language;1.6041466752637437e-15
model;2.2355908132961787e-15
into;1.5504161737032181e-15
python;1.7205161050032406e-15
for;1.4063865866743935e-15
use;1.345477111728919e-15
with;1.381142926531272e-15
PyTorch;2.350113118941196e-15
is;1.4135086187473775e-15
using;1.4708288394320094e-15
the;1.421219964999725e-15
Hugging;1.9474827180015177e-15
Face;2.040376400518739e-15
Transformer's;2.7033583674569404e-14
library,;1.5406430245510111e-15
which;1.28896669177426e-15
has;1.2870043378671266e-15
built;1.2753036256588789e-15
in;1.4683514295606015e-15
methods;1.4569601241478757e-15
for;1.3922712607963624e-15
pre-training,;2.3138036968498323e-15
inference,;1.6102460586516399e-15
and;1.2690790649972653e-15
deploying;1.449476675495573e-15
BERT.;0.999999999999833
â€˜**;2.0498501327030263e-15
from;1.5307774577698144e-15
transformers;1.9854868483703424e-15
import;1.6063154838408417e-15
AutoTokenizer,;3.3702094451362466e-15
BertModel;3.141020954453532e-15
import;1.6280563734575276e-15
torch;1.5727395212632348e-15
tokenizer;2.1784294725352624e-15
=;1.5413938675417667e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.853632015318499e-14
model;1.5564714355452015e-15
=;1.3547015750975622e-15
"BertModel.from_pretrained(""bert-base-uncased"")";9.71850616007939e-15
inputs;1.635600539369337e-15
=;1.3842295138896821e-15
"tokenizer(""Hello,";3.194811326041935e-15
my;1.276645265692122e-15
dog;1.348381859069366e-15
is;1.2750141591215932e-15
"cute"",";1.5769930437902063e-15
"return_tensors=""pt"")";3.3972422418578897e-15
outputs;1.4615109641124465e-15
=;1.3264669908553319e-15
model(**inputs);2.828087157265634e-15
last_hidden_states;2.218537563310512e-15
=;1.3393580270213006e-15
outputs.last_hidden_state;1.8602803824520257e-15
***;1.2359020783428234e-15
