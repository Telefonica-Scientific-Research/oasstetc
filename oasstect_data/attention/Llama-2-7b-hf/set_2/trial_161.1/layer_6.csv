text;attention
The;1.882758949947835e-16
easiest;1.3441169792881292e-16
way;1.3250981551663437e-16
to;1.2943906395049352e-16
import;1.6491001916053497e-16
the;1.3351571622854336e-16
BERT;1.90566289401014e-16
language;1.6284046099396946e-16
model;1.703875707670419e-16
into;1.6327037939410406e-16
python;1.5125915440472985e-16
for;1.4452634329267716e-16
use;1.2719165068140782e-16
with;1.3092277467967786e-16
PyTorch;1.8284802313715234e-16
is;1.6565524806379954e-16
using;1.3901233964209532e-16
the;1.2285200809871826e-16
Hugging;1.541108334038939e-16
Face;1.821412484844555e-16
Transformer's;3.062701509717649e-16
library,;1.5864045980610215e-16
which;1.262333782330411e-16
has;1.2162355801351068e-16
built;1.1701685664795375e-16
in;1.225427840159787e-16
methods;1.276473910758923e-16
for;1.2591021134778806e-16
pre-training,;2.032982398401944e-16
inference,;1.4467399804562244e-16
and;1.1843532077507705e-16
deploying;1.3130110499775846e-16
BERT.;0.9999999999999858
â€˜**;1.6919705609184794e-16
from;1.6812879279396673e-16
transformers;1.6546100818331873e-16
import;1.6207354876488873e-16
AutoTokenizer,;2.881899186855453e-16
BertModel;1.8108446379185993e-16
import;1.5062396200060815e-16
torch;1.4088197158483886e-16
tokenizer;1.6655439532449758e-16
=;1.5832350711031125e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";3.695555294194141e-15
model;1.6406761125843562e-16
=;1.4347704255838508e-16
"BertModel.from_pretrained(""bert-base-uncased"")";6.138513895597458e-16
inputs;1.4190008409446357e-16
=;1.4425260762189437e-16
"tokenizer(""Hello,";2.9237490834285115e-16
my;1.2152376484256095e-16
dog;1.2258810532368e-16
is;1.1972868096751654e-16
"cute"",";1.3894772841747246e-16
"return_tensors=""pt"")";2.146977989670309e-16
outputs;1.2899681230555124e-16
=;1.220411550148482e-16
model(**inputs);1.901971839036148e-16
last_hidden_states;2.2778249942694094e-16
=;1.256958102715493e-16
outputs.last_hidden_state;1.4525950632015047e-16
***;1.1341399932127988e-16
