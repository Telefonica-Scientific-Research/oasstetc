text;attention
The;4.124000742641294e-13
easiest;3.2977942847540975e-13
way;3.304639245590646e-13
to;3.0095114262700856e-13
import;5.189698732285776e-13
the;3.158328518029013e-13
BERT;2.176908272656729e-12
language;4.0913165952299787e-13
model;7.575021663154233e-13
into;4.158361762512678e-13
python;6.229323514233978e-13
for;3.4974230999639864e-13
use;3.164535080314601e-13
with;3.214800845031712e-13
PyTorch;9.88708876913672e-13
is;3.9826503663144717e-13
using;3.919490155328501e-13
the;3.32287796953088e-13
Hugging;4.079087818451399e-13
Face;6.930441902849264e-13
Transformer's;7.693697767509418e-12
library,;4.3664474232173854e-13
which;2.957420930679074e-13
has;2.8609219436953374e-13
built;2.70684535670038e-13
in;3.138978599971418e-13
methods;3.123927382030229e-13
for;3.173895065470403e-13
pre-training,;6.806767835502088e-13
inference,;4.973881202871502e-13
and;2.864432674791891e-13
deploying;3.601661844021831e-13
BERT.;0.9999999999069049
â€˜**;4.602417898378615e-13
from;3.7512955737566804e-13
transformers;5.798216614623705e-13
import;4.359672117150507e-13
AutoTokenizer,;1.3880967764416452e-12
BertModel;9.216573724003543e-13
import;4.173146420542881e-13
torch;4.684830422577479e-13
tokenizer;6.971496179732131e-13
=;3.8205769388140203e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";5.065428652454018e-11
model;3.88871666286761e-13
=;3.2893394310809633e-13
"BertModel.from_pretrained(""bert-base-uncased"")";5.88090740155688e-12
inputs;3.9473157462393875e-13
=;3.157175079207892e-13
"tokenizer(""Hello,";1.0354537053934433e-12
my;2.83881995653958e-13
dog;3.1053002835102145e-13
is;2.8389438186595766e-13
"cute"",";3.7244163963050804e-13
"return_tensors=""pt"")";1.0625750366405457e-12
outputs;3.5955934087687756e-13
=;3.0934870712756537e-13
model(**inputs);7.311648670109698e-13
last_hidden_states;6.510815469249404e-13
=;3.130983776444723e-13
outputs.last_hidden_state;4.861239807823392e-13
***;2.6475864337699587e-13
