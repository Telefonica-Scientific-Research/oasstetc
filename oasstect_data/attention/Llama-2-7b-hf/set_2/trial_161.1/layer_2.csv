text;attention
The;7.841714795038567e-20
easiest;8.45238618532993e-20
way;7.655976180528968e-20
to;7.07310681894703e-20
import;8.765692060800232e-20
the;7.080658468083818e-20
BERT;8.76181170724176e-20
language;7.979341043817485e-20
model;6.914048205934404e-20
into;7.075573656028641e-20
python;8.74894261371005e-20
for;6.861900239501217e-20
use;7.186050645709424e-20
with;6.732751882564884e-20
PyTorch;1.1357159149171848e-19
is;6.970222216814685e-20
using;6.972567557973912e-20
the;7.105382757572876e-20
Hugging;9.068360301535328e-20
Face;7.210861096900741e-20
Transformer's;9.909412264067501e-20
library,;8.191077176913113e-20
which;6.785542494829442e-20
has;6.737684903628368e-20
built;7.014097655394452e-20
in;6.603475945893493e-20
methods;7.339077952765495e-20
for;6.600527867561376e-20
pre-training,;9.759219367938222e-20
inference,;7.595986070582263e-20
and;6.602667938970241e-20
deploying;7.066768103860413e-20
BERT.;1.0
â€˜**;1.0460763031672933e-19
from;6.921227579447633e-20
transformers;7.655691662975539e-20
import;7.63454278937277e-20
AutoTokenizer,;9.940522828144117e-20
BertModel;7.507297178518231e-20
import;6.949208328165888e-20
torch;9.10262252551059e-20
tokenizer;7.258933805603326e-20
=;6.887668332747653e-20
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.829361415569738e-19
model;6.357255094991961e-20
=;6.701870150787032e-20
"BertModel.from_pretrained(""bert-base-uncased"")";3.062402508242196e-19
inputs;6.60957849726558e-20
=;6.564439182752793e-20
"tokenizer(""Hello,";1.1409600755050267e-19
my;6.513486552627334e-20
dog;6.762147173741035e-20
is;6.385710309080914e-20
"cute"",";7.493009322267e-20
"return_tensors=""pt"")";1.2585604197593653e-19
outputs;6.49064050953019e-20
=;6.521501670607791e-20
model(**inputs);1.059487367342694e-19
last_hidden_states;9.063971239196153e-20
=;6.332599829487837e-20
outputs.last_hidden_state;1.010062239335932e-19
***;6.095673628532079e-20
