text;attention
The;3.434453242950769e-15
easiest;3.3012715728386878e-15
way;3.151578339267844e-15
to;2.878012539286018e-15
import;4.761889777195752e-15
the;2.9931124572714502e-15
BERT;1.3948077202820216e-14
language;4.7651680149238225e-15
model;5.635943534265609e-15
into;3.567080076380904e-15
python;4.702340634202569e-15
for;3.1850825912051487e-15
use;2.961296459804382e-15
with;3.146082878993563e-15
PyTorch;1.0877261216402748e-14
is;3.1322697304001863e-15
using;3.448778145185318e-15
the;3.2881015072619715e-15
Hugging;4.569931037242021e-15
Face;5.292942347527822e-15
Transformer's;1.4483030633549016e-13
library,;3.625748615278543e-15
which;2.9804301885003073e-15
has;2.933429517638271e-15
built;2.812689268956651e-15
in;3.20180666433667e-15
methods;3.0576686567530316e-15
for;3.094680151588297e-15
pre-training,;6.211660979122849e-15
inference,;3.8664181798618505e-15
and;2.8947068242749215e-15
deploying;3.36426602846753e-15
BERT.;0.9999999999995282
â€˜**;4.050867279632156e-15
from;3.512098106585314e-15
transformers;6.159825517272999e-15
import;3.315003736317125e-15
AutoTokenizer,;8.48564310751366e-15
BertModel;6.853618952399961e-15
import;3.500567098064863e-15
torch;8.833156005571892e-15
tokenizer;5.755718078790306e-15
=;3.4250764574244633e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";5.661589386842108e-14
model;3.4334014537848275e-15
=;3.0784779730479967e-15
"BertModel.from_pretrained(""bert-base-uncased"")";1.9793712144877174e-14
inputs;3.3913150653802055e-15
=;3.2884936498780362e-15
"tokenizer(""Hello,";5.883982617041749e-15
my;2.946054789663593e-15
dog;3.443639967378171e-15
is;2.9274845561750734e-15
"cute"",";3.3461331890699074e-15
"return_tensors=""pt"")";7.0654563843396754e-15
outputs;3.1558175988871777e-15
=;3.0821557799130355e-15
model(**inputs);5.442448565627321e-15
last_hidden_states;4.9380806556267995e-15
=;3.07303080419795e-15
outputs.last_hidden_state;4.093123975192239e-15
***;2.8548302556858434e-15
