text;attention
The;6.354252662394261e-18
easiest;7.020375060465883e-18
way;6.38883742711024e-18
to;5.573091759260784e-18
import;8.069559295679161e-18
the;5.633764688205875e-18
BERT;7.614166994888477e-18
language;6.390537564058587e-18
model;7.052408482393098e-18
into;5.661198343901032e-18
python;6.192170958333691e-18
for;5.61664437697399e-18
use;5.640389940390608e-18
with;5.447941669112918e-18
PyTorch;8.655415555021894e-18
is;6.101055761639991e-18
using;5.7425950173875266e-18
the;5.241161517976458e-18
Hugging;7.240227781367802e-18
Face;6.309078283763847e-18
Transformer's;9.357701845419431e-18
library,;6.537057104478328e-18
which;5.143795740470337e-18
has;5.193213179666977e-18
built;5.259561781641362e-18
in;6.1775077356253374e-18
methods;5.860101628855893e-18
for;5.437568064915435e-18
pre-training,;8.018784967356363e-18
inference,;5.5929932589495785e-18
and;4.993711952611018e-18
deploying;5.611962804549243e-18
BERT.;0.9999999999999996
â€˜**;7.909794921787746e-18
from;5.843690590724371e-18
transformers;6.076327042856975e-18
import;6.052408562001648e-18
AutoTokenizer,;7.905789347020794e-18
BertModel;6.039674456188853e-18
import;5.7182931534550205e-18
torch;6.316667027950694e-18
tokenizer;6.011333598892397e-18
=;5.990201547938504e-18
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.998568429797233e-17
model;5.3380989297333726e-18
=;5.519491825891138e-18
"BertModel.from_pretrained(""bert-base-uncased"")";2.4984866785283157e-17
inputs;5.709662186637751e-18
=;5.58815586420924e-18
"tokenizer(""Hello,";9.922407856602656e-18
my;5.043154087450173e-18
dog;5.583704690788793e-18
is;4.98949434498608e-18
"cute"",";5.726419540875916e-18
"return_tensors=""pt"")";9.273802966618544e-18
outputs;5.220085463671356e-18
=;5.207149366126907e-18
model(**inputs);8.69492900580387e-18
last_hidden_states;7.423614912666531e-18
=;5.152250652553342e-18
outputs.last_hidden_state;7.28187794480578e-18
***;4.729863242609393e-18
