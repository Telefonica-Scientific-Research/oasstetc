text;attention
The;2.3465615448340774e-15
easiest;1.958316140137887e-15
way;1.9429638761987814e-15
to;1.8395402551488828e-15
import;2.798127622543232e-15
the;1.9642394350996263e-15
BERT;6.605098331999315e-15
language;2.186722204052815e-15
model;2.358354436379996e-15
into;2.3543888119816673e-15
python;2.313493757137067e-15
for;2.0305286935725405e-15
use;1.8878494145979103e-15
with;2.068304036714413e-15
PyTorch;3.623002257977148e-15
is;2.0549156809982043e-15
using;2.1225649017910545e-15
the;2.0291435966204353e-15
Hugging;2.9397093118055988e-15
Face;2.1630319835534078e-15
Transformer's;4.8491741824169586e-14
library,;2.2107523981334082e-15
which;1.9751739443505468e-15
has;1.8723103420053193e-15
built;1.874744852058348e-15
in;1.9040214590777335e-15
methods;1.9461224715136676e-15
for;1.9512297648143333e-15
pre-training,;3.0587936272662095e-15
inference,;2.1757170713647447e-15
and;1.8056373903605e-15
deploying;2.118317156003085e-15
BERT.;0.999999999999784
â€˜**;2.974382107184878e-15
from;2.0493493909122795e-15
transformers;2.739368335738715e-15
import;2.2729167995902937e-15
AutoTokenizer,;3.8254007712676924e-15
BertModel;3.1528618420379277e-15
import;2.1049113229819706e-15
torch;2.179766193540159e-15
tokenizer;2.6637014898777873e-15
=;1.9571222350676323e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.2124080595247014e-14
model;2.04766589886621e-15
=;1.8999142770308117e-15
"BertModel.from_pretrained(""bert-base-uncased"")";6.870559290923684e-15
inputs;2.2206445152662012e-15
=;1.9015486475948725e-15
"tokenizer(""Hello,";3.937921746716329e-15
my;1.846007434261306e-15
dog;1.9299815038540344e-15
is;1.883123132952706e-15
"cute"",";2.1507655039634514e-15
"return_tensors=""pt"")";5.1267488609540446e-15
outputs;2.044244879957789e-15
=;1.817211634514958e-15
model(**inputs);2.760004305935016e-15
last_hidden_states;2.6782811499798833e-15
=;1.7895348875784274e-15
outputs.last_hidden_state;2.434488325408236e-15
***;1.8344248419913372e-15
