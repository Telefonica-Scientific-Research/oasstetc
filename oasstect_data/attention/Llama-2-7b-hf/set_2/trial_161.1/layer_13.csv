text;attention
The;1.1656389062678406e-12
easiest;6.870815247106921e-13
way;6.727448643522929e-13
to;6.48043255370097e-13
import;8.183389942599834e-13
the;6.584591068985104e-13
BERT;1.6953278218382535e-12
language;7.270473089550821e-13
model;1.192517526085682e-12
into;9.335712065973364e-13
python;1.2209383641048097e-12
for;8.60649594443549e-13
use;6.501676565455252e-13
with;6.604271111562156e-13
PyTorch;1.5931835022216321e-12
is;1.0250373762995209e-12
using;8.34400875083167e-13
the;7.038821061808493e-13
Hugging;9.214348023486198e-13
Face;1.1557606572183105e-12
Transformer's;1.2291858393653838e-11
library,;1.2320318889357473e-12
which;7.100468680887293e-13
has;6.648301595355229e-13
built;5.836748607494434e-13
in;7.541661284020481e-13
methods;6.691396695393818e-13
for;6.909503876892385e-13
pre-training,;1.5508359757080953e-12
inference,;9.64277043014955e-13
and;6.447240826833062e-13
deploying;7.238075089054654e-13
BERT.;0.9999999998354223
â€˜**;1.2573419337692744e-12
from;9.309129329958032e-13
transformers;9.810009130184355e-13
import;1.1428933178557841e-12
AutoTokenizer,;3.0360692509036536e-12
BertModel;1.4772997629492902e-12
import;1.1519757849839378e-12
torch;1.0284554362825998e-12
tokenizer;1.5139107752335992e-12
=;1.5150747269029991e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";7.555743351025843e-11
model;1.0025641393149238e-12
=;8.425903934081239e-13
"BertModel.from_pretrained(""bert-base-uncased"")";1.5402064389038893e-11
inputs;7.829844343065723e-13
=;7.549954053678754e-13
"tokenizer(""Hello,";3.053030554174277e-12
my;6.455793965486465e-13
dog;6.649460586972306e-13
is;6.226471958882304e-13
"cute"",";8.025372538954573e-13
"return_tensors=""pt"")";2.9179660755715484e-12
outputs;7.148581460641989e-13
=;7.011683335801377e-13
model(**inputs);1.8895762306167506e-12
last_hidden_states;1.4573157225498503e-12
=;7.086812851940543e-13
outputs.last_hidden_state;1.135514023150327e-12
***;5.813651558473115e-13
