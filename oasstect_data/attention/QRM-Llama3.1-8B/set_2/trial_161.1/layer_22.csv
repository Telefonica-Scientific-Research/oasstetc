text;attention
The;0.01555583017741326
easiest;0.013702810500553476
way;0.012811889527573074
to;0.012417043006602868
import;0.016446419881182006
the;0.013637401594843444
BERT;0.031145196746462274
language;0.013487332016880195
model;0.018838952281114128
into;0.013597092794544996
python;0.015422645762169004
for;0.01314182461245123
use;0.01281373635274591
with;0.012822200251177071
PyTorch;0.01971281893274802
is;0.014169610390480142
using;0.013433113211190782
the;0.013453696929182324
Hugging;0.014953320992357811
Face;0.01433223904101829
Transformer's;0.018564463822809064
library,;0.015629969478738008
which;0.012737320456088472
has;0.013023706181664924
built;0.01187641282951329
in;0.012739770641289574
methods;0.013000963204156018
for;0.012706439011950142
pre-training,;0.015262085089477693
inference,;0.01482152753973797
and;0.011811343714074968
deploying;0.012012019588873811
BERT.;0.018941298497845767
â€˜**;0.02146868967358796
from;0.013798597034838737
transformers;0.013303389002550327
import;0.013223073893414847
AutoTokenizer,;0.023163637691757683
BertModel;0.020996656502195805
import;0.013625910875586013
torch;0.014305177491202145
tokenizer;0.015540991863955392
=;0.01312622732746868
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0661580890542136
model;0.013814725444912204
=;0.012710633221754697
"BertModel.from_pretrained(""bert-base-uncased"")";0.03375920203926085
inputs;0.01591430454293536
=;0.012807481554502927
"tokenizer(""Hello,";0.02053332913957063
my;0.011778398385852477
dog;0.01216140684028705
is;0.011499764445873101
"cute"",";0.013536473507074076
"return_tensors=""pt"")";0.022326045681481783
outputs;0.014534424953810169
=;0.012335719360886614
model(**inputs);0.0166831055381566
last_hidden_states;0.019115127251639774
=;0.012208664493804559
outputs.last_hidden_state;0.014630005414378922
***;0.011918252714137078
