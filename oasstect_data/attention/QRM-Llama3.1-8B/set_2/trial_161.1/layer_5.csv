text;attention
The;0.014687837866912955
easiest;0.012831359832754026
way;0.012234783981144092
to;0.011825584811273278
import;0.017261395015056146
the;0.012601527496403349
BERT;0.030769593055347624
language;0.016092940892712435
model;0.016059290445347915
into;0.019717542114946057
python;0.02023846414616726
for;0.015114313128833925
use;0.0111362769390994
with;0.011194166077185416
PyTorch;0.017698955952683792
is;0.0158494737460402
using;0.012533176666782105
the;0.011158471229282837
Hugging;0.01349965819897773
Face;0.012925563567628836
Transformer's;0.021780046414329887
library,;0.018100012296028374
which;0.011850258758191767
has;0.01092559774994761
built;0.010071061714433048
in;0.011019843990292688
methods;0.011614694622427476
for;0.01183880726154768
pre-training,;0.02153231608622013
inference,;0.012930282406767834
and;0.010189222687174537
deploying;0.010888746976270606
BERT.;0.01934975732478519
â€˜**;0.017759823865831362
from;0.014999559087112653
transformers;0.014478329576924742
import;0.013446170235662622
AutoTokenizer,;0.02683557661077224
BertModel;0.016835261733103533
import;0.01435154382139975
torch;0.01080315540074218
tokenizer;0.01278295065768745
=;0.014467075572191552
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0863308106292309
model;0.012452344014485812
=;0.013351867170241585
"BertModel.from_pretrained(""bert-base-uncased"")";0.04313353261333308
inputs;0.013784471792758015
=;0.012906637727760028
"tokenizer(""Hello,";0.028560703868823388
my;0.010685569377031254
dog;0.010669064047156715
is;0.01009473753692246
"cute"",";0.011507527965701607
"return_tensors=""pt"")";0.018970230412071097
outputs;0.011290603527871396
=;0.010986488494710797
model(**inputs);0.015450317358793726
last_hidden_states;0.013962640782937704
=;0.01083235704896131
outputs.last_hidden_state;0.011034137706288706
***;0.009715487910495996
