text;attention
The;0.004588652189531629
easiest;0.0038738071091130805
way;0.003495744670020874
to;0.0033466760075620654
import;0.004823894473506571
the;0.004162605043938603
BERT;0.013738386466505943
language;0.003701028466160091
model;0.004495596295208033
into;0.004192424573842719
python;0.005173350177090618
for;0.004005033603443359
use;0.003404990587361981
with;0.0036580931089200855
PyTorch;0.009247492712814793
is;0.004177665990088553
using;0.004028179664330401
the;0.004567579031036542
Hugging;0.005162858235590232
Face;0.005348578240301374
Transformer's;0.0070291190736610325
library,;0.004599890848161207
which;0.0034652826195712655
has;0.0034795850674058664
built;0.003291174053913103
in;0.0034172151451119953
methods;0.003549541927795936
for;0.0035890088807966083
pre-training,;0.0064175358367614494
inference,;0.004525527169301787
and;0.0030005580781689942
deploying;0.0032099184365465645
BERT.;0.009272468161110152
â€˜**;0.017947299818986163
from;0.004258571044780705
transformers;0.0038976614808286236
import;0.0047231243197444804
AutoTokenizer,;0.01508568419253912
BertModel;0.00942172579354283
import;0.0048109754109503435
torch;0.005152345192047189
tokenizer;0.005276184727150096
=;0.0036007534189182676
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5683740966144087
model;0.004003280479738022
=;0.003435981697061687
"BertModel.from_pretrained(""bert-base-uncased"")";0.12517378545593244
inputs;0.004333104082581579
=;0.003430741167138037
"tokenizer(""Hello,";0.007957678351109236
my;0.0032661717579528433
dog;0.003298348268245761
is;0.003027442464949476
"cute"",";0.004138356950502329
"return_tensors=""pt"")";0.008395280598664194
outputs;0.003532056419106866
=;0.0029919144815081125
model(**inputs);0.006385273842885417
last_hidden_states;0.0058448519213553955
=;0.002996254214237467
outputs.last_hidden_state;0.00415644805439275
***;0.0030451458340684597
