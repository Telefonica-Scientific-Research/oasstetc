text;attention
The;0.015521838222101148
easiest;0.0050233486422501614
way;0.004014836757496378
to;0.012720384034463596
import;0.00920276744400736
the;0.011266996094958492
BERT;0.010326490956813388
language;0.004064785308284793
model;0.004155455199140262
into;0.005613847466836384
python;0.00932569333237142
for;0.013292084455341822
use;0.004032598465918312
with;0.013355948570852027
PyTorch;0.011423145084486183
is;0.006924062718267146
using;0.0047663481768387755
the;0.009453316923976428
Hugging;0.010324816452398874
Face;0.0036483376624422232
Transformer's;0.011275266433446264
library,;0.031003666343983008
which;0.0038637751952888245
has;0.005261055527507797
built;0.004008507642496806
in;0.009314687240032126
methods;0.0038537244566375175
for;0.01020619935775365
pre-training,;0.02939081982939615
inference,;0.024521414449330536
and;0.010861688244078405
deploying;0.004189840168292568
BERT.;0.02787034754140657
â€˜**;0.00529467837837999
from;0.006599223520830489
transformers;0.0044800464398430185
import;0.00546046708890256
AutoTokenizer,;0.0254422204793934
BertModel;0.004363762921133567
import;0.005367783060943908
torch;0.005014882342790295
tokenizer;0.004743965512287471
=;0.007709870115177956
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2920783564484994
model;0.0034696070117218314
=;0.007161678266980037
"BertModel.from_pretrained(""bert-base-uncased"")";0.17269906385864542
inputs;0.00355442995833121
=;0.006177798037596961
"tokenizer(""Hello,";0.03351741495537683
my;0.004449732563733626
dog;0.0033228634927077775
is;0.004487726568239864
"cute"",";0.005896759114758007
"return_tensors=""pt"")";0.01343667760042925
outputs;0.0031688803105159626
=;0.005115977958612592
model(**inputs);0.007483151651734197
last_hidden_states;0.004329226505687565
=;0.004130639659840339
outputs.last_hidden_state;0.004064827995231301
***;0.002900193782779827
