text;attention
The;0.004445689749708813
easiest;0.004062247997590843
way;0.003839199056669089
to;0.003548359379103624
import;0.004900685363150321
the;0.0041169079298229065
BERT;0.007099977445640676
language;0.003922427565084903
model;0.004756910924748667
into;0.0042534854527343825
python;0.004567057745026776
for;0.004246014228708414
use;0.00400756007872635
with;0.0039057985978276913
PyTorch;0.00835063112117356
is;0.004146763295720107
using;0.004775987246057644
the;0.004699888587355592
Hugging;0.004790031032818461
Face;0.004597795574103942
Transformer's;0.007325062826511766
library,;0.005622618485028224
which;0.0039032470878117313
has;0.004090016837722589
built;0.003802135117962857
in;0.003990522505798719
methods;0.004072633445166938
for;0.004255275353015261
pre-training,;0.007658920944729926
inference,;0.005349926982709806
and;0.003518516847174369
deploying;0.004066263629923169
BERT.;0.009176832147831765
â€˜**;0.00987158129679933
from;0.004552602897890966
transformers;0.00447515404938174
import;0.004935486302570262
AutoTokenizer,;0.010252956507363886
BertModel;0.008249429656569239
import;0.004450787380293661
torch;0.004853076519834432
tokenizer;0.004904768919391795
=;0.004124340650410684
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.47897708914468223
model;0.004375556727381938
=;0.004046715059689967
"BertModel.from_pretrained(""bert-base-uncased"")";0.20586850759618494
inputs;0.004655713027307673
=;0.003931938983640615
"tokenizer(""Hello,";0.011727796315297575
my;0.003858935294634765
dog;0.0038204354683159035
is;0.0035224504020049128
"cute"",";0.005346119151491018
"return_tensors=""pt"")";0.012294906212607525
outputs;0.0038160974441113315
=;0.0034327839262968942
model(**inputs);0.009508575667583029
last_hidden_states;0.006789970999171254
=;0.0034539437425293185
outputs.last_hidden_state;0.006460414124897716
***;0.003576475948505895
