text;attention
The;0.015671415777329445
easiest;0.014007233474845837
way;0.01383239447454985
to;0.013280767902891673
import;0.016314517237236918
the;0.014426339122965823
BERT;0.019334230450049106
language;0.013663909116932322
model;0.014902973864757472
into;0.013972261867259191
python;0.015654603957128907
for;0.013822227901652604
use;0.01330560385382942
with;0.01361732975399345
PyTorch;0.021310884928416013
is;0.015462791821213967
using;0.013746427385446874
the;0.014390733930327545
Hugging;0.015208980857412784
Face;0.015289590107209382
Transformer's;0.016571406214280854
library,;0.015642124694812482
which;0.013438185825007444
has;0.01337632461911969
built;0.013457250404008744
in;0.013427871382835946
methods;0.01444393862988885
for;0.013497401978786482
pre-training,;0.015477446438779852
inference,;0.014488540624453287
and;0.012935430785830997
deploying;0.013612672456228414
BERT.;0.020698372248718462
â€˜**;0.01839969604131124
from;0.015141880954242359
transformers;0.013826054581808257
import;0.014210030865323542
AutoTokenizer,;0.020147549561378875
BertModel;0.017005067927520278
import;0.014771187784227683
torch;0.015841523248388424
tokenizer;0.015044930323133567
=;0.014121583177570698
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.05551082472841455
model;0.014008627820854292
=;0.013858436089310738
"BertModel.from_pretrained(""bert-base-uncased"")";0.03900117875049333
inputs;0.015167671828669666
=;0.013759602713074163
"tokenizer(""Hello,";0.021679372175853674
my;0.013114786403221465
dog;0.014114977915317545
is;0.012833792068166903
"cute"",";0.014697415503606518
"return_tensors=""pt"")";0.020740957871319736
outputs;0.013850510973555203
=;0.013048294364259968
model(**inputs);0.01608050931843418
last_hidden_states;0.016079025011717634
=;0.013042060720587051
outputs.last_hidden_state;0.0156896023543805
***;0.012930664835657826
