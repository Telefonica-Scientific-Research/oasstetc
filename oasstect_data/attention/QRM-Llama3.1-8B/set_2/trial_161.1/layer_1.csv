text;attention
The;0.0144682225086465
easiest;0.015099830742331432
way;0.01307014539023775
to;0.012568887208756374
import;0.015835364788761856
the;0.01244141226217059
BERT;0.014667695267489468
language;0.013287354535445509
model;0.012134899487532373
into;0.012296514158619905
python;0.012336100592107917
for;0.012702264630294387
use;0.012076056532154511
with;0.011861896163003127
PyTorch;0.019097135823680007
is;0.012383347858172671
using;0.012078604854265602
the;0.012274283641903298
Hugging;0.014664831462277046
Face;0.012828671834552
Transformer's;0.014316546927309964
library,;0.015735555619822785
which;0.0115998418983577
has;0.011583716601148823
built;0.011627162025684402
in;0.011512758234730497
methods;0.012065979193690068
for;0.011545075804459126
pre-training,;0.019049872206556517
inference,;0.013571079868067215
and;0.011042006000480591
deploying;0.011586653601439465
BERT.;0.01764779033000303
â€˜**;0.01590683454401335
from;0.012029839010792555
transformers;0.013671669210416187
import;0.013630602853428838
AutoTokenizer,;0.017934645651977818
BertModel;0.01334642241694212
import;0.011457518016211208
torch;0.012044611809812965
tokenizer;0.01256488693482295
=;0.012128808149049583
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.10348639229417551
model;0.010806558741674143
=;0.011682531456381623
"BertModel.from_pretrained(""bert-base-uncased"")";0.08221246296017863
inputs;0.011211199768389083
=;0.011342052393834628
"tokenizer(""Hello,";0.024341678676828248
my;0.01125813750386156
dog;0.011131793903128429
is;0.010872517541650178
"cute"",";0.013024360294701448
"return_tensors=""pt"")";0.03201792015422708
outputs;0.011534882070484863
=;0.011110304782052806
model(**inputs);0.018030955385876013
last_hidden_states;0.014738400988303585
=;0.010715551703200847
outputs.last_hidden_state;0.014810948398223793
***;0.00989795433120744
