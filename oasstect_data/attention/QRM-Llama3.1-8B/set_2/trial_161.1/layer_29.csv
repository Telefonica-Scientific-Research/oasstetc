text;attention
The;0.012939464221653258
easiest;0.01145684358048884
way;0.011515583824313898
to;0.010561193873516197
import;0.012467750599811794
the;0.011754695883339761
BERT;0.021699290161597554
language;0.011101054623354796
model;0.013642921579682134
into;0.011609105676524052
python;0.013423551146772955
for;0.011251661942177714
use;0.010740080071078458
with;0.011004340113271624
PyTorch;0.022205912753887547
is;0.012126928593169238
using;0.011421608967714561
the;0.012196665918812737
Hugging;0.0129794209268999
Face;0.013893585640282867
Transformer's;0.0148688297003175
library,;0.011967168854270727
which;0.010941032001737676
has;0.010896603388566618
built;0.010357854551989471
in;0.01089728585963691
methods;0.010678149840483672
for;0.011460435756618849
pre-training,;0.0157365342644053
inference,;0.012961609603995464
and;0.010328904870649976
deploying;0.011004430957151598
BERT.;0.019413624413981496
â€˜**;0.02092146379320923
from;0.01141912932213217
transformers;0.012444490250093983
import;0.011937611182434792
AutoTokenizer,;0.019673926159274786
BertModel;0.017327551267574376
import;0.011370702474935174
torch;0.013004694780826003
tokenizer;0.011973792596551859
=;0.010607385599077496
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1543988997693423
model;0.010766127012428219
=;0.010746496056397794
"BertModel.from_pretrained(""bert-base-uncased"")";0.06983651278681383
inputs;0.014121513664172872
=;0.010714281184998146
"tokenizer(""Hello,";0.018155467626428192
my;0.010356190779486813
dog;0.01197256974492271
is;0.010437786335168255
"cute"",";0.012011490144489738
"return_tensors=""pt"")";0.022388634519548655
outputs;0.01123844111841976
=;0.010064718686949343
model(**inputs);0.013729838921174744
last_hidden_states;0.014291889821635366
=;0.01013993347008899
outputs.last_hidden_state;0.012373441901326526
***;0.010070894867942645
