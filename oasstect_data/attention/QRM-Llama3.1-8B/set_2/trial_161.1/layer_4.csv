text;attention
The;0.014096515532760636
easiest;0.013637359644228566
way;0.012710241269844874
to;0.012181680582090529
import;0.018093713002630085
the;0.013200310770502882
BERT;0.019600135009169857
language;0.014557413908458099
model;0.015127452867517639
into;0.01675211590475568
python;0.017743719300541406
for;0.013800035503355896
use;0.012189231116612631
with;0.012347774801424648
PyTorch;0.017893546289957308
is;0.015518303888628534
using;0.013074596055243703
the;0.011730302809136683
Hugging;0.014350570010898508
Face;0.014588275148258915
Transformer's;0.01934505293318505
library,;0.0163824257591625
which;0.01218460378690761
has;0.011634875807254785
built;0.011221676412111374
in;0.012525858833645941
methods;0.012520746791330912
for;0.011861579971632829
pre-training,;0.017954345921690477
inference,;0.013703892036743857
and;0.011114676434379675
deploying;0.011944226312913978
BERT.;0.01685695173874356
â€˜**;0.018096136485515394
from;0.013957576956759055
transformers;0.015437789188838737
import;0.013335575315715676
AutoTokenizer,;0.02150955701178768
BertModel;0.01496882306486389
import;0.013495197040990116
torch;0.011696207276280499
tokenizer;0.013563666938825268
=;0.013294999441977623
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08767575110998942
model;0.01399528874128744
=;0.012779825294837004
"BertModel.from_pretrained(""bert-base-uncased"")";0.0499745509454902
inputs;0.015042252691007955
=;0.012374315109458292
"tokenizer(""Hello,";0.024384211119028335
my;0.011369377478281973
dog;0.011617899368533963
is;0.01097308964436495
"cute"",";0.012993252553291285
"return_tensors=""pt"")";0.023146743386114745
outputs;0.012881106291607047
=;0.011346101765135855
model(**inputs);0.016060484304765203
last_hidden_states;0.016753550816336026
=;0.011180245554750662
outputs.last_hidden_state;0.013118401724793971
***;0.010533817223652035
