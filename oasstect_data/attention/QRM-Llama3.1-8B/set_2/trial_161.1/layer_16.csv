text;attention
The;0.009154709041265986
easiest;0.007067137314224902
way;0.006933861715162349
to;0.006127399508039774
import;0.009195298423999186
the;0.0070698223497771455
BERT;0.041157047102788946
language;0.007376348383103282
model;0.011625091000381865
into;0.007797115307333263
python;0.012227524747700216
for;0.007503631011934229
use;0.006450249962473446
with;0.006164751698555435
PyTorch;0.028463183754328124
is;0.008619093932908356
using;0.007852028860707596
the;0.007244925124969609
Hugging;0.010950768968765503
Face;0.012188879190716132
Transformer's;0.018380627773058728
library,;0.01027657757035428
which;0.006764078596477936
has;0.006500195552209527
built;0.00627989776767083
in;0.006566074760444497
methods;0.006544924119190535
for;0.006331999527328701
pre-training,;0.011317151764358114
inference,;0.008067135088000226
and;0.005692805896684551
deploying;0.005808211744507659
BERT.;0.011711289235444322
â€˜**;0.01437527148203926
from;0.009102257944038525
transformers;0.008632364230520042
import;0.007358462123786606
AutoTokenizer,;0.02675330770149429
BertModel;0.019192840942314507
import;0.009345667085401391
torch;0.007915794944502345
tokenizer;0.00889673851332859
=;0.0073289279943539
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3526858991168932
model;0.007754600644873203
=;0.006972521434928465
"BertModel.from_pretrained(""bert-base-uncased"")";0.04814079246051636
inputs;0.008915075085373267
=;0.006377208882958176
"tokenizer(""Hello,";0.015378615668675984
my;0.005765117076724979
dog;0.006308951512481586
is;0.005451786160811008
"cute"",";0.006824165440898595
"return_tensors=""pt"")";0.0205050005301524
outputs;0.008007634590218625
=;0.0061013677402542875
model(**inputs);0.011406750886044238
last_hidden_states;0.013526588573248637
=;0.005878675602917916
outputs.last_hidden_state;0.008114530239391432
***;0.005573248595992828
