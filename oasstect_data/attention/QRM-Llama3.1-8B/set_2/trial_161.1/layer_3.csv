text;attention
The;0.014531257343629275
easiest;0.017426818266092395
way;0.015559997525592603
to;0.014206715824847527
import;0.020856240025174586
the;0.014629544354724178
BERT;0.020025735131435463
language;0.015679721897343346
model;0.015834549398304913
into;0.0154133276828987
python;0.016720216217099747
for;0.014376931011946668
use;0.013498668118301756
with;0.013476381877073517
PyTorch;0.018342102421104536
is;0.014761516645889573
using;0.014002388873880516
the;0.013030300993618226
Hugging;0.015016120761373986
Face;0.01676710371897928
Transformer's;0.0190413761306239
library,;0.017807616632609224
which;0.013591809445610673
has;0.013308163044193152
built;0.01347454786959828
in;0.01374503587640348
methods;0.014947431144876767
for;0.013534816578440554
pre-training,;0.017287191822188113
inference,;0.014222975638717179
and;0.012675537395208823
deploying;0.013382339525050793
BERT.;0.016221148691636572
â€˜**;0.01687804922188455
from;0.015547115505583579
transformers;0.01927717412175126
import;0.015549400867148845
AutoTokenizer,;0.019058223182097135
BertModel;0.015191085678096
import;0.014398321360253442
torch;0.013807571927622594
tokenizer;0.014261664563584903
=;0.015165415776679216
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04739841319444359
model;0.013457281772725812
=;0.014234628303954316
"BertModel.from_pretrained(""bert-base-uncased"")";0.03500465445056728
inputs;0.013925831942908536
=;0.013608752628388428
"tokenizer(""Hello,";0.023156144439020105
my;0.012933971522135471
dog;0.014394949002407903
is;0.012608782387585017
"cute"",";0.013594229635491272
"return_tensors=""pt"")";0.021338534965083193
outputs;0.013462862953450986
=;0.013118868389473433
model(**inputs);0.01622599100512802
last_hidden_states;0.015726034673300263
=;0.012847022431619557
outputs.last_hidden_state;0.014063899059013939
***;0.012369497154133123
