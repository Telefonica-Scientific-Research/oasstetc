text;attention
The;0.01548169585919869
easiest;0.013386394175127025
way;0.013566226686127164
to;0.012846719853908251
import;0.016629911006318644
the;0.013528317729114454
BERT;0.02790610132508243
language;0.01293729876203743
model;0.014750858503100182
into;0.013812961420139429
python;0.015231976806908382
for;0.013379991432562873
use;0.012853450513347226
with;0.013122056653098797
PyTorch;0.021417663503653407
is;0.01397764241669669
using;0.013213664527222809
the;0.013580421171471306
Hugging;0.016701605788169583
Face;0.014970259069813452
Transformer's;0.017180467320687006
library,;0.01470119823286161
which;0.012899488795825883
has;0.012717754353992923
built;0.012906228062518708
in;0.013382421465688563
methods;0.013226163377567332
for;0.012608480536082286
pre-training,;0.01486679028409134
inference,;0.013969868121586114
and;0.012390591527948266
deploying;0.012811621721417234
BERT.;0.01834406991759111
â€˜**;0.018168751869148805
from;0.014547628934334375
transformers;0.013901020477856362
import;0.014601109527331116
AutoTokenizer,;0.020114772309802544
BertModel;0.020555391727265688
import;0.014526026296601765
torch;0.015022511723739728
tokenizer;0.013903258820299228
=;0.013351080290641429
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07357401820482203
model;0.013595285654514544
=;0.013357782692840218
"BertModel.from_pretrained(""bert-base-uncased"")";0.03505505019999285
inputs;0.01572396372525493
=;0.013581224175741402
"tokenizer(""Hello,";0.020947693889406916
my;0.012317263884655812
dog;0.012531341593720867
is;0.01219903344914995
"cute"",";0.013619381996244658
"return_tensors=""pt"")";0.019205104342646648
outputs;0.014925377469331145
=;0.012638786784291353
model(**inputs);0.017359199754815945
last_hidden_states;0.01570909178926992
=;0.01255863730443295
outputs.last_hidden_state;0.014641430716906184
***;0.01246841947398394
