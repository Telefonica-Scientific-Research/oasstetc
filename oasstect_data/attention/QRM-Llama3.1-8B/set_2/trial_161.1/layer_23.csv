text;attention
The;0.01678687161852498
easiest;0.015137477967995404
way;0.014272911735398578
to;0.014243916490087912
import;0.01672481783614891
the;0.014677163318557182
BERT;0.025162125297177543
language;0.014369767921850369
model;0.016377975356892498
into;0.015288919636265334
python;0.015627443479097534
for;0.014687317010607177
use;0.013791790486494579
with;0.014439177443183227
PyTorch;0.01840676529765806
is;0.014637354157231258
using;0.014609466799721506
the;0.01460415549834511
Hugging;0.01470901649811116
Face;0.016443660245276614
Transformer's;0.016506096007517984
library,;0.01619044342160959
which;0.01389178282275982
has;0.013964575176051448
built;0.01332445003451797
in;0.01391799742643135
methods;0.01393679069609318
for;0.014377405477987111
pre-training,;0.016103942138534888
inference,;0.0159603549917429
and;0.013259141798794359
deploying;0.014131357911630135
BERT.;0.024313883345240414
â€˜**;0.0172919061917058
from;0.01476916119513705
transformers;0.014159808690687264
import;0.01449292060089464
AutoTokenizer,;0.02207662879409706
BertModel;0.018576311185007764
import;0.01432241341359912
torch;0.016266734538165677
tokenizer;0.01568917051217207
=;0.013971705167333405
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04337039000681826
model;0.014068077241905175
=;0.013891744837633701
"BertModel.from_pretrained(""bert-base-uncased"")";0.027868241973888597
inputs;0.015768039468238446
=;0.013982827705952069
"tokenizer(""Hello,";0.019184929077754934
my;0.01345819916126858
dog;0.014417760987926975
is;0.013225648260964007
"cute"",";0.014892790057724528
"return_tensors=""pt"")";0.019730986396002795
outputs;0.014104697936056388
=;0.013425390507868739
model(**inputs);0.015886220774841926
last_hidden_states;0.01565546443017708
=;0.01331251053306175
outputs.last_hidden_state;0.014140014546456204
***;0.013122990463123776
