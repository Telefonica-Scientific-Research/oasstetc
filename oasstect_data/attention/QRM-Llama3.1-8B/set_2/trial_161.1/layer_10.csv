text;attention
The;0.011149785285840718
easiest;0.007731392546984704
way;0.007764684803261839
to;0.00707043269493455
import;0.010130663280128224
the;0.009128574817159648
BERT;0.057665167113661826
language;0.008192110378415433
model;0.01893381119545084
into;0.016146501927335556
python;0.018389653199158952
for;0.013241744957648285
use;0.00812409068291876
with;0.007492760326335156
PyTorch;0.03324120875226263
is;0.014212372530553042
using;0.00956569523384601
the;0.008762722471791511
Hugging;0.01104713995576523
Face;0.012398910399249166
Transformer's;0.019107590313093233
library,;0.013094612411311003
which;0.007461105483048605
has;0.006730576207891048
built;0.007533479428375921
in;0.007862433026302988
methods;0.007520894537802382
for;0.007499617371520457
pre-training,;0.015119561004237213
inference,;0.011332257754519985
and;0.007020612414371388
deploying;0.0068496487639350175
BERT.;0.01738540071872385
â€˜**;0.012583308992099496
from;0.010683647353439383
transformers;0.009332933127642377
import;0.009386824813296446
AutoTokenizer,;0.05363908201920976
BertModel;0.026500971738466474
import;0.01043248281112965
torch;0.008951834249671104
tokenizer;0.009756828590732656
=;0.009857074551023878
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2073445620038759
model;0.009186633807036212
=;0.008160178203402165
"BertModel.from_pretrained(""bert-base-uncased"")";0.03707242898060082
inputs;0.009976451619911364
=;0.008199728767338597
"tokenizer(""Hello,";0.026248636285134856
my;0.0070447594022815425
dog;0.007685929990337449
is;0.006600942636660147
"cute"",";0.007920478101521118
"return_tensors=""pt"")";0.01652683036663111
outputs;0.008256219675701533
=;0.0069473780319176505
model(**inputs);0.013729912016488925
last_hidden_states;0.011737997321169782
=;0.00711278236682115
outputs.last_hidden_state;0.007848690188037287
***;0.006367260000586025
