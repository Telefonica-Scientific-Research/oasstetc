text;attention
The;0.015297250194132667
easiest;0.013274315520166065
way;0.012895830609465128
to;0.012360364887907649
import;0.017251281251426526
the;0.012917706706929223
BERT;0.0256924387038372
language;0.012673821289287722
model;0.015745725183640733
into;0.014423160966698302
python;0.016343479277810474
for;0.013572302384581876
use;0.012980014796319194
with;0.012672442158791796
PyTorch;0.019007363075790658
is;0.013888871624614184
using;0.013368747942286471
the;0.012886621804100055
Hugging;0.013196032834945033
Face;0.013900518436861001
Transformer's;0.01791444702755845
library,;0.014881515032866192
which;0.012653499275282113
has;0.01271260743082961
built;0.011609146368007986
in;0.012929629381637919
methods;0.013055243052296009
for;0.012680041799105535
pre-training,;0.015567530853791459
inference,;0.01373355693188975
and;0.011714163889895306
deploying;0.011920543565884642
BERT.;0.016663181220562883
â€˜**;0.017051723633275503
from;0.014871598269305081
transformers;0.013627440781670937
import;0.014457316318987196
AutoTokenizer,;0.028190388313646085
BertModel;0.02104177433368658
import;0.013892873571792964
torch;0.01447748521839076
tokenizer;0.015185665706216597
=;0.013183928155721829
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09125051645262278
model;0.014043668504379818
=;0.013129091264534331
"BertModel.from_pretrained(""bert-base-uncased"")";0.029907692054662354
inputs;0.015361314840410572
=;0.012666916189769995
"tokenizer(""Hello,";0.019498724949726143
my;0.011739547596543626
dog;0.012197038041819173
is;0.011504628324713837
"cute"",";0.01263138607623873
"return_tensors=""pt"")";0.018930014578833424
outputs;0.014199890822531909
=;0.012619947858799373
model(**inputs);0.017008938823863836
last_hidden_states;0.01739026612983442
=;0.012492056126796235
outputs.last_hidden_state;0.013562300487939527
***;0.011502471094086582
