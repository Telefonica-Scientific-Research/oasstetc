text;attention
The;0.01744338295785141
easiest;0.016767809709847
way;0.015560131219465857
to;0.014632728682668067
import;0.019517284146526544
the;0.015038149756791484
BERT;0.01909724010904008
language;0.016166909917183704
model;0.015764552763359314
into;0.015657993771646835
python;0.015996751782835425
for;0.01485860153283894
use;0.014717486884129402
with;0.014374074522097364
PyTorch;0.01744249416127766
is;0.01566288987191738
using;0.015373093334926953
the;0.014160809049493963
Hugging;0.015740941321764044
Face;0.015042094409874439
Transformer's;0.01564416958246621
library,;0.01593691221655357
which;0.014450556583755481
has;0.014215210170155973
built;0.014698902983603043
in;0.014719120926439127
methods;0.014608311614230613
for;0.014287263012472542
pre-training,;0.01842356984631158
inference,;0.014943703009545097
and;0.013678707378067484
deploying;0.01417708872981902
BERT.;0.01662943766165707
â€˜**;0.017704995481589728
from;0.015353309475692292
transformers;0.015808867068463548
import;0.015642297654080987
AutoTokenizer,;0.018903667596200654
BertModel;0.015292097277715442
import;0.015014848777059784
torch;0.014369802058674622
tokenizer;0.014887996899340759
=;0.01617240663384228
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.040165228520204176
model;0.015673183649441765
=;0.015645269347231188
"BertModel.from_pretrained(""bert-base-uncased"")";0.02435460859634166
inputs;0.01498727997853091
=;0.016230707814945244
"tokenizer(""Hello,";0.022076951159388417
my;0.014110727171874572
dog;0.013791234824348476
is;0.013562370456720792
"cute"",";0.014464580241310357
"return_tensors=""pt"")";0.01860400392620032
outputs;0.014190089832772576
=;0.014684681194315173
model(**inputs);0.01608313848402294
last_hidden_states;0.015408010293767474
=;0.01378620276090865
outputs.last_hidden_state;0.014258213133622449
***;0.01334485607078016
