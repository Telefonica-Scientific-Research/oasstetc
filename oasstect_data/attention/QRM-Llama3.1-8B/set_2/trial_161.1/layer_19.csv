text;attention
The;0.01430962676847068
easiest;0.012949005679091767
way;0.01259548251603365
to;0.01202316435778317
import;0.015259651480652808
the;0.012797125903882586
BERT;0.02946837705486733
language;0.012810788267053903
model;0.016095368848219098
into;0.013307386303932632
python;0.015501011582003486
for;0.0129940011410405
use;0.01263296095117785
with;0.012314788730844196
PyTorch;0.022421138943801353
is;0.013593656786188036
using;0.012951348560353798
the;0.012554846524495143
Hugging;0.014925943862399909
Face;0.014493834760918436
Transformer's;0.01807006382634031
library,;0.01432503133029252
which;0.012297513069915448
has;0.012359576206450932
built;0.011623207930199388
in;0.012311000401353953
methods;0.01279323790262893
for;0.012306753941133107
pre-training,;0.01595623815152377
inference,;0.013740665782694563
and;0.01177253872930633
deploying;0.012058897606438754
BERT.;0.01678855450739055
â€˜**;0.015952854974392156
from;0.014261793248555331
transformers;0.01525716271070881
import;0.013896335133929622
AutoTokenizer,;0.026903734006773674
BertModel;0.020385098228057553
import;0.014652209520349227
torch;0.014103549231548662
tokenizer;0.016696769541126286
=;0.013746832961888326
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0861878909931497
model;0.013434101748753135
=;0.012819780123362831
"BertModel.from_pretrained(""bert-base-uncased"")";0.029302828787990485
inputs;0.01820464650008286
=;0.012604536426267838
"tokenizer(""Hello,";0.02125743759447153
my;0.011771793944307319
dog;0.012528872895012259
is;0.011613622662432089
"cute"",";0.013356787919622365
"return_tensors=""pt"")";0.023279487651446537
outputs;0.01330028604687645
=;0.012052874771184684
model(**inputs);0.016033735451817924
last_hidden_states;0.016926058668294584
=;0.012066310274600601
outputs.last_hidden_state;0.013409260750286919
***;0.011590558823831417
