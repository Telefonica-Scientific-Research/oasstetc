text;attention
The;0.012764879200755311
easiest;0.013082623777644518
way;0.012688537231330227
to;0.011613836738588972
import;0.014792698278936148
the;0.011443668453565507
BERT;0.02050805238316649
language;0.01157809963771258
model;0.014961693270891956
into;0.012502198476975247
python;0.01291113774218032
for;0.012462343104313905
use;0.011096957517913996
with;0.01147282514123783
PyTorch;0.014601023616500923
is;0.012464904402841932
using;0.01234352058401187
the;0.011399099425628481
Hugging;0.015721603543416646
Face;0.012342431933838487
Transformer's;0.024771225894558914
library,;0.013949889026791057
which;0.012025856528233786
has;0.01147936804620905
built;0.011127228654634672
in;0.011588281314741228
methods;0.012319190971873955
for;0.012178508148251058
pre-training,;0.018718611808527266
inference,;0.014180918180259655
and;0.010710035647893386
deploying;0.011667960102678291
BERT.;0.01822023318101822
â€˜**;0.02230957644940147
from;0.01594973811276313
transformers;0.014537434284985629
import;0.013461867862811902
AutoTokenizer,;0.024102765875077498
BertModel;0.015397359560529454
import;0.014559831880353032
torch;0.012721008671621696
tokenizer;0.01488373090854498
=;0.011867135822019618
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.10282510239070405
model;0.012789211775867596
=;0.010815729264093693
"BertModel.from_pretrained(""bert-base-uncased"")";0.06447978513703481
inputs;0.011854994241850962
=;0.01110607312246115
"tokenizer(""Hello,";0.020965146669380343
my;0.010841037537923799
dog;0.010803670109130466
is;0.010580525325688936
"cute"",";0.012917085097986659
"return_tensors=""pt"")";0.0197137562485295
outputs;0.01153161879936522
=;0.01060631609600808
model(**inputs);0.018320472189846265
last_hidden_states;0.01636430899539582
=;0.010579266192732147
outputs.last_hidden_state;0.016315927728998973
***;0.011110081681771087
