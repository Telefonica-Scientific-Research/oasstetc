text;attention
The;0.008470428204694855
easiest;0.00735899869294882
way;0.007186631729516833
to;0.006143600226994777
import;0.009859431255599288
the;0.006845062762177617
BERT;0.013744659526818581
language;0.006244666427485082
model;0.008193583306703913
into;0.007488128024053063
python;0.009180402563788095
for;0.00688104692661743
use;0.006388654914567957
with;0.006741854059068527
PyTorch;0.011132773421637298
is;0.00861280901076655
using;0.0073282460463450156
the;0.006816073875658465
Hugging;0.00954092668205167
Face;0.007564064295220013
Transformer's;0.019129265631001955
library,;0.008802954789002598
which;0.007539288413181692
has;0.006630934606856551
built;0.006140662369653218
in;0.006434176353830573
methods;0.0072882106215197365
for;0.0069706803141725325
pre-training,;0.012412562296346023
inference,;0.008756310201966674
and;0.005840893438060169
deploying;0.006660804438013899
BERT.;0.011286751411363729
â€˜**;0.01624970510454643
from;0.010330141903426848
transformers;0.008641218707268484
import;0.007770185061994313
AutoTokenizer,;0.014502553041658704
BertModel;0.010905344498754419
import;0.008552544062448358
torch;0.007134552839927584
tokenizer;0.009196725372901041
=;0.007729688018829046
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.273822794433847
model;0.006604317451445678
=;0.006316556134398488
"BertModel.from_pretrained(""bert-base-uncased"")";0.19126080665844228
inputs;0.007613222765717797
=;0.00710125505597941
"tokenizer(""Hello,";0.023636583731709456
my;0.0061104853222347195
dog;0.00628923462144116
is;0.005947472356802579
"cute"",";0.007718599295940031
"return_tensors=""pt"")";0.019143081776780025
outputs;0.006038355753250397
=;0.006135391772489817
model(**inputs);0.013321825077758743
last_hidden_states;0.01188515552171553
=;0.006012507501666984
outputs.last_hidden_state;0.01286271910033959
***;0.005551440218601772
