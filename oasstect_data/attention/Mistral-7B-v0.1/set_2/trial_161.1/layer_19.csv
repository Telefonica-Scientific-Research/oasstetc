text;attention
The;0.005108845487815071
easiest;0.0035570004114749924
way;0.003445733398719294
to;0.0031711843503886223
import;0.0066099381095214085
the;0.003707748514358665
BERT;0.035760415925119395
language;0.0037865065321672836
model;0.008068835941479724
into;0.004733433753360364
python;0.007565073288344409
for;0.004144782614364342
use;0.00345828726942045
with;0.0037629805190098303
PyTorch;0.014246706388291916
is;0.004607201679020268
using;0.004406286777615157
the;0.0039317253756230115
Hugging;0.011434771533758839
Face;0.006832396915455186
Transformer's;0.035105726333931615
library,;0.005296739841811888
which;0.003635595217313075
has;0.0034348748633362533
built;0.003172062551911937
in;0.0034025861220600313
methods;0.0035468885053866065
for;0.0036347183448080284
pre-training,;0.008299833862030861
inference,;0.004667093148091729
and;0.003038368949600766
deploying;0.00445693203901959
BERT.;0.01039960217363467
â€˜**;0.006200154928277565
from;0.00873518794602637
transformers;0.00839327620977156
import;0.0058133668669288424
AutoTokenizer,;0.04995095111494253
BertModel;0.009961072083298913
import;0.004810901623384008
torch;0.004805381660715837
tokenizer;0.008449839052016203
=;0.005405882163723009
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4693327391641301
model;0.004868777770846169
=;0.004381887099000504
"BertModel.from_pretrained(""bert-base-uncased"")";0.08008259295866864
inputs;0.0051870997052946045
=;0.0041301545717007445
"tokenizer(""Hello,";0.013363615894109646
my;0.0030326814857152508
dog;0.0035149694473675668
is;0.0029664099719818172
"cute"",";0.003922848045494911
"return_tensors=""pt"")";0.015663197403635254
outputs;0.0035863710592843666
=;0.003422136264760053
model(**inputs);0.014582542596165782
last_hidden_states;0.007437402963073916
=;0.003815245605289076
outputs.last_hidden_state;0.006847796662456819
***;0.0029046409436947057
