text;attention
The;0.011602272532730155
easiest;0.011479068981153894
way;0.010802255990121159
to;0.009803620794135139
import;0.013703149657229867
the;0.010269447113938018
BERT;0.02282753135073198
language;0.011314057331096663
model;0.012006828209396556
into;0.01099393803251371
python;0.014705468377977877
for;0.011242870952018043
use;0.010118845388213199
with;0.010428406721143825
PyTorch;0.016977422350154633
is;0.010995761691360515
using;0.011274113903139165
the;0.01050910180008248
Hugging;0.014691661656784183
Face;0.012296204048858103
Transformer's;0.0232614787812833
library,;0.012022999648937212
which;0.01060297001148143
has;0.01093601251133665
built;0.009803956942250886
in;0.010921963208939461
methods;0.010744274241609598
for;0.011440126730013331
pre-training,;0.016070168470486347
inference,;0.011963598069753448
and;0.009940679854754886
deploying;0.01065190605947173
BERT.;0.013911798493966565
â€˜**;0.016065562019775156
from;0.014545895859169928
transformers;0.011497758808097258
import;0.012189114402996146
AutoTokenizer,;0.02253773918852138
BertModel;0.013336016555019975
import;0.012050487677171184
torch;0.013283985358153318
tokenizer;0.01565342075629467
=;0.012920804700618455
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.12948857257933022
model;0.011371636270840995
=;0.01112102750407523
"BertModel.from_pretrained(""bert-base-uncased"")";0.08275163037905787
inputs;0.01169320860205264
=;0.011316597126336389
"tokenizer(""Hello,";0.034267821764340796
my;0.009590752694081122
dog;0.010345831026578713
is;0.009324919676584395
"cute"",";0.01347872470942486
"return_tensors=""pt"")";0.021305147164431715
outputs;0.010634938470116895
=;0.010157204616547693
model(**inputs);0.018930283325491203
last_hidden_states;0.01465497379759305
=;0.010130955265237704
outputs.last_hidden_state;0.01570295385560171
***;0.009338075939395342
