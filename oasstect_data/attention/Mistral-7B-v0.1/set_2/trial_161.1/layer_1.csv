text;attention
The;0.009437226040235744
easiest;0.01002871467070936
way;0.008802294435062974
to;0.008017519674538722
import;0.009800373773319833
the;0.007804998093138714
BERT;0.010851500553082831
language;0.009055635755519554
model;0.007914873531074649
into;0.008289552074727802
python;0.00875375913628465
for;0.00799254538364424
use;0.007836847142553666
with;0.007721309278978828
PyTorch;0.01328796368313709
is;0.008403959864678968
using;0.00853609335121373
the;0.007504114052946754
Hugging;0.01051408011811164
Face;0.008099652608851171
Transformer's;0.017492027186412996
library,;0.009983118547027266
which;0.007781061690094312
has;0.00746524823553434
built;0.007929419074635484
in;0.0074953684543805224
methods;0.008142641950246792
for;0.007620110867966142
pre-training,;0.016042933533689994
inference,;0.011400323258492153
and;0.007305614183944874
deploying;0.008885389391314901
BERT.;0.013491383098498701
â€˜**;0.014366540853187075
from;0.008279628423030802
transformers;0.009069098950697426
import;0.008306498082720985
AutoTokenizer,;0.014707359346886658
BertModel;0.00931002541664947
import;0.007851876750358719
torch;0.00831572231942469
tokenizer;0.009336024590612703
=;0.008287628535911295
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.26804492161144444
model;0.007213848296607958
=;0.008090887953102886
"BertModel.from_pretrained(""bert-base-uncased"")";0.15046506820116592
inputs;0.007495785291907133
=;0.007817329252998469
"tokenizer(""Hello,";0.01880325495142493
my;0.007106042201868471
dog;0.007284317560241231
is;0.006983339638250974
"cute"",";0.009822841277654459
"return_tensors=""pt"")";0.022703294190544603
outputs;0.007384693422243002
=;0.007598016785958837
model(**inputs);0.014378839123156812
last_hidden_states;0.013455845182644047
=;0.007103199352685586
outputs.last_hidden_state;0.014412489623364302
***;0.006317930119206826
