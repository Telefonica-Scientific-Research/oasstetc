text;attention
The;0.004721800696049948
easiest;0.0016530026859718782
way;0.0015574589048059167
to;0.0015072787063275896
import;0.0025083868855796543
the;0.0017023264051341475
BERT;0.016064200490446524
language;0.0020426262041762385
model;0.005568260930917815
into;0.0036285965289471903
python;0.005355419164778635
for;0.002723570473300771
use;0.001752516030753606
with;0.0017865129674994656
PyTorch;0.013731502414454205
is;0.002431855426738386
using;0.001921954983747593
the;0.0019181919663501371
Hugging;0.007295270733149735
Face;0.00353647838581983
Transformer's;0.016991557449030265
library,;0.002842133392344339
which;0.0015597192103337354
has;0.001531173114723344
built;0.0013702595131726177
in;0.001588400496316382
methods;0.0016970997853250425
for;0.0015150218274261865
pre-training,;0.00900522004936582
inference,;0.0033903653892609353
and;0.001511404284524738
deploying;0.001998661524844804
BERT.;0.006119792142557423
â€˜**;0.002825552201618489
from;0.0032391805233867354
transformers;0.003517517985757196
import;0.002704556336526024
AutoTokenizer,;0.08554361748256716
BertModel;0.010682171057981102
import;0.002708523090790038
torch;0.0026700539672528494
tokenizer;0.00499672438866975
=;0.0022271812466610652
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6487396406864601
model;0.002164474405295865
=;0.001803493575734074
"BertModel.from_pretrained(""bert-base-uncased"")";0.04392191547668914
inputs;0.0025502675626844773
=;0.0018451775155411848
"tokenizer(""Hello,";0.006918556552883926
my;0.0014750377362440774
dog;0.0015309410225286483
is;0.0013989906925314024
"cute"",";0.0018144874628682253
"return_tensors=""pt"")";0.011585570578455403
outputs;0.0017375205688935633
=;0.0015597163051330198
model(**inputs);0.005652154838220397
last_hidden_states;0.004440505582302345
=;0.0015795050734331375
outputs.last_hidden_state;0.0023228075328007897
***;0.0013161393859148851
