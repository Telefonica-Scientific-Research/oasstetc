text;attention
The;0.005915516404528706
easiest;0.0051631950027232806
way;0.005057488975912841
to;0.004672608641478337
import;0.006707805916901582
the;0.006092692671325995
BERT;0.016133269644319295
language;0.005664940426764527
model;0.008194349826504065
into;0.00607308788359656
python;0.008013989766660894
for;0.0060621540492090085
use;0.005108240808184691
with;0.005923879628731179
PyTorch;0.012213107615603153
is;0.006624784333842375
using;0.006873026535869106
the;0.006245962677733791
Hugging;0.008980129425922289
Face;0.007395507550911377
Transformer's;0.0643658301835931
library,;0.007066855212235709
which;0.004812023569080314
has;0.004773794194915096
built;0.004765022629967141
in;0.004993449855441108
methods;0.00508352551882271
for;0.005647120988317561
pre-training,;0.014727902241832553
inference,;0.00635399490171666
and;0.004304710848017464
deploying;0.006131036926964456
BERT.;0.015456485015046265
â€˜**;0.017261051323696385
from;0.005991144277581043
transformers;0.007652014445153679
import;0.007393898781825327
AutoTokenizer,;0.025822490918763527
BertModel;0.01008532299107517
import;0.005540081370389925
torch;0.00638643773157185
tokenizer;0.008164374446296597
=;0.005617923430920052
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.24468876663821382
model;0.005573873494274425
=;0.005085740034694264
"BertModel.from_pretrained(""bert-base-uncased"")";0.2371073574922832
inputs;0.006294470121884069
=;0.005266206699782038
"tokenizer(""Hello,";0.014759989364015196
my;0.0043083459504268595
dog;0.004625877290249325
is;0.004275030835014791
"cute"",";0.00632297873311878
"return_tensors=""pt"")";0.015613543338430909
outputs;0.004692394696940948
=;0.004460066033987129
model(**inputs);0.01157018003233977
last_hidden_states;0.008817447664147356
=;0.004448513099656539
outputs.last_hidden_state;0.016435843850650427
***;0.004141145039943643
