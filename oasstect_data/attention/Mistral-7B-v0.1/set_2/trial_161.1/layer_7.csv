text;attention
The;0.012450511907649717
easiest;0.0074170296490324765
way;0.007393154877367048
to;0.0065250331507778255
import;0.011715934776384428
the;0.007150685103468625
BERT;0.0230271293314066
language;0.008329232338400303
model;0.016049048612036845
into;0.011984486694262697
python;0.014311207226219027
for;0.00993693706586606
use;0.007694886883017178
with;0.007065475101696906
PyTorch;0.014160247555814178
is;0.011263543581245454
using;0.007767583306717184
the;0.007009397062877468
Hugging;0.011695012833819804
Face;0.011489058269986627
Transformer's;0.03981651912435027
library,;0.014520392359350303
which;0.007503256776096781
has;0.006910087425204587
built;0.005949272110805115
in;0.007121074412151185
methods;0.007503959182045571
for;0.006917818489999246
pre-training,;0.014258812542933718
inference,;0.00921705084664285
and;0.006188166694320716
deploying;0.008268707716817793
BERT.;0.01364338670265462
â€˜**;0.011169835797259218
from;0.009616651475712605
transformers;0.011415155095561407
import;0.009108610680055956
AutoTokenizer,;0.024722858490846066
BertModel;0.014167403669513365
import;0.008243194067281035
torch;0.007615306781526285
tokenizer;0.011403767713829114
=;0.009447759005360306
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2432643223581716
model;0.00893620316061464
=;0.008056937651823234
"BertModel.from_pretrained(""bert-base-uncased"")";0.10017350805323819
inputs;0.009836453463319274
=;0.007342124174280625
"tokenizer(""Hello,";0.027431125312838717
my;0.006145496510253701
dog;0.006788292098354216
is;0.006070387746575493
"cute"",";0.008389567449560011
"return_tensors=""pt"")";0.024517821162741083
outputs;0.007504585163668495
=;0.006605851835486968
model(**inputs);0.014843766363688098
last_hidden_states;0.014400108280168375
=;0.006452869750706649
outputs.last_hidden_state;0.008227628332109486
***;0.005848308644036608
