text;attention
The;0.002064272818780895
easiest;0.0014099924935864648
way;0.0013783006374010265
to;0.0012210579776744715
import;0.0020788965227405126
the;0.0013917586409703143
BERT;0.012077132727050102
language;0.00168622189929192
model;0.0033687943741492294
into;0.002107138052838639
python;0.0034563802813830213
for;0.0018995809213955541
use;0.0014360222810472633
with;0.0016361952514066342
PyTorch;0.008264812983529424
is;0.001827391100211816
using;0.0018414544666976623
the;0.001596842448558727
Hugging;0.004431875683914345
Face;0.0025292715357162457
Transformer's;0.011299282563917375
library,;0.0023228285569581574
which;0.0013716780517960046
has;0.001263713040069646
built;0.0012398703904140382
in;0.0013876215177764533
methods;0.0014621617692248498
for;0.0013786215237582083
pre-training,;0.004708792238973434
inference,;0.0022523505363174815
and;0.0012316086876591192
deploying;0.0015791430632593922
BERT.;0.0039699965178956065
â€˜**;0.0024385628085202135
from;0.0024433032657978832
transformers;0.003765914394637751
import;0.0021260027526428795
AutoTokenizer,;0.031497319194132074
BertModel;0.005660579385155295
import;0.002219619854931595
torch;0.0022927985599549253
tokenizer;0.002948205083017808
=;0.0016572060049808942
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.755884969706795
model;0.0016734131297267084
=;0.0014297886728815528
"BertModel.from_pretrained(""bert-base-uncased"")";0.05289047854640665
inputs;0.0017867950780997223
=;0.0014295913122457048
"tokenizer(""Hello,";0.004930020297220692
my;0.001205550412548593
dog;0.0014459681757363672
is;0.0011849532728967246
"cute"",";0.0014509815845001504
"return_tensors=""pt"")";0.008157722329456668
outputs;0.0014844469261905786
=;0.0012430645344576488
model(**inputs);0.004431866240190972
last_hidden_states;0.0041016692072976604
=;0.0012326559351684772
outputs.last_hidden_state;0.0026863860438213114
***;0.0011291057342234616
