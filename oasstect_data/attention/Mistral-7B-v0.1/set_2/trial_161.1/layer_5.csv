text;attention
The;0.012190236999112439
easiest;0.012556605398775742
way;0.01141848601901049
to;0.01020299579115195
import;0.016986661112641112
the;0.011172180300352302
BERT;0.02181768036497611
language;0.011721424959760637
model;0.01401942741492534
into;0.013317292465200118
python;0.013091943960047094
for;0.011438915651741785
use;0.00981914043616615
with;0.009776340254080088
PyTorch;0.014983039335878742
is;0.01354710806989618
using;0.010886352839798209
the;0.009932992236299327
Hugging;0.014301722376443029
Face;0.018754699233253236
Transformer's;0.020292492710004936
library,;0.014785593805394525
which;0.010352451184408772
has;0.010133440545418532
built;0.009520611013018451
in;0.011408640017902679
methods;0.011365896450157538
for;0.010950723822207615
pre-training,;0.01959839795627835
inference,;0.013118513294058389
and;0.009428596472383038
deploying;0.011362812984885284
BERT.;0.013693354470951902
â€˜**;0.015131208716866315
from;0.014493860240402098
transformers;0.015491245455749226
import;0.01256678595356863
AutoTokenizer,;0.02106824831059311
BertModel;0.014307369474422228
import;0.011930488203373045
torch;0.01130512735891272
tokenizer;0.015143776363458013
=;0.014018516197667997
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.14214745287964883
model;0.012168301970840541
=;0.011665307807343078
"BertModel.from_pretrained(""bert-base-uncased"")";0.06812546074545786
inputs;0.01196682438539114
=;0.010792187475409488
"tokenizer(""Hello,";0.026134938531616015
my;0.009662216895621639
dog;0.009984172475966354
is;0.00917166102191625
"cute"",";0.010614838529474932
"return_tensors=""pt"")";0.020316468647486298
outputs;0.01102005910238946
=;0.009652003728754386
model(**inputs);0.015885269429728368
last_hidden_states;0.017128335532993658
=;0.009568867559445067
outputs.last_hidden_state;0.01171601739050941
***;0.008876219668413714
