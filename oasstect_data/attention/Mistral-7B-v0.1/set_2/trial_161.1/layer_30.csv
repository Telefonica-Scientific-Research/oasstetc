text;attention
The;0.0009978054671659728
easiest;0.0008991473286219479
way;0.0008563823742188235
to;0.0008143098037058335
import;0.0013132352423763122
the;0.001094587317938058
BERT;0.0024209451905048657
language;0.0008670878360386356
model;0.0010161167197024065
into;0.0009727469918364626
python;0.0011361841798843988
for;0.0009891793563788765
use;0.0007952095245513605
with;0.0009019721069577494
PyTorch;0.0016360420003739351
is;0.0009545649835442459
using;0.001141040841738213
the;0.0010867816173336935
Hugging;0.0014289683922233854
Face;0.0011321793442814088
Transformer's;0.00935536036357542
library,;0.0011379171818799152
which;0.0008337969146631365
has;0.0008379081098851751
built;0.0008116792740909135
in;0.0009288352698244808
methods;0.0008308609086985129
for;0.0009778047237694151
pre-training,;0.0035321931092891184
inference,;0.0012261991584569794
and;0.0007696895442747905
deploying;0.0009169887602796492
BERT.;0.0016683669921517938
â€˜**;0.003500049123392617
from;0.0013113319593011155
transformers;0.0013451669616981355
import;0.0015498137355761151
AutoTokenizer,;0.010645756401111742
BertModel;0.003407748572788839
import;0.0012090942350919728
torch;0.0010584403480086246
tokenizer;0.0015603670308078992
=;0.0011654389901418116
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.8494620145111048
model;0.000949058240808354
=;0.0008888033971810509
"BertModel.from_pretrained(""bert-base-uncased"")";0.0492181163837234
inputs;0.001060008803187566
=;0.0009748430873201383
"tokenizer(""Hello,";0.003413986432096073
my;0.0009006553959216557
dog;0.0008551336938159023
is;0.0007459177488499933
"cute"",";0.0011304870195294723
"return_tensors=""pt"")";0.004634421218028867
outputs;0.0008802120618479866
=;0.000803790973491124
model(**inputs);0.002714664076682901
last_hidden_states;0.0028685023265140786
=;0.0007379643608741476
outputs.last_hidden_state;0.004007660312703291
***;0.0007184656681844691
