text;attention
The;0.012426358587640749
easiest;0.009427367567355696
way;0.008979503703469803
to;0.008430083778509435
import;0.011218863160620504
the;0.008948272384657624
BERT;0.0186671598658893
language;0.009453434527503348
model;0.010956471757447272
into;0.01279268823331276
python;0.011594308258662778
for;0.010346346862043625
use;0.008388530831283264
with;0.008682349650480056
PyTorch;0.014324310188958902
is;0.017590773526809877
using;0.009684646061038046
the;0.008660910176268792
Hugging;0.013364931903410501
Face;0.011640118636937272
Transformer's;0.022009698220703797
library,;0.01390571937952699
which;0.009237252399014436
has;0.00830048063923003
built;0.007266775407103094
in;0.00901760984745468
methods;0.008628064472280357
for;0.009771954060920318
pre-training,;0.015280450718673283
inference,;0.011093202808049617
and;0.00783854994950403
deploying;0.010166618347675592
BERT.;0.015375665050296637
â€˜**;0.019621481470124914
from;0.014965828954584887
transformers;0.011371610838843615
import;0.01043982892412037
AutoTokenizer,;0.020610794824418445
BertModel;0.01246225276834728
import;0.010527862331663066
torch;0.008770774612355814
tokenizer;0.0117007347944773
=;0.011613815027452727
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.20382842075399168
model;0.009889588447393186
=;0.009504309308587509
"BertModel.from_pretrained(""bert-base-uncased"")";0.09810679374035766
inputs;0.010024519134237622
=;0.008615469364258842
"tokenizer(""Hello,";0.025722415267792526
my;0.007688347006043704
dog;0.007721976517301656
is;0.007256681965209419
"cute"",";0.009187686301971533
"return_tensors=""pt"")";0.019572577523093857
outputs;0.008937432841490966
=;0.007794501317453781
model(**inputs);0.01769531429781904
last_hidden_states;0.016988560914672313
=;0.008240900475850868
outputs.last_hidden_state;0.010747683243640433
***;0.0069223660697123705
