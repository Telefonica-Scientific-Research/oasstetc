text;attention
The;0.0026286167659699197
easiest;0.0008796158060598643
way;0.0008638006056559838
to;0.0007183724256512638
import;0.001352704608494583
the;0.0008672591937812334
BERT;0.005065969894465618
language;0.0008272146560017439
model;0.002399341846484272
into;0.001907669172719131
python;0.00290173469495587
for;0.0014993377256574455
use;0.0009274145696354827
with;0.0008057128889575975
PyTorch;0.004226942997798441
is;0.0018222244832836801
using;0.0011371211769075028
the;0.0009873400677017734
Hugging;0.0026015112225298525
Face;0.0014986594904699334
Transformer's;0.007128658791894283
library,;0.001875044580155336
which;0.0009351441220283486
has;0.0008752108408445246
built;0.0006751623923478327
in;0.0009567347908819272
methods;0.000957240686738418
for;0.0009154964114269279
pre-training,;0.00481507799883587
inference,;0.001798486911414138
and;0.0007700590126367117
deploying;0.0009469052942128394
BERT.;0.003941413529845595
â€˜**;0.0015807436598521552
from;0.0018773628811469662
transformers;0.0021798814162438144
import;0.001645539038973138
AutoTokenizer,;0.01648519844898197
BertModel;0.0022028835019488767
import;0.001333678240991987
torch;0.0010469070215531476
tokenizer;0.0026365365130805313
=;0.0014464076882377322
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7514270630799126
model;0.0011858675632862726
=;0.0011288283263965189
"BertModel.from_pretrained(""bert-base-uncased"")";0.11261915090641147
inputs;0.0015244571939780635
=;0.0011539274404890624
"tokenizer(""Hello,";0.013783853249113862
my;0.0007339145524065598
dog;0.0008032474069972274
is;0.0006967861869221024
"cute"",";0.0011719764027405446
"return_tensors=""pt"")";0.006043372305151739
outputs;0.0009766363066200374
=;0.0008589778351631651
model(**inputs);0.005458189595133874
last_hidden_states;0.002503992224718567
=;0.00086007211810371
outputs.last_hidden_state;0.0015049436683793552
***;0.0006204055706209775
