text;attention
The;0.012418095208160283
easiest;0.0026821262278103287
way;0.0025060492075426742
to;0.0019938947139646356
import;0.0042682446895939115
the;0.0022616694878131696
BERT;0.011464477734458526
language;0.0020894006714825488
model;0.006047172085200083
into;0.0053897406625708875
python;0.005696834979029604
for;0.0038665692375599516
use;0.002544449084875913
with;0.002132827038071802
PyTorch;0.0078627246599488
is;0.008139183373342657
using;0.003898413780711334
the;0.0026844917059116865
Hugging;0.0053122161838253966
Face;0.003283022476879752
Transformer's;0.016364543422171657
library,;0.005580522559666102
which;0.002668072800904649
has;0.0021685750544340587
built;0.0017782340104484865
in;0.0024819997675686618
methods;0.002515441061317048
for;0.002523365276885933
pre-training,;0.006909606328132794
inference,;0.0038705883383576786
and;0.0020592373553909977
deploying;0.0023748063437454576
BERT.;0.010525682991725866
â€˜**;0.007078922256433211
from;0.0036966392662015984
transformers;0.004596481078645925
import;0.0033894454411749096
AutoTokenizer,;0.02149681666301424
BertModel;0.005192705213070458
import;0.004153590146886927
torch;0.0027767280867810455
tokenizer;0.006554627386158229
=;0.003981206977308842
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5956622438519593
model;0.002908978140828405
=;0.0030352736530915017
"BertModel.from_pretrained(""bert-base-uncased"")";0.10742301583997366
inputs;0.0032612409374531536
=;0.002919669287707857
"tokenizer(""Hello,";0.019536770490550536
my;0.0019016592812640218
dog;0.0020367362618329997
is;0.0018752709537985797
"cute"",";0.0025684064258128885
"return_tensors=""pt"")";0.01002438343593665
outputs;0.0024558294426868903
=;0.0022343413737149916
model(**inputs);0.00824606868695252
last_hidden_states;0.005211968252224068
=;0.002263134831777864
outputs.last_hidden_state;0.0034796812705843727
***;0.001675886546671153
