text;attention
The;0.015115492562291732
easiest;0.014927590277705467
way;0.01294453193161226
to;0.01259362465671063
import;0.015970437953979958
the;0.012943838322945206
BERT;0.014972573130050253
language;0.013294061290275706
model;0.012400921027431877
into;0.01241026299704567
python;0.012032696635776288
for;0.012594932336907495
use;0.01194151867251715
with;0.011845750681696665
PyTorch;0.01979273751818676
is;0.01258719895421361
using;0.012234823831574617
the;0.012488709370476045
Hugging;0.014409521259286639
Face;0.012600710800639663
Transformer's;0.014833742572875768
library,;0.016126108095071506
which;0.01154313033206584
has;0.011255404275489555
built;0.011281819539100945
in;0.011166988145361753
methods;0.011876436085330396
for;0.011332122935155398
pre-training,;0.018487086364675866
inference,;0.013436526507793827
and;0.011101014241315302
deploying;0.011418248426114884
BERT.;0.019395765166245808
â€˜**;0.016851491879577726
from;0.011570037761136857
transformers;0.013223311703923393
import;0.013345573330737277
AutoTokenizer,;0.017932366570902837
BertModel;0.013101674052573968
import;0.011406758353411586
torch;0.011756042909138849
tokenizer;0.012561600417141605
=;0.012050618209266874
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.10362969236974122
model;0.01066557236593056
=;0.011431081685151003
"BertModel.from_pretrained(""bert-base-uncased"")";0.08098130994419385
inputs;0.011053459794125708
=;0.011088452728504335
"tokenizer(""Hello,";0.025652079831203524
my;0.011150777646800161
dog;0.0110483416470035
is;0.010648803728254228
"cute"",";0.012745080722450876
"return_tensors=""pt"")";0.03312036203618531
outputs;0.011169016426029474
=;0.01087312485911456
model(**inputs);0.018265748315866522
last_hidden_states;0.014611397347500762
=;0.010489776459822919
outputs.last_hidden_state;0.01442347720703658
***;0.009796642799353368
