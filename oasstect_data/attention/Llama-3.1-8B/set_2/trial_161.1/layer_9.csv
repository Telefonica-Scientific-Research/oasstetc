text;attention
The;0.011720260769095459
easiest;0.0073190945601187656
way;0.007384420162921377
to;0.006757162281694835
import;0.010071397546393033
the;0.00757828023178464
BERT;0.0307604437528418
language;0.008174140044781427
model;0.0193941502529663
into;0.0129975219817933
python;0.021866488941021153
for;0.010682275675495572
use;0.008000092827021155
with;0.0074415371241301174
PyTorch;0.03244936100532425
is;0.013367570686058471
using;0.009248409089366926
the;0.007672714528437652
Hugging;0.011315396648160496
Face;0.011847866003466226
Transformer's;0.02254041854419137
library,;0.013100923357058408
which;0.007394185966318294
has;0.006468283134546161
built;0.005660318315710991
in;0.007297667838144148
methods;0.0074852634114646365
for;0.006914322441139187
pre-training,;0.01380605231179034
inference,;0.009379677330321603
and;0.0061553273174507215
deploying;0.006893465574915543
BERT.;0.016850103493892064
â€˜**;0.014985698500007772
from;0.008936735910090862
transformers;0.015664312528668168
import;0.009614823538895192
AutoTokenizer,;0.035549226065650946
BertModel;0.021241084089785183
import;0.01199137102010135
torch;0.010359024019195607
tokenizer;0.00802728296028706
=;0.010443440648660207
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2501724826100694
model;0.00832629861079707
=;0.007694323073540931
"BertModel.from_pretrained(""bert-base-uncased"")";0.05643990445156671
inputs;0.008034598647217263
=;0.0066367942070566285
"tokenizer(""Hello,";0.026634594629164995
my;0.006000205618123871
dog;0.006525074056661118
is;0.005865051940507536
"cute"",";0.007475067822650222
"return_tensors=""pt"")";0.0223066816835374
outputs;0.007601933562157723
=;0.006277214777611256
model(**inputs);0.012663149097245762
last_hidden_states;0.009217547870733943
=;0.006296875342885192
outputs.last_hidden_state;0.007262147465272065
***;0.0057624621020421615
