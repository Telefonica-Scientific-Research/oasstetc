text;attention
The;0.013814656061067838
easiest;0.012481405560738968
way;0.012351292092112921
to;0.01177009415586337
import;0.014906911397949953
the;0.012439616597704116
BERT;0.029650100656774963
language;0.012834908963231315
model;0.016304383765581583
into;0.013264997031461237
python;0.01512706907461849
for;0.013039543619375886
use;0.012148264408716455
with;0.01231553013295537
PyTorch;0.02109104104517721
is;0.013308207592816325
using;0.013395378199793377
the;0.012587712709419094
Hugging;0.014667035651773401
Face;0.014077953780255711
Transformer's;0.019038965964568562
library,;0.013852954846613643
which;0.01215353504792882
has;0.011899537273314624
built;0.011337042657748384
in;0.012245855742570325
methods;0.012611658706043817
for;0.012214827325873917
pre-training,;0.016473388342513762
inference,;0.013704490650025077
and;0.011499688781817646
deploying;0.011963811456352728
BERT.;0.017064009633222344
â€˜**;0.016530465238812343
from;0.013876351090074028
transformers;0.015319379632300048
import;0.013863221176602956
AutoTokenizer,;0.026320128010795762
BertModel;0.019192236062852763
import;0.014200960991746224
torch;0.013474973974063328
tokenizer;0.017007089527146076
=;0.01323255861026768
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0892745667228134
model;0.013132428588990453
=;0.012333747616941455
"BertModel.from_pretrained(""bert-base-uncased"")";0.035847494515491976
inputs;0.0180722333203193
=;0.012306254523412306
"tokenizer(""Hello,";0.02199303071732497
my;0.011619101903658367
dog;0.01230877750602786
is;0.011450630265721453
"cute"",";0.01319914820392189
"return_tensors=""pt"")";0.02427906761979125
outputs;0.013425293074123984
=;0.011917873632661976
model(**inputs);0.015985946666042
last_hidden_states;0.016148215376783903
=;0.011680233696545913
outputs.last_hidden_state;0.013096571213123357
***;0.011276151595661758
