text;attention
The;0.014576368940146354
easiest;0.012747489176700412
way;0.012120896944723455
to;0.012080523453906217
import;0.01615334513798061
the;0.012820678954113148
BERT;0.02590224327118176
language;0.015163832978794037
model;0.014790366552310566
into;0.01777766785096011
python;0.01801623248905449
for;0.015444051437201541
use;0.011435453599754189
with;0.011547622533212424
PyTorch;0.017587582721264
is;0.017221185493577516
using;0.013015170430720256
the;0.01164930311494997
Hugging;0.013712560741026308
Face;0.013434581602851964
Transformer's;0.020615996473310153
library,;0.01792938644406051
which;0.012571167293064178
has;0.011276280332766142
built;0.010516363301910038
in;0.011622363921846877
methods;0.01174791120396985
for;0.012449446183854983
pre-training,;0.021536558879498423
inference,;0.012835563724631417
and;0.010502125706762223
deploying;0.011009433009088429
BERT.;0.019663989322127814
â€˜**;0.018832985733094207
from;0.015620173401405264
transformers;0.016319196951531712
import;0.013593169842838235
AutoTokenizer,;0.02477492438661387
BertModel;0.016490726086399482
import;0.014331099996599296
torch;0.01110940351519742
tokenizer;0.01366405795232592
=;0.014618088414857709
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08070552264212934
model;0.012614900024357745
=;0.01301565529263284
"BertModel.from_pretrained(""bert-base-uncased"")";0.045060523538448694
inputs;0.013866125020764331
=;0.012833325356559469
"tokenizer(""Hello,";0.030663492294623212
my;0.011036472455709798
dog;0.011032925539909937
is;0.01033039775129679
"cute"",";0.011778371595622259
"return_tensors=""pt"")";0.020166077332872522
outputs;0.011852679252479344
=;0.01132656628809226
model(**inputs);0.015513173053005772
last_hidden_states;0.01455896808750999
=;0.01116472589379708
outputs.last_hidden_state;0.01151489548140794
***;0.010137633596597188
