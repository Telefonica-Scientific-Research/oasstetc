text;attention
The;0.015052504673908076
easiest;0.013241247517607182
way;0.013313140082211313
to;0.012795894742712182
import;0.015910086054655215
the;0.013847058390860313
BERT;0.01945189615310757
language;0.013509352069180141
model;0.014362286855369487
into;0.013578403656725614
python;0.014668486780401883
for;0.013249201793295843
use;0.012670348867301185
with;0.013404467537360867
PyTorch;0.019575517791087304
is;0.014070960432139004
using;0.013884843154742268
the;0.013843265193837611
Hugging;0.01449323650304796
Face;0.014895286700301581
Transformer's;0.016537371314099006
library,;0.014527681517964547
which;0.013085667221699078
has;0.012852757194365523
built;0.01289725888853831
in;0.013058690285575747
methods;0.013933381568523267
for;0.01312654688497822
pre-training,;0.01527800667119747
inference,;0.01369123600286238
and;0.012442156408461308
deploying;0.013400629043292792
BERT.;0.019340811360692222
â€˜**;0.01827229035619812
from;0.014315977829341835
transformers;0.014008320002690873
import;0.014057299909963183
AutoTokenizer,;0.020252210729087308
BertModel;0.01571379770062936
import;0.013802220601674874
torch;0.015629597474482778
tokenizer;0.014494392075481933
=;0.013543789254742885
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.061274088588075454
model;0.013507633033581011
=;0.013330688341173316
"BertModel.from_pretrained(""bert-base-uncased"")";0.05919375377091343
inputs;0.014852302044496874
=;0.013464217142335214
"tokenizer(""Hello,";0.02178553924295639
my;0.012766352928699199
dog;0.01377701729557643
is;0.012605359050511265
"cute"",";0.014820215755085141
"return_tensors=""pt"")";0.021192690270586904
outputs;0.013820567363541116
=;0.012809064005672607
model(**inputs);0.016187375233835257
last_hidden_states;0.015881176060433183
=;0.012675916755349459
outputs.last_hidden_state;0.015533771002067267
***;0.012442696868714753
