text;attention
The;0.014999743786476692
easiest;0.012970729963341636
way;0.01336947092860064
to;0.012693775343429895
import;0.01634121507107426
the;0.013363750336785501
BERT;0.025950983804072385
language;0.012874850764567462
model;0.014702881287554639
into;0.013785749335314203
python;0.014795566622058732
for;0.013276689794576706
use;0.012474521782405762
with;0.01313176683214014
PyTorch;0.019221067767372167
is;0.013553692647870832
using;0.013589074476890997
the;0.01333672575224868
Hugging;0.015987264071513047
Face;0.014842519078118116
Transformer's;0.016907729542358734
library,;0.014279124214128152
which;0.012691657963487271
has;0.0125231683063878
built;0.012545529819496151
in;0.013312782491672639
methods;0.013120851626048056
for;0.01256411701269371
pre-training,;0.015088386428589054
inference,;0.013315730585000863
and;0.012184996220637175
deploying;0.012990069070840913
BERT.;0.01840633918864447
â€˜**;0.02004097529812697
from;0.014191053811164096
transformers;0.014504636094394006
import;0.01471633671879118
AutoTokenizer,;0.020174554980306153
BertModel;0.019273781647153456
import;0.013594712848630553
torch;0.015443221438130608
tokenizer;0.014069896993683297
=;0.013215451991161819
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07697953221929457
model;0.013236025246607849
=;0.013277703555675839
"BertModel.from_pretrained(""bert-base-uncased"")";0.04090016374558907
inputs;0.015467480595914617
=;0.013526444799896618
"tokenizer(""Hello,";0.022669939049683303
my;0.012229973718696762
dog;0.012289200779848835
is;0.012089105657050465
"cute"",";0.013392638674299745
"return_tensors=""pt"")";0.019568466514940083
outputs;0.01516444744472157
=;0.012524421173608392
model(**inputs);0.017522424291855604
last_hidden_states;0.01536584951388998
=;0.01246449230931665
outputs.last_hidden_state;0.014640694880069896
***;0.01227385209110049
