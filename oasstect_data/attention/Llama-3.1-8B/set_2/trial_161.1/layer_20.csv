text;attention
The;0.014723636711359261
easiest;0.01317622856177002
way;0.013185929221819988
to;0.012676289965531658
import;0.01715027838521712
the;0.013132913438743067
BERT;0.02569887902278863
language;0.013281579053793418
model;0.016329643979855472
into;0.014787480383604907
python;0.016512348652658247
for;0.014062280277535371
use;0.01290819761442303
with;0.013265055464193577
PyTorch;0.018857224102578136
is;0.013726525268143436
using;0.014296736473240371
the;0.013320049573277625
Hugging;0.01348794361631938
Face;0.014396513824153244
Transformer's;0.01895287546351228
library,;0.014740506105895518
which;0.013049326744710777
has;0.012827535066669461
built;0.012125847939302313
in;0.0136736516104793
methods;0.013575162616988917
for;0.013438847587579348
pre-training,;0.016685825185327935
inference,;0.013882983932778141
and;0.012000723741755566
deploying;0.012569615814256778
BERT.;0.017143267533466202
â€˜**;0.018171355814090268
from;0.015201752621385105
transformers;0.01422234651432744
import;0.01494322067407539
AutoTokenizer,;0.02694459323440449
BertModel;0.019674542117156302
import;0.013778991913742452
torch;0.014719066456132883
tokenizer;0.014970461254764004
=;0.01309170113127045
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07535069658136127
model;0.013820036417108392
=;0.012934742230943508
"BertModel.from_pretrained(""bert-base-uncased"")";0.031206775364868336
inputs;0.015675622356229414
=;0.012726358209498669
"tokenizer(""Hello,";0.020213722476727754
my;0.012156295645272465
dog;0.012499648459865639
is;0.011873551365849019
"cute"",";0.01275324075150439
"return_tensors=""pt"")";0.019594246554021102
outputs;0.014616352703182645
=;0.012583922106149533
model(**inputs);0.01696288642070554
last_hidden_states;0.017487175346593355
=;0.012657137351204821
outputs.last_hidden_state;0.013800656308524914
***;0.011726998685312132
