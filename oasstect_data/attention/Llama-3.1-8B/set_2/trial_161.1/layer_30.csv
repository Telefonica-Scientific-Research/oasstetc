text;attention
The;0.004111916558189785
easiest;0.003339975092956649
way;0.003250836403558718
to;0.0031476237948052403
import;0.004129209928732205
the;0.003798386173363138
BERT;0.011539928511158341
language;0.0036258167242944873
model;0.0042929503109283285
into;0.00394759469800984
python;0.0050448956849368344
for;0.0039073211586675574
use;0.0031579813706548335
with;0.0037379397446742264
PyTorch;0.007491755138649691
is;0.003861264354150991
using;0.0041162997418581395
the;0.0042316037737356
Hugging;0.004510662656202627
Face;0.004760739700203176
Transformer's;0.007514979838797917
library,;0.004297476009567221
which;0.003276305592093348
has;0.003225343251194443
built;0.002982088816590845
in;0.0033813929587051834
methods;0.0033477651282141415
for;0.0035056700186369073
pre-training,;0.005762906665474696
inference,;0.004009742463446911
and;0.0027947458863608037
deploying;0.0031556451200713763
BERT.;0.008196562140505412
â€˜**;0.012620318935434976
from;0.0040718798629582375
transformers;0.0037271304013016923
import;0.004766526003601351
AutoTokenizer,;0.013950698991292065
BertModel;0.007337224067266054
import;0.004295643522404901
torch;0.0045938418652740395
tokenizer;0.0047728194333127956
=;0.003383307589815429
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.46215927997938516
model;0.00351069164727413
=;0.003139057945849621
"BertModel.from_pretrained(""bert-base-uncased"")";0.26183327528196687
inputs;0.003915851600532007
=;0.0032264801988162958
"tokenizer(""Hello,";0.007935893538694654
my;0.003120848971355264
dog;0.0030525072481079225
is;0.0028836486732246474
"cute"",";0.004038625207069614
"return_tensors=""pt"")";0.006902788906100118
outputs;0.003421452414511968
=;0.002880821707334038
model(**inputs);0.004959988829605496
last_hidden_states;0.004696064710172176
=;0.002834020334462676
outputs.last_hidden_state;0.0036565688095572123
***;0.0028574179139291217
