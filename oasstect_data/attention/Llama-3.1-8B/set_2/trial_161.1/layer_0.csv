text;attention
The;0.014340631460189634
easiest;0.005410671634327848
way;0.004213973573893382
to;0.012136303392396545
import;0.008015346683566271
the;0.01129324939250265
BERT;0.010697952590541094
language;0.00427961638742238
model;0.004371175890776376
into;0.005781226212202665
python;0.008707874127040329
for;0.012932286798480075
use;0.00414264745547977
with;0.012712312755959174
PyTorch;0.011033951557277958
is;0.007166034606092665
using;0.004810754979186336
the;0.009633128944993321
Hugging;0.009885121606883651
Face;0.003927739267000056
Transformer's;0.010022414039715254
library,;0.03754451689932273
which;0.004067068765858142
has;0.005434727099701507
built;0.004184393947360236
in;0.009490321376150376
methods;0.0040743565001275
for;0.01018242725582881
pre-training,;0.03574054020930945
inference,;0.027250518273397223
and;0.010832333901313454
deploying;0.004406603289312959
BERT.;0.03148877830496044
â€˜**;0.0055194665334946415
from;0.006441109348062088
transformers;0.0045958245154369875
import;0.005254726590892762
AutoTokenizer,;0.02991488353958405
BertModel;0.004676213439542298
import;0.0051456486292992395
torch;0.0058466501104417635
tokenizer;0.0045979862140290975
=;0.008441847694701462
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.27278143890817985
model;0.0036388569270226867
=;0.007892728982898535
"BertModel.from_pretrained(""bert-base-uncased"")";0.1625944221667518
inputs;0.003591049475931617
=;0.006783201844083361
"tokenizer(""Hello,";0.035556344851110526
my;0.004631332929475364
dog;0.003511985186263221
is;0.00476849521400142
"cute"",";0.006034397693719199
"return_tensors=""pt"")";0.013852913040140784
outputs;0.0034296152410777966
=;0.005605832966747236
model(**inputs);0.007942364100307521
last_hidden_states;0.004640062740935655
=;0.004540614755730589
outputs.last_hidden_state;0.004491820311308923
***;0.0030671668702593395
