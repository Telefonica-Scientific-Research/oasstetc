text;attention
The;0.010608651175652491
easiest;0.009399367566621627
way;0.009817858576191107
to;0.009370521856064025
import;0.011368551822213671
the;0.010545099519325392
BERT;0.01865133079019812
language;0.009628157828634826
model;0.012061965428307819
into;0.011112752046246017
python;0.011570278781933988
for;0.010628348021139046
use;0.00927692344177579
with;0.010158761850260274
PyTorch;0.015971678334605607
is;0.010244451886269256
using;0.011496055809462122
the;0.010552999729217746
Hugging;0.010630284566143068
Face;0.012655019917480905
Transformer's;0.016815740850854525
library,;0.01176513170629715
which;0.009615604009663782
has;0.00996958579393346
built;0.009272539602430754
in;0.01030643460090188
methods;0.009586861651088623
for;0.011083977852781701
pre-training,;0.0170491670052238
inference,;0.011518781237824068
and;0.008747459523028108
deploying;0.009702838846344587
BERT.;0.01591102589282974
â€˜**;0.017157527650225976
from;0.01025796580617213
transformers;0.01096022763530648
import;0.010630008234682674
AutoTokenizer,;0.01955149608507104
BertModel;0.014681406708436238
import;0.009547704751669373
torch;0.010995748728765397
tokenizer;0.010806637715353807
=;0.009784245149498704
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.20320994254071964
model;0.009775329522972737
=;0.009416815373583421
"BertModel.from_pretrained(""bert-base-uncased"")";0.09767936045661696
inputs;0.01033680350798528
=;0.009294549892245093
"tokenizer(""Hello,";0.02165487961303214
my;0.00945807033006222
dog;0.010034544347245882
is;0.009037064137274812
"cute"",";0.011459975503215838
"return_tensors=""pt"")";0.01948096172879117
outputs;0.01021531111278896
=;0.008694647295603906
model(**inputs);0.01564990843327941
last_hidden_states;0.01346482931149513
=;0.008837994225327418
outputs.last_hidden_state;0.011795832951291205
***;0.009036003730341876
