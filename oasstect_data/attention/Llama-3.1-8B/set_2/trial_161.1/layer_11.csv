text;attention
The;0.012132679728604637
easiest;0.0058700538197586135
way;0.006161634100023824
to;0.005369042230701694
import;0.00768747150571969
the;0.006453657658575889
BERT;0.03720293490888883
language;0.005901257458426417
model;0.013915995809247739
into;0.011400851014867434
python;0.013493607881156866
for;0.009104982504279518
use;0.006356641662413933
with;0.005460432791983321
PyTorch;0.023477718739884326
is;0.012910646226758925
using;0.008270200041707542
the;0.007543080222453511
Hugging;0.007732818733268661
Face;0.007781167040799088
Transformer's;0.015939773167602162
library,;0.010121713007021959
which;0.00634925447294139
has;0.005470664149672351
built;0.004552728070012468
in;0.00570821294516995
methods;0.005578551361509797
for;0.005579772629230625
pre-training,;0.01018457015278797
inference,;0.00792551488244312
and;0.005076899210398681
deploying;0.00532794496761299
BERT.;0.0196172557157761
â€˜**;0.01656540328320965
from;0.007879814084861858
transformers;0.008143741876853154
import;0.008350582854303849
AutoTokenizer,;0.04005917846429559
BertModel;0.014841668668421612
import;0.009080418939966943
torch;0.00753357756738327
tokenizer;0.008825643311212905
=;0.008386573156757417
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3666693025393224
model;0.006026277997240598
=;0.006191137622806251
"BertModel.from_pretrained(""bert-base-uncased"")";0.04552921668129764
inputs;0.006554393668785934
=;0.00585797345435995
"tokenizer(""Hello,";0.02384617718231881
my;0.0048180788556373925
dog;0.005031870060987482
is;0.004574956306353345
"cute"",";0.005846482761159991
"return_tensors=""pt"")";0.022824164743645955
outputs;0.006452223775690047
=;0.005009285047756529
model(**inputs);0.010326321923292487
last_hidden_states;0.008544038330377915
=;0.004877674333234904
outputs.last_hidden_state;0.005318563596828186
***;0.004375530099937789
