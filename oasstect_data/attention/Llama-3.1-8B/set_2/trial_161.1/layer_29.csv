text;attention
The;0.011414441953823006
easiest;0.010139418732516756
way;0.010598813349981778
to;0.00966907052471568
import;0.010896320578933421
the;0.010595513819664651
BERT;0.020474466710718463
language;0.010392652363314483
model;0.012077282543477418
into;0.010490640945134822
python;0.011844546265054608
for;0.010672455810186067
use;0.009751655813648528
with;0.010321502130000574
PyTorch;0.019251529358794185
is;0.01067215015508913
using;0.0110651223201687
the;0.010784941116199943
Hugging;0.011573168812994576
Face;0.012342369019113975
Transformer's;0.01393913154510799
library,;0.011019654896407287
which;0.010111004009150305
has;0.01001220390694435
built;0.00945597870107117
in;0.010294128297174325
methods;0.009760439821069989
for;0.010660076153486792
pre-training,;0.0145107305901597
inference,;0.011107493629153755
and;0.0093022496955232
deploying;0.010286860886413014
BERT.;0.017644610180113172
â€˜**;0.01995113041814573
from;0.01045257228433163
transformers;0.01144119755826709
import;0.010991200455729808
AutoTokenizer,;0.017838299127452086
BertModel;0.014743654410484098
import;0.010286825170796883
torch;0.01165183116408517
tokenizer;0.011316789752116219
=;0.009811939467357193
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.17007219146322533
model;0.009822834302290244
=;0.00974841686099561
"BertModel.from_pretrained(""bert-base-uncased"")";0.12059158895430114
inputs;0.013066399872347692
=;0.009982312351363973
"tokenizer(""Hello,";0.016397679610335773
my;0.009605691084685633
dog;0.011128044366176155
is;0.00982512718881145
"cute"",";0.011056526179132775
"return_tensors=""pt"")";0.020771536314408083
outputs;0.011459222343037888
=;0.009369678225403171
model(**inputs);0.012608057680548944
last_hidden_states;0.012668157154400417
=;0.009365710183933606
outputs.last_hidden_state;0.0115463077398296
***;0.009296453680700673
