text;attention
The;0.014600292063435365
easiest;0.01708065307707953
way;0.015565682457220734
to;0.014492160746840902
import;0.020217519215460784
the;0.0146014434432607
BERT;0.020289250564721795
language;0.0158538248377869
model;0.015968132591973678
into;0.015089114740292503
python;0.01580372481528451
for;0.01435469330435267
use;0.013568875791715132
with;0.013477489635030057
PyTorch;0.018713993332708758
is;0.014984872588343153
using;0.014136304296047113
the;0.013230904561502265
Hugging;0.014840450611115922
Face;0.01685582227368356
Transformer's;0.01904612285167764
library,;0.017874642252023957
which;0.01383105034653042
has;0.013317018639287066
built;0.013437783549426941
in;0.01400856531207917
methods;0.014705184595912673
for;0.013563774940942672
pre-training,;0.017165051549704137
inference,;0.014143116218220304
and;0.01278474698856173
deploying;0.01333170377784242
BERT.;0.016604537301072846
â€˜**;0.01717565431304202
from;0.014966356606798256
transformers;0.019075974299241728
import;0.015317873531871072
AutoTokenizer,;0.01862889715737212
BertModel;0.015218470734763012
import;0.014264478569805457
torch;0.013608853944562748
tokenizer;0.014550491904354925
=;0.01518584388721437
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04553293799392508
model;0.013711195774740866
=;0.014217739354037041
"BertModel.from_pretrained(""bert-base-uncased"")";0.035232051453878324
inputs;0.013931338116899858
=;0.013716795086771877
"tokenizer(""Hello,";0.023616120288694777
my;0.013033989150896534
dog;0.014338531662241924
is;0.012600413808968926
"cute"",";0.013619627299541102
"return_tensors=""pt"")";0.021958565525438213
outputs;0.013630043693562157
=;0.013277454113753142
model(**inputs);0.016692159850254778
last_hidden_states;0.0157796674308397
=;0.012972010353715343
outputs.last_hidden_state;0.014123940519950483
***;0.012484020301726073
