text;attention
The;0.9999991347448615
easiest;1.0339279339474841e-24
way;9.217856023587182e-25
to;9.786789154746264e-25
import;1.6543569809404758e-24
the;1.060137841351291e-24
BERT;3.946822065238597e-24
language;1.2513179606207223e-24
model;3.0938813295652347e-24
into;1.0740348712733548e-24
python;1.0729379163832022e-24
for;9.613243949235385e-25
use;8.253105003742612e-25
with;9.128575673861992e-25
PyTorch;2.156163641539678e-24
is;9.681432568687985e-25
using;9.927118636595827e-25
the;9.666240167979416e-25
Hugging;2.0733740227660823e-24
Face;1.3565103247797216e-24
Transformer's;2.710303128565177e-24
library,;1.077322323820991e-24
which;8.481071798218708e-25
has;8.588134485386556e-25
built;7.73020592790349e-25
in;8.60569202786955e-25
methods;9.032848995939559e-25
for;9.153227089065131e-25
pre-training,;1.6325579413389077e-24
inference,;1.197245602521662e-24
and;8.197123490179952e-25
deploying;8.935146559437708e-25
BERT.;8.652551385933003e-07
â€˜**;1.1223224074849106e-22
from;1.2826532556949791e-24
transformers;1.2794127864984814e-24
import;1.04800675011991e-24
AutoTokenizer,;3.619176611662605e-24
BertModel;1.9382202816441306e-24
import;1.113058342741567e-24
torch;1.0418856242908212e-24
tokenizer;1.627800878729626e-24
=;1.281983557359891e-24
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.719983464155373e-23
model;9.908415406107928e-25
=;9.686150881541017e-25
"BertModel.from_pretrained(""bert-base-uncased"")";8.488895951418031e-24
inputs;1.0517266319021652e-24
=;1.0196572037088945e-24
"tokenizer(""Hello,";2.0652789781718355e-24
my;7.83792930426577e-25
dog;8.20268158087055e-25
is;7.684506843598707e-25
"cute"",";9.181258254197427e-25
"return_tensors=""pt"")";4.613917139492674e-24
outputs;9.724252673831497e-25
=;8.721750360912124e-25
model(**inputs);1.980532496833251e-24
last_hidden_states;1.8078727554990524e-24
=;8.75654419560294e-25
outputs.last_hidden_state;1.2005402379907723e-24
***;7.608217089540757e-25
