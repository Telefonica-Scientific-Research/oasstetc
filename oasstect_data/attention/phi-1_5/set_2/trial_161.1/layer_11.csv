text;attention
The;0.99999871632179
easiest;5.145665862107711e-24
way;4.558472332296992e-24
to;4.615869402336901e-24
import;8.307324423957076e-24
the;5.783270561317232e-24
BERT;1.8256457951122823e-23
language;7.196476574233375e-24
model;1.1847952521008041e-23
into;7.039395681431584e-24
python;6.2327638081701766e-24
for;6.356794841853745e-24
use;4.87852553567907e-24
with;5.669774741475534e-24
PyTorch;8.7435569871992e-24
is;5.941980756207982e-24
using;5.620231265415406e-24
the;5.196442196121238e-24
Hugging;1.1063546545015784e-23
Face;7.458065727083393e-24
Transformer's;1.8255544999579815e-23
library,;7.035748582322224e-24
which;4.812693306354498e-24
has;5.768678477041357e-24
built;4.2889390908191905e-24
in;4.952529163880121e-24
methods;5.5051949197925804e-24
for;5.377840628490251e-24
pre-training,;1.3880083800469707e-23
inference,;8.483159698684154e-24
and;4.702992406453222e-24
deploying;5.383335800081544e-24
BERT.;1.2836782098909877e-06
â€˜**;6.978463003737501e-23
from;6.179499991927926e-24
transformers;7.831376683379345e-24
import;6.564758298514746e-24
AutoTokenizer,;2.4782172838357985e-23
BertModel;9.148654959174287e-24
import;7.52248120973302e-24
torch;5.5297589609543116e-24
tokenizer;7.900364999268801e-24
=;6.309956914121076e-24
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.1738449408094403e-22
model;6.165097172797731e-24
=;5.879148630095949e-24
"BertModel.from_pretrained(""bert-base-uncased"")";9.322944900958698e-23
inputs;6.37446048142691e-24
=;6.052369711764926e-24
"tokenizer(""Hello,";1.8976250819197712e-23
my;4.590048771350837e-24
dog;4.848500557278324e-24
is;4.373272473041746e-24
"cute"",";6.1083997723966975e-24
"return_tensors=""pt"")";3.591312191146994e-23
outputs;5.636993075804105e-24
=;5.1076748633225745e-24
model(**inputs);1.3259828974611845e-23
last_hidden_states;1.3494793975178044e-23
=;5.278197961221671e-24
outputs.last_hidden_state;8.329472583776808e-24
***;4.094052507806038e-24
