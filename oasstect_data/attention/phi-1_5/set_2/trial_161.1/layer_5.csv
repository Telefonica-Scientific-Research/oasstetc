text;attention
The;0.9999998839438967
easiest;1.660882735088244e-26
way;1.600482984597375e-26
to;1.607788498557366e-26
import;2.2664309297602862e-26
the;1.6601214793629776e-26
BERT;4.467891771944497e-26
language;2.0132233176772627e-26
model;2.2416647825273005e-26
into;1.9942205335769784e-26
python;1.6106217155091723e-26
for;1.768646414087312e-26
use;1.610889867323814e-26
with;1.5627099329212195e-26
PyTorch;1.9920320375558182e-26
is;1.7512798751827237e-26
using;1.7334340578264524e-26
the;1.443530143860733e-26
Hugging;1.9971001344211108e-26
Face;1.5750932964276624e-26
Transformer's;2.695813341120514e-26
library,;1.773855443704156e-26
which;1.4173563886610607e-26
has;1.3838718656628438e-26
built;1.3386751710317668e-26
in;1.3757708783034957e-26
methods;1.4485661605076137e-26
for;1.4334379054466422e-26
pre-training,;2.4785897494045183e-26
inference,;1.5163776202299336e-26
and;1.2795908346628216e-26
deploying;1.429350186300092e-26
BERT.;1.160561032214556e-07
â€˜**;6.987399965943592e-26
from;2.086209218665968e-26
transformers;1.884519618279703e-26
import;2.0552546213680805e-26
AutoTokenizer,;5.821968264196249e-26
BertModel;2.3117147726465377e-26
import;2.9757970656047146e-26
torch;1.5520339871797606e-26
tokenizer;2.0514705700232926e-26
=;3.865353623686046e-26
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";3.7792359165913186e-25
model;1.8035925117640497e-26
=;3.4384003154670363e-26
"BertModel.from_pretrained(""bert-base-uncased"")";8.380162080881963e-26
inputs;1.780248297813672e-26
=;2.3805008051307032e-26
"tokenizer(""Hello,";3.988683506891553e-26
my;1.348742699677561e-26
dog;1.435952159014965e-26
is;1.287342892840052e-26
"cute"",";1.5420915829940288e-26
"return_tensors=""pt"")";2.2382022900168815e-26
outputs;1.7149954345911878e-26
=;1.7473961294838937e-26
model(**inputs);2.8079190469766073e-26
last_hidden_states;3.184882742496964e-26
=;1.592322157048756e-26
outputs.last_hidden_state;1.828477276071113e-26
***;1.2071941640556163e-26
