text;attention
The;0.9999999987648021
easiest;5.915207205536255e-27
way;5.541462680348762e-27
to;4.6640578311963894e-27
import;6.354794558239641e-27
the;4.55046327308475e-27
BERT;7.302358665727087e-27
language;5.801088219785425e-27
model;4.951211788043427e-27
into;4.549975629928342e-27
python;4.2804929731107795e-27
for;4.2227548043889584e-27
use;4.435526595753888e-27
with;4.094562939888323e-27
PyTorch;5.631078760224558e-27
is;4.592890017732283e-27
using;4.508399825842565e-27
the;3.749885173762484e-27
Hugging;5.4718820005851945e-27
Face;4.7053819458305554e-27
Transformer's;6.64111322835875e-27
library,;5.337696361807991e-27
which;3.527743592253206e-27
has;3.533209899110255e-27
built;3.668081610651481e-27
in;4.031913809341772e-27
methods;3.965225874720098e-27
for;3.794831554746058e-27
pre-training,;5.891485824394701e-27
inference,;4.0383413200217035e-27
and;3.484762363126891e-27
deploying;3.951736034030446e-27
BERT.;1.2351980052940002e-09
â€˜**;4.752053422707284e-26
from;4.9471747829552925e-27
transformers;4.748785116865718e-27
import;4.975094742125635e-27
AutoTokenizer,;1.5355830377744005e-26
BertModel;5.456124230671805e-27
import;4.9637088410737295e-27
torch;4.0656105529671965e-27
tokenizer;4.815126888143698e-27
=;5.443644593534458e-27
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.1481826712194904e-25
model;3.794345334833408e-27
=;5.1072999293361524e-27
"BertModel.from_pretrained(""bert-base-uncased"")";3.866319967064142e-26
inputs;3.79044878043914e-27
=;5.614536244409002e-27
"tokenizer(""Hello,";1.1878315398273439e-26
my;3.569602026703092e-27
dog;3.836685958110273e-27
is;3.469181447015063e-27
"cute"",";4.447300272990546e-27
"return_tensors=""pt"")";7.735392520870019e-27
outputs;3.6613996140401855e-27
=;4.491183811890736e-27
model(**inputs);8.715658200384024e-27
last_hidden_states;7.983675393145991e-27
=;3.859375264926788e-27
outputs.last_hidden_state;4.9374062877438495e-27
***;3.076172613143887e-27
