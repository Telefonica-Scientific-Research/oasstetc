text;attention
The;0.9999972246366403
easiest;9.806744084255497e-28
way;9.194577039188096e-28
to;9.424602099018806e-28
import;1.2091345621145045e-27
the;9.192724989246661e-28
BERT;2.2694850830288897e-27
language;9.526025272328857e-28
model;1.3403270783346505e-27
into;9.48597282425752e-28
python;9.712194890600558e-28
for;8.977125109902752e-28
use;8.386903326942038e-28
with;8.590153035787244e-28
PyTorch;1.4076544150415158e-27
is;9.232220625459835e-28
using;9.318260064914435e-28
the;8.81940938976852e-28
Hugging;1.317394107553762e-27
Face;9.662157745602384e-28
Transformer's;1.852228613091232e-27
library,;1.0397618631703643e-27
which;8.938668431338077e-28
has;8.597165226520396e-28
built;8.55389872061539e-28
in;8.989081498432568e-28
methods;9.184846680846314e-28
for;9.394671933251421e-28
pre-training,;1.8645840635332093e-27
inference,;1.008057011389193e-27
and;7.9734470869711725e-28
deploying;8.84945576012114e-28
BERT.;2.775363359683203e-06
â€˜**;3.254245232695785e-26
from;9.96110569404512e-28
transformers;1.0749663350190887e-27
import;9.437907966439421e-28
AutoTokenizer,;1.6590309502417692e-27
BertModel;1.23161036388107e-27
import;1.1418911683299258e-27
torch;1.0575462686793791e-27
tokenizer;1.2721007084552955e-27
=;9.599983046905677e-28
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";8.53661483982557e-27
model;1.0105635816319415e-27
=;9.532494373077303e-28
"BertModel.from_pretrained(""bert-base-uncased"")";2.769730327235914e-27
inputs;1.0857319924824165e-27
=;8.75901972055203e-28
"tokenizer(""Hello,";1.97700629746163e-27
my;8.002339233956718e-28
dog;8.527249815939508e-28
is;7.972704892830128e-28
"cute"",";8.270007759504185e-28
"return_tensors=""pt"")";1.830706461970042e-27
outputs;9.931903907017457e-28
=;8.969496263181801e-28
model(**inputs);1.443255105071187e-27
last_hidden_states;1.2262605714910122e-27
=;8.191395516568057e-28
outputs.last_hidden_state;1.0229855572383285e-27
***;7.770364842380434e-28
