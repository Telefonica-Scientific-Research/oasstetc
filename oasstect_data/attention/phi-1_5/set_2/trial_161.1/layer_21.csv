text;attention
The;0.9985602826228354
easiest;1.603437084657004e-26
way;1.4419135710681956e-26
to;1.632588310461861e-26
import;2.0642436374442674e-26
the;1.5189496376572726e-26
BERT;5.38054000320036e-26
language;1.54879198256806e-26
model;1.769813908817855e-26
into;1.497066969921847e-26
python;1.5068644562735693e-26
for;1.550903307230509e-26
use;1.3490049490804554e-26
with;1.552105737749609e-26
PyTorch;1.9549048717708025e-26
is;1.6488642813288925e-26
using;1.5809852620388035e-26
the;1.545324080974844e-26
Hugging;2.137022027490065e-26
Face;1.802625634877807e-26
Transformer's;3.617427027690825e-26
library,;1.6710425511011203e-26
which;1.431761993045574e-26
has;1.492862021494107e-26
built;1.29912941733551e-26
in;1.398460179348186e-26
methods;1.456728557309902e-26
for;1.5932628098896465e-26
pre-training,;2.552720560035477e-26
inference,;1.7530287267613388e-26
and;1.29309340803223e-26
deploying;1.5217757559775598e-26
BERT.;0.0014397173771645386
â€˜**;8.628651947295665e-26
from;1.6205754026750158e-26
transformers;1.7771663033267448e-26
import;1.7588815856253143e-26
AutoTokenizer,;3.1701895460834635e-26
BertModel;2.0797085928050996e-26
import;1.8689553413771687e-26
torch;1.5702389354538525e-26
tokenizer;1.8848355162768035e-26
=;1.49782940729601e-26
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.3767905043524327e-25
model;1.5375719594535297e-26
=;1.416661984833671e-26
"BertModel.from_pretrained(""bert-base-uncased"")";4.9111064568952294e-26
inputs;1.5959638842853617e-26
=;1.42158801934274e-26
"tokenizer(""Hello,";2.90203231239583e-26
my;1.416691940110662e-26
dog;1.4695560966027005e-26
is;1.3645842013181145e-26
"cute"",";1.5364900875501603e-26
"return_tensors=""pt"")";3.92163319210373e-26
outputs;1.4686678719438218e-26
=;1.3451096000747928e-26
model(**inputs);2.2479987036518799e-26
last_hidden_states;2.2580292845780736e-26
=;1.2840009268957555e-26
outputs.last_hidden_state;1.7354957595767252e-26
***;1.2880641873442686e-26
