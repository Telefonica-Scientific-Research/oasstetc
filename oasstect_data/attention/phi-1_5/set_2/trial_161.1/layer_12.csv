text;attention
The;0.9999988629886394
easiest;9.514867711575291e-26
way;8.542735182799341e-26
to;8.465535459844638e-26
import;1.298393977135476e-25
the;9.875278091174544e-26
BERT;2.3994838060728075e-25
language;1.1610094211529595e-25
model;2.169597069777748e-25
into;1.2050443546491083e-25
python;1.1078529989320468e-25
for;1.145280678983411e-25
use;8.751068195804503e-26
with;9.535069608914387e-26
PyTorch;1.6370751515817648e-25
is;1.5107086596141303e-25
using;1.0757135933807875e-25
the;9.585154445679217e-26
Hugging;1.5226615261321645e-25
Face;1.0574682384165163e-25
Transformer's;2.1549731803179836e-25
library,;1.1621548563280268e-25
which;8.810987356253403e-26
has;9.058971709099605e-26
built;7.915585252812576e-26
in;8.588174620030557e-26
methods;8.844728298323438e-26
for;9.46433156983912e-26
pre-training,;1.659020943998342e-25
inference,;1.4158021492042715e-25
and;8.362178468893177e-26
deploying;9.478378653119352e-26
BERT.;1.1370113606425749e-06
â€˜**;1.884699426620896e-24
from;1.072117303313977e-25
transformers;1.1516445813749792e-25
import;1.0858070032557621e-25
AutoTokenizer,;3.2928182915584985e-25
BertModel;1.587427548013549e-25
import;1.2278883327195555e-25
torch;9.484329500694563e-26
tokenizer;1.4668755737478071e-25
=;1.181785676911444e-25
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";3.8903963177096874e-24
model;1.0844302651754093e-25
=;1.0166903209970506e-25
"BertModel.from_pretrained(""bert-base-uncased"")";7.260532503812485e-25
inputs;1.2604178796525786e-25
=;1.0878639760575459e-25
"tokenizer(""Hello,";2.241152420469239e-25
my;8.265964988290273e-26
dog;8.761412371690453e-26
is;8.152352069493155e-26
"cute"",";9.637521598896755e-26
"return_tensors=""pt"")";2.5757275228742914e-25
outputs;1.0193473162175066e-25
=;9.597730866048051e-26
model(**inputs);1.7856587932664618e-25
last_hidden_states;1.8694057866417774e-25
=;9.017541629582803e-26
outputs.last_hidden_state;1.1357255371681034e-25
***;7.683940864412343e-26
