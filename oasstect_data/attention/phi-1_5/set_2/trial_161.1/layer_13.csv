text;attention
The;0.9999904894719045
easiest;7.790561851261712e-25
way;6.6095765301223705e-25
to;6.4247326423389905e-25
import;1.1698202137944655e-24
the;7.664064655304071e-25
BERT;3.243721006604911e-24
language;9.494390315398252e-25
model;1.8153759639754483e-24
into;9.172992729116005e-25
python;7.023824432084378e-25
for;7.528728515474175e-25
use;6.077636589437287e-25
with;7.139267910661658e-25
PyTorch;1.030557714318494e-24
is;7.775734307545512e-25
using;8.183284343601746e-25
the;6.894199019675775e-25
Hugging;1.211346690962543e-24
Face;8.1995145937251025e-25
Transformer's;2.0079422633909683e-24
library,;8.26297726624711e-25
which;6.086315898956026e-25
has;6.400908795161034e-25
built;5.4428036519315135e-25
in;6.417745803137174e-25
methods;6.637837919067861e-25
for;7.711995729047922e-25
pre-training,;1.3371789310599351e-24
inference,;1.0226451856297586e-24
and;5.823431181779523e-25
deploying;6.711675687691595e-25
BERT.;9.510528095450955e-06
â€˜**;6.266254054853177e-24
from;7.232813914276013e-25
transformers;7.925504359302534e-25
import;7.076917071634896e-25
AutoTokenizer,;2.0287695422364466e-24
BertModel;1.2900933142756731e-24
import;8.782541171601352e-25
torch;6.357715125423503e-25
tokenizer;1.0827985707299518e-24
=;8.526206598060314e-25
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.1530989939806782e-23
model;7.755861130892131e-25
=;7.9980538463444e-25
"BertModel.from_pretrained(""bert-base-uncased"")";6.703135560287643e-24
inputs;7.5021303487935275e-25
=;6.980873386842738e-25
"tokenizer(""Hello,";1.349436350402748e-24
my;5.527560497384113e-25
dog;5.797565627038106e-25
is;5.4478319533730825e-25
"cute"",";6.30135855848569e-25
"return_tensors=""pt"")";2.4263351774186944e-24
outputs;6.338052637625196e-25
=;6.02729387458423e-25
model(**inputs);1.2442796792993656e-24
last_hidden_states;1.0843101875306658e-24
=;5.918501790737319e-25
outputs.last_hidden_state;7.460847023341352e-25
***;5.245499858237001e-25
