text;attention
The;0.9999956297657228
easiest;3.985673576559668e-27
way;3.8578497791705785e-27
to;3.769123004037773e-27
import;5.9063098985967505e-27
the;3.738535575151067e-27
BERT;1.0249298770082404e-26
language;4.579781591006385e-27
model;4.8068128115281426e-27
into;3.783787934665265e-27
python;4.252322406967318e-27
for;3.595550689930962e-27
use;3.393817597603459e-27
with;3.481614143198976e-27
PyTorch;6.046942997084576e-27
is;3.6334547353756886e-27
using;3.684929904634596e-27
the;3.544625678920779e-27
Hugging;7.616585000559172e-27
Face;4.066354135680878e-27
Transformer's;6.224974150108369e-27
library,;3.933078684750898e-27
which;3.540917927108716e-27
has;3.461294288920208e-27
built;3.332571007331046e-27
in;3.471633516735637e-27
methods;3.535364202267114e-27
for;3.5757167371544995e-27
pre-training,;5.975848440132448e-27
inference,;4.2678228163104555e-27
and;3.3477593341166016e-27
deploying;3.804136440708227e-27
BERT.;4.370234277178136e-06
â€˜**;1.5712473616184593e-25
from;4.420173357171151e-27
transformers;4.286046496970952e-27
import;3.928319155668703e-27
AutoTokenizer,;1.1497182916151386e-26
BertModel;6.254581136579989e-27
import;4.885615563502244e-27
torch;4.0707335169718796e-27
tokenizer;5.469613758097594e-27
=;3.7768082649900644e-27
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";5.896185466678579e-26
model;3.849268678080157e-27
=;3.5651836054818885e-27
"BertModel.from_pretrained(""bert-base-uncased"")";1.3869223806500284e-26
inputs;4.033973451875157e-27
=;3.634289440877367e-27
"tokenizer(""Hello,";7.368338761006133e-27
my;3.406407890463097e-27
dog;3.8078762125957824e-27
is;3.513288677387719e-27
"cute"",";4.4900432932323505e-27
"return_tensors=""pt"")";9.181022427539671e-27
outputs;3.797219103306563e-27
=;3.533773908974933e-27
model(**inputs);5.5763375553135054e-27
last_hidden_states;5.439222679150795e-27
=;3.416035280875568e-27
outputs.last_hidden_state;4.543971749644695e-27
***;3.126009166307017e-27
