text;attention
The;5.638687462826671e-06
easiest;2.291702034327335e-06
way;7.473976886066002e-07
to;3.2629590248211806e-06
import;4.487386228743806e-06
the;2.0895870817433594e-06
BERT;2.5497448643215926e-06
language;7.672192126417975e-07
model;7.436088630056686e-07
into;9.283236856764509e-07
python;5.7061911041905295e-06
for;1.2049573239309688e-06
use;5.519269917533498e-07
with;1.6343861830708854e-06
PyTorch;1.5775611286800933e-05
is;8.476567335627441e-07
using;7.383215438330424e-07
the;1.1498179229555285e-06
Hugging;3.4861569424056306e-06
Face;6.944964375476633e-07
Transformer's;4.2002858287705595e-06
library,;3.5330353279578564e-06
which;5.077456323278305e-07
has;5.563100205699233e-07
built;6.317080429017719e-07
in;7.61390426798453e-07
methods;4.828309196916562e-07
for;7.803691228141569e-07
pre-training,;1.882378267375844e-05
inference,;2.8240044644370665e-06
and;7.095929913401089e-07
deploying;8.067196795667658e-07
BERT.;5.345559946354631e-06
â€˜**;2.717578384122294e-06
from;6.428619442428764e-07
transformers;7.999357413409576e-07
import;1.1055397506210132e-06
AutoTokenizer,;1.3288549503043947e-05
BertModel;1.4373308682801133e-06
import;9.28862558298819e-07
torch;1.2343212308353233e-06
tokenizer;8.669844459904745e-07
=;1.189286774645927e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.9092932610401994
model;3.629996350763165e-07
=;9.283163818279188e-07
"BertModel.from_pretrained(""bert-base-uncased"")";0.09051824397654396
inputs;4.169461047608681e-07
=;8.209952285695593e-07
"tokenizer(""Hello,";1.9433870755412648e-05
my;3.577254487442068e-07
dog;3.544213039025797e-07
is;4.2631709383211427e-07
"cute"",";8.683452713351584e-07
"return_tensors=""pt"")";2.953320342934863e-05
outputs;3.767585887693243e-07
=;6.557848882090678e-07
model(**inputs);6.6074552924336655e-06
last_hidden_states;3.2957537500950913e-06
=;4.4062430337902213e-07
outputs.last_hidden_state;3.8952217732671784e-06
***;2.495191123174723e-07
