text;attention
The;1.1598327616808928e-06
easiest;1.7433686857473817e-09
way;1.1762658976886688e-09
to;7.637262344315758e-09
import;1.7016180772498924e-09
the;4.802825228223084e-05
BERT;2.897257922728204e-09
language;9.137628936803556e-10
model;9.419103165658481e-10
into;1.1808909991884324e-09
python;7.42585744639798e-10
for;9.829545179674188e-10
use;7.66254788621256e-10
with;1.5714836343089344e-09
PyTorch;2.8857256411578797e-09
is;2.3772066961929162e-09
using;1.0188811004567119e-09
the;9.175683926084953e-08
Hugging;2.199706349777363e-09
Face;1.010700802280267e-09
Transformer's;2.2795926783096195e-09
library,;5.9351127256393006e-08
which;9.67242242300092e-10
has;1.0548270800260595e-09
built;8.590626640786799e-10
in;7.819081056787639e-10
methods;7.251709219895102e-10
for;1.1007173482406217e-09
pre-training,;2.0722852781814304e-08
inference,;4.23334518014052e-09
and;1.0219650144579638e-09
deploying;7.838617219875862e-10
BERT.;0.9999215479602079
â€˜**;1.214296538896718e-08
from;7.500291553861832e-10
transformers;8.963283528675974e-10
import;8.003002001645125e-10
AutoTokenizer,;1.0967601584435869e-08
BertModel;9.775205265706716e-10
import;7.495225034926327e-10
torch;7.266574102972383e-10
tokenizer;1.0180309917353698e-09
=;1.4311576568128123e-09
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.451512762879325e-05
model;7.009485313258369e-10
=;1.2537443245323406e-09
"BertModel.from_pretrained(""bert-base-uncased"")";4.447412410537819e-06
inputs;7.441807606645768e-10
=;1.183890878650451e-09
"tokenizer(""Hello,";8.477229565611846e-09
my;7.42723278039865e-10
dog;7.185548447639179e-10
is;7.157794253470859e-10
"cute"",";1.6344456697747782e-09
"return_tensors=""pt"")";1.508566781593981e-08
outputs;7.249707597231307e-10
=;1.1544962671091144e-09
model(**inputs);8.128821043604211e-09
last_hidden_states;3.4936036784710264e-09
=;8.963129932159429e-10
outputs.last_hidden_state;7.433440681328064e-09
***;4.794356069427358e-10
