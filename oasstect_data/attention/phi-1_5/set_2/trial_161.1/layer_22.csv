text;attention
The;0.999939187530543
easiest;7.361580821622967e-26
way;7.482436732873575e-26
to;7.94829358968856e-26
import;9.439636730352115e-26
the;7.275721917778701e-26
BERT;1.4237479283770582e-25
language;6.761166756700299e-26
model;8.214900388040447e-26
into;7.921387535688642e-26
python;7.557183465914048e-26
for;7.629041852196236e-26
use;6.265851135740378e-26
with;7.865442259488106e-26
PyTorch;1.1367770296350667e-25
is;7.095115995818716e-26
using;8.78851859022997e-26
the;8.425091744946392e-26
Hugging;1.5619640389441758e-25
Face;8.724838953497032e-26
Transformer's;1.7149973449723934e-25
library,;8.269953674513603e-26
which;6.679826831147725e-26
has;6.960360569468735e-26
built;6.288828752022668e-26
in;6.716674427694374e-26
methods;6.782948692149385e-26
for;8.405163952802884e-26
pre-training,;1.8900603075591148e-25
inference,;8.492531352855287e-26
and;6.042976077211721e-26
deploying;6.848214866520554e-26
BERT.;6.081246945703511e-05
â€˜**;1.282035917703129e-24
from;6.858204830509764e-26
transformers;7.707360988455577e-26
import;7.926237307266355e-26
AutoTokenizer,;1.664068041192826e-25
BertModel;8.492852159561229e-26
import;8.932238466279689e-26
torch;7.287709660242567e-26
tokenizer;1.004144339303414e-25
=;7.614239655369163e-26
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";9.122250481822993e-25
model;6.699624393454817e-26
=;6.625137180840355e-26
"BertModel.from_pretrained(""bert-base-uncased"")";3.2072062708301105e-25
inputs;6.199628774115637e-26
=;6.747965291209032e-26
"tokenizer(""Hello,";1.8067758799066217e-25
my;6.521403127583385e-26
dog;6.795467103238022e-26
is;6.121948571868651e-26
"cute"",";7.640621256095195e-26
"return_tensors=""pt"")";2.769895322449417e-25
outputs;6.344584742240747e-26
=;6.245512898281927e-26
model(**inputs);1.0703184241640568e-25
last_hidden_states;1.3764683773723654e-25
=;5.958752637596305e-26
outputs.last_hidden_state;1.1726296997539417e-25
***;5.903412125341988e-26
