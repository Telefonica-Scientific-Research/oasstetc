text;attention
The;0.9999996337175139
easiest;7.028701732389502e-24
way;6.592882631984808e-24
to;6.090913493857067e-24
import;1.1604353288912345e-23
the;6.2806110820537786e-24
BERT;1.5854419539769946e-23
language;6.546864535915709e-24
model;8.639933600737998e-24
into;6.868839826154175e-24
python;5.6377378124778934e-24
for;5.666868447779285e-24
use;5.5259312952177415e-24
with;5.522231719704265e-24
PyTorch;7.414736037827499e-24
is;6.3732718853713596e-24
using;6.145621632915407e-24
the;5.3509289043708755e-24
Hugging;9.095974316947323e-24
Face;9.760807063744929e-24
Transformer's;1.3138040818458581e-23
library,;7.565708053885158e-24
which;5.129584519651345e-24
has;5.075557228753526e-24
built;5.088034374449065e-24
in;5.223388479683254e-24
methods;5.7532515750775066e-24
for;5.681889436387468e-24
pre-training,;1.066608465943353e-23
inference,;5.529562199053728e-24
and;4.40992654479039e-24
deploying;4.986007007359432e-24
BERT.;3.662824861662107e-07
â€˜**;3.4807550777381217e-23
from;8.669981264443304e-24
transformers;1.0616770070180756e-23
import;6.769362924655559e-24
AutoTokenizer,;2.2344755509335404e-23
BertModel;9.306846861423765e-24
import;7.503373093022045e-24
torch;5.3380579484247675e-24
tokenizer;7.715595309294473e-24
=;8.248451814502594e-24
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.850563385291574e-22
model;5.271079128425833e-24
=;8.112111708008404e-24
"BertModel.from_pretrained(""bert-base-uncased"")";1.578034903433034e-22
inputs;6.374939951865585e-24
=;8.371590532536655e-24
"tokenizer(""Hello,";2.871577974402748e-23
my;4.6612961638026486e-24
dog;5.2140256008970526e-24
is;4.770729053032765e-24
"cute"",";5.955785083766431e-24
"return_tensors=""pt"")";1.375293013682128e-23
outputs;5.231399901054434e-24
=;7.183291961031387e-24
model(**inputs);1.7381501726574302e-23
last_hidden_states;1.3537805858608353e-23
=;5.351666823438528e-24
outputs.last_hidden_state;8.743221206312204e-24
***;4.0366268155951625e-24
