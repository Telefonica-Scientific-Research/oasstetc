text;attention
The;0.9985590345234001
easiest;8.340920300678456e-06
way;6.632851460826718e-06
to;5.391061632970011e-06
import;8.177520021084558e-06
the;6.2894443501324205e-06
BERT;6.666567218889772e-05
language;7.472211067632858e-06
model;1.4554142128506137e-05
into;1.4373430944603681e-05
python;1.1884288345390074e-05
for;9.190089713673986e-06
use;6.563977865598644e-06
with;5.780070254551479e-06
PyTorch;1.9359356268311113e-05
is;1.2430078087494663e-05
using;6.649886341419069e-06
the;6.384265670082117e-06
Hugging;1.4569767145327906e-05
Face;7.827997817662477e-06
Transformer's;1.784095674174586e-05
library,;9.619146960394226e-06
which;5.529977611269911e-06
has;5.447074704739702e-06
built;4.7469703618639355e-06
in;6.705054548691819e-06
methods;6.695246067222055e-06
for;6.134457015680845e-06
pre-training,;1.8882490644532283e-05
inference,;9.547316965502514e-06
and;5.305889411295975e-06
deploying;6.82356125475772e-06
BERT.;1.4768133393061425e-05
â€˜**;7.778856371319089e-06
from;1.0449158351435023e-05
transformers;5.769909424688441e-06
import;6.352063593183005e-06
AutoTokenizer,;5.915366218139107e-05
BertModel;1.3029301819798555e-05
import;9.512119784381768e-06
torch;7.66600221570639e-06
tokenizer;1.4831249282519092e-05
=;1.0117792100622952e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0007335421679382934
model;7.0525716566161e-06
=;6.781236798853391e-06
"BertModel.from_pretrained(""bert-base-uncased"")";4.272579799139188e-05
inputs;9.004342268462298e-06
=;7.142819628538419e-06
"tokenizer(""Hello,";3.0223324550192848e-05
my;4.903438096904859e-06
dog;5.4040562828545e-06
is;4.718306964942443e-06
"cute"",";6.842305341990041e-06
"return_tensors=""pt"")";2.3960385405242457e-05
outputs;6.02951274537825e-06
=;5.415368661853263e-06
model(**inputs);1.5850026599905024e-05
last_hidden_states;1.3341433660164605e-05
=;5.64200101650374e-06
outputs.last_hidden_state;6.656998759229975e-06
***;4.485959821734071e-06
