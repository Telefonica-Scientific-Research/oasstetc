text;attention
The;0.9936815373193074
easiest;3.640941405192919e-05
way;3.191452288168589e-05
to;2.9175707687324768e-05
import;4.469271371529359e-05
the;3.2282576010567355e-05
BERT;0.0002673407132524364
language;4.389340082559509e-05
model;7.134237860739992e-05
into;7.327516760178703e-05
python;6.842232781292214e-05
for;4.546281978481997e-05
use;3.6061148264410114e-05
with;3.216106168647407e-05
PyTorch;0.00014127833029348204
is;5.6812164802837464e-05
using;3.877916865509474e-05
the;3.0420457519051416e-05
Hugging;6.033092882062494e-05
Face;4.630513657578509e-05
Transformer's;0.0001365905109660242
library,;7.358602530868163e-05
which;3.071663989060895e-05
has;3.0210294846904118e-05
built;2.6227818463345653e-05
in;3.288631194744335e-05
methods;3.098361840981669e-05
for;2.9474924954072447e-05
pre-training,;8.556382505822689e-05
inference,;6.488121692849904e-05
and;2.8119171545514874e-05
deploying;4.2160979319126366e-05
BERT.;0.00011887250597386865
â€˜**;4.407622803596581e-05
from;4.2372128169342585e-05
transformers;4.3768861175567074e-05
import;4.069355487058652e-05
AutoTokenizer,;0.00047516429608898924
BertModel;0.0001072754211654266
import;4.6058060398634274e-05
torch;4.184220458703764e-05
tokenizer;9.24317768866377e-05
=;4.958932199633595e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.002442756313174401
model;4.1120896394820444e-05
=;3.5693740126229406e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.00020129500159670648
inputs;4.857151965340614e-05
=;3.079204678304445e-05
"tokenizer(""Hello,";0.00010421888911144621
my;2.7329093900834786e-05
dog;3.0691384488024706e-05
is;2.7720718047693396e-05
"cute"",";3.8589500181605714e-05
"return_tensors=""pt"")";0.00010978152385724025
outputs;3.4191102506814884e-05
=;2.8025500847128707e-05
model(**inputs);6.013399351700572e-05
last_hidden_states;6.373586738364483e-05
=;3.0574793419885614e-05
outputs.last_hidden_state;3.727940799372279e-05
***;2.6055551872700652e-05
