text;attention
The;0.2007354580145583
easiest;0.006395237057210413
way;0.005957905102437688
to;0.00512169603351543
import;0.009974946799518287
the;0.0056676727216129165
BERT;0.03739563152844009
language;0.006889808191856995
model;0.01118304325633529
into;0.00877778946628691
python;0.011049548328260491
for;0.006859256978476577
use;0.006357730150811336
with;0.006348037368022713
PyTorch;0.013525801506945864
is;0.0072478119569871135
using;0.006506917174645995
the;0.0052957068977882065
Hugging;0.009336749296543016
Face;0.007419078587380426
Transformer's;0.021709136944119457
library,;0.009575652085320515
which;0.005374484884672772
has;0.005206315862489578
built;0.004564625997090725
in;0.0055520066710491485
methods;0.006160979535809895
for;0.0056402964445666495
pre-training,;0.01150918483001384
inference,;0.007427870259364671
and;0.004768161482533173
deploying;0.006774936120481315
BERT.;0.00972285945097181
â€˜**;0.008443528222449023
from;0.010050612165733254
transformers;0.008333564977045077
import;0.0071677147123289636
AutoTokenizer,;0.025504956147007866
BertModel;0.013254311493896621
import;0.006906782121081442
torch;0.006059709397419653
tokenizer;0.007282233375380658
=;0.010076297414324456
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2276103427440017
model;0.00745868803107987
=;0.0071914128946331595
"BertModel.from_pretrained(""bert-base-uncased"")";0.055596068040176694
inputs;0.007427023631185324
=;0.0054846595148709395
"tokenizer(""Hello,";0.01690375932052466
my;0.004606278838445722
dog;0.005518281497829857
is;0.004672610585220966
"cute"",";0.006384199636787939
"return_tensors=""pt"")";0.01688121429378743
outputs;0.005680263033943802
=;0.0048879610525832725
model(**inputs);0.01067560681927523
last_hidden_states;0.011857735186713209
=;0.0050341077949647055
outputs.last_hidden_state;0.006711956074050512
***;0.004305783997140064
