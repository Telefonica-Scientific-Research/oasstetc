text;attention
The;0.8463467665939588
easiest;0.0006819508172714726
way;0.0006372523811408609
to;0.0005544139429941862
import;0.0013281280644583673
the;0.0006611060387621283
BERT;0.01961937847277859
language;0.0005838816731414942
model;0.001390712410599647
into;0.0008312202400974267
python;0.0009840501930271175
for;0.0006060649705194529
use;0.0005902225464331272
with;0.0006175219498346904
PyTorch;0.0036369319152512757
is;0.0008697304644908772
using;0.0007918118491445767
the;0.0006771327630896065
Hugging;0.0012269483849237871
Face;0.0009125117606504318
Transformer's;0.003980653305685398
library,;0.0009408541569806148
which;0.0005664115476730887
has;0.000526240730125629
built;0.0004864574795274584
in;0.0005448928778225931
methods;0.0005652952860991415
for;0.000594269853166448
pre-training,;0.0013623151765190867
inference,;0.0007896144120282503
and;0.000489827590586361
deploying;0.0006801237078559138
BERT.;0.0016101783299699451
â€˜**;0.0009050661070603646
from;0.0017281722425701908
transformers;0.001351594219022696
import;0.0009154959473375847
AutoTokenizer,;0.006153422215112496
BertModel;0.0018857500796476015
import;0.0013295851383608739
torch;0.0007434649997824429
tokenizer;0.001083796175546591
=;0.0009977575475919613
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06088085545321561
model;0.0007530340624510865
=;0.0006316061387576081
"BertModel.from_pretrained(""bert-base-uncased"")";0.008580821060355907
inputs;0.0007846445968530993
=;0.0006552150931515122
"tokenizer(""Hello,";0.002402202930581394
my;0.0005092317560673653
dog;0.0005827916299321478
is;0.0004670281033816571
"cute"",";0.0006522866371827606
"return_tensors=""pt"")";0.0020993084952263853
outputs;0.0006552812342063958
=;0.0005482420689640649
model(**inputs);0.0019490545908833088
last_hidden_states;0.0016384355298823478
=;0.0006620084175620774
outputs.last_hidden_state;0.00131240959442161
***;0.0004565660482830418
