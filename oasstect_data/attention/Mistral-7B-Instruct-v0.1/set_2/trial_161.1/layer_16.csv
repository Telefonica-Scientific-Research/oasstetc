text;attention
The;0.9990725442285616
easiest;2.798032534942217e-06
way;2.646311516136059e-06
to;1.8174172991438795e-06
import;2.7811013622860308e-06
the;1.935010234362968e-06
BERT;1.7000645154793795e-05
language;2.041222868604849e-06
model;3.893242685441143e-06
into;3.281914293346125e-06
python;3.3241053518190085e-06
for;2.4034930343154138e-06
use;1.7914274422561319e-06
with;1.866314530572203e-06
PyTorch;7.845911809612425e-06
is;3.2757108780685217e-06
using;2.429538589638585e-06
the;2.500457940890359e-06
Hugging;6.508422762381574e-06
Face;2.755667121854966e-06
Transformer's;9.197530908889341e-06
library,;3.921345982312211e-06
which;1.7961560626063806e-06
has;1.6934920035239774e-06
built;1.6322582919006432e-06
in;1.9333378416409457e-06
methods;1.984251598479494e-06
for;1.91160242008466e-06
pre-training,;7.0566212496123775e-06
inference,;3.4960465867413676e-06
and;1.5846745818723163e-06
deploying;2.101619612934602e-06
BERT.;4.898049007474751e-06
â€˜**;2.9768863834738523e-06
from;4.099259159579232e-06
transformers;2.598456399009516e-06
import;2.3827907626497577e-06
AutoTokenizer,;2.936518949270627e-05
BertModel;4.000406285704946e-06
import;3.8196121731962065e-06
torch;2.6705255106677907e-06
tokenizer;5.143717376858264e-06
=;5.003580522384422e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0006600550336882716
model;2.2465402710581904e-06
=;2.1276106807056453e-06
"BertModel.from_pretrained(""bert-base-uncased"")";2.105256265655624e-05
inputs;3.225830020358359e-06
=;2.0295813920043074e-06
"tokenizer(""Hello,";1.4660017013258381e-05
my;1.6354875513343717e-06
dog;1.655003362752814e-06
is;1.519627713446693e-06
"cute"",";2.2855784525256796e-06
"return_tensors=""pt"")";1.58783133073314e-05
outputs;2.070598154170747e-06
=;1.7630914110198612e-06
model(**inputs);6.076560843339789e-06
last_hidden_states;4.6070793426685895e-06
=;1.9348213238760502e-06
outputs.last_hidden_state;2.9211821640145933e-06
***;1.547894464716254e-06
