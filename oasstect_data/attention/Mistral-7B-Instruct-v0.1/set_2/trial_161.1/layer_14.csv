text;attention
The;0.9965892168624841
easiest;5.5266979746819145e-06
way;4.509116168174468e-06
to;3.6007687426469356e-06
import;5.142770743252019e-06
the;4.059933360276205e-06
BERT;2.6090399601321158e-05
language;4.750440475895702e-06
model;7.88415422761052e-06
into;6.3213382324673645e-06
python;7.762126238369465e-06
for;4.934864963898952e-06
use;3.877273012302086e-06
with;3.6709927450295135e-06
PyTorch;1.833725865533168e-05
is;6.724234297084592e-06
using;5.251640263633071e-06
the;4.674758064169871e-06
Hugging;1.0600731030857991e-05
Face;5.337322556459845e-06
Transformer's;1.937299110145434e-05
library,;8.071728457716941e-06
which;3.82456897757666e-06
has;3.629231966658432e-06
built;3.278730325853961e-06
in;4.355862334611946e-06
methods;4.187344282919786e-06
for;3.579997832883489e-06
pre-training,;1.4859091523168226e-05
inference,;7.689319900801585e-06
and;3.592518567406326e-06
deploying;4.322404828268682e-06
BERT.;9.797569022713721e-06
â€˜**;5.92204704518975e-06
from;6.490013953292224e-06
transformers;5.434186033243327e-06
import;5.580997564474873e-06
AutoTokenizer,;4.7198198484128524e-05
BertModel;8.561261275409792e-06
import;6.323850354443127e-06
torch;4.996810687775574e-06
tokenizer;8.51314642885107e-06
=;1.04569854793136e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.002892614553908934
model;5.286109891791439e-06
=;4.841273443465688e-06
"BertModel.from_pretrained(""bert-base-uncased"")";5.473424804964309e-05
inputs;5.446907767166152e-06
=;4.3480741854834955e-06
"tokenizer(""Hello,";2.917557817060859e-05
my;3.364148436502443e-06
dog;3.5824843053064194e-06
is;3.1518327806298147e-06
"cute"",";4.500562402783328e-06
"return_tensors=""pt"")";1.9012384988248878e-05
outputs;3.825807924125372e-06
=;3.602107484935169e-06
model(**inputs);1.3042359678436985e-05
last_hidden_states;8.521307227128844e-06
=;3.779523530181121e-06
outputs.last_hidden_state;5.8539546328098465e-06
***;3.0042409266129935e-06
