text;attention
The;0.9994233569493154
easiest;6.918189019711218e-07
way;6.814320233759575e-07
to;6.430607565480371e-07
import;9.10352041934571e-07
the;1.0230873354240885e-06
BERT;2.7287121254701995e-06
language;7.407638366172841e-07
model;6.882569336959335e-07
into;7.458986594877683e-07
python;7.109609768981476e-07
for;7.176040497778597e-07
use;4.991592314785745e-07
with;5.866488091296347e-07
PyTorch;1.652885271997648e-06
is;6.726649710390742e-07
using;8.876458473246342e-07
the;9.581117501391097e-07
Hugging;1.7140696501425023e-06
Face;7.385328861840391e-07
Transformer's;5.164909634759205e-06
library,;1.140348331755429e-06
which;5.296337857738922e-07
has;5.375101442327865e-07
built;4.822972065379195e-07
in;6.195996276391054e-07
methods;5.604444341455621e-07
for;5.878930109718471e-07
pre-training,;2.2498903844642207e-06
inference,;8.15576192910204e-07
and;5.025969148672116e-07
deploying;5.657266315433436e-07
BERT.;1.2167562375261658e-06
â€˜**;1.5546617299920915e-06
from;8.900780654729374e-07
transformers;7.572359130135102e-07
import;8.126363383774103e-07
AutoTokenizer,;3.2883376197681336e-06
BertModel;9.466181501360468e-07
import;7.334565077964704e-07
torch;9.7350944860251e-07
tokenizer;1.0130709921570449e-06
=;7.715009901889172e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0004960814576891655
model;4.655228245985819e-07
=;5.299144579171059e-07
"BertModel.from_pretrained(""bert-base-uncased"")";1.7736746897065447e-05
inputs;6.78971622150329e-07
=;5.712957017972358e-07
"tokenizer(""Hello,";4.214166698314668e-06
my;5.207339264405545e-07
dog;5.199780445652351e-07
is;4.957855668470496e-07
"cute"",";6.687842929478927e-07
"return_tensors=""pt"")";2.6546759851204936e-06
outputs;5.418283201369844e-07
=;4.776130640613521e-07
model(**inputs);1.6271108332834882e-06
last_hidden_states;1.830218713376845e-06
=;5.044530613906636e-07
outputs.last_hidden_state;1.3844894707351513e-06
***;4.3334916356520336e-07
