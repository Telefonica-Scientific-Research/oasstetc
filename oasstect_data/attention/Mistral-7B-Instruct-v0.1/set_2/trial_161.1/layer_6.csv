text;attention
The;0.07260310164767207
easiest;0.00981519114833969
way;0.008261023095362594
to;0.007816153584830864
import;0.014414480948909502
the;0.007632198322471391
BERT;0.03128616179846919
language;0.00840383095105657
model;0.007967166461292058
into;0.009147502374721589
python;0.008869633241632043
for;0.0086305824093949
use;0.006768723153682883
with;0.00791649757357035
PyTorch;0.01109467584117012
is;0.014091585351761747
using;0.007857258181682609
the;0.006744669295807768
Hugging;0.014046479447360162
Face;0.009337762410136303
Transformer's;0.02259505477742477
library,;0.009191537225659359
which;0.007298340870411173
has;0.006848576033118448
built;0.005758491578404593
in;0.007232406857816502
methods;0.007134882546290218
for;0.007842244566183095
pre-training,;0.012722135005384822
inference,;0.009086675060966352
and;0.006067114226582731
deploying;0.008019845689520463
BERT.;0.010978510078256816
â€˜**;0.01582368104177679
from;0.01232807661398139
transformers;0.008677824649227362
import;0.009567532491861309
AutoTokenizer,;0.018857012072936646
BertModel;0.011140531905569075
import;0.009646946933358288
torch;0.006600356887778358
tokenizer;0.007813846775741573
=;0.01217671298426075
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.26015074958483614
model;0.007906133587994597
=;0.009134060256793301
"BertModel.from_pretrained(""bert-base-uncased"")";0.08059266085078495
inputs;0.008408173687269828
=;0.00710194916490244
"tokenizer(""Hello,";0.02147313925793154
my;0.006010467377800921
dog;0.006120059533597202
is;0.005604616649098513
"cute"",";0.0068919298026988976
"return_tensors=""pt"")";0.015775108478210217
outputs;0.007005159898813409
=;0.006281441383624469
model(**inputs);0.013145830108568181
last_hidden_states;0.012598170467829949
=;0.006674839904987912
outputs.last_hidden_state;0.007783603479571473
***;0.005228892412850776
