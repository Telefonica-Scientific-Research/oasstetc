text;attention
The;0.9992438848479035
easiest;2.7866174097557356e-06
way;2.1055424810796535e-06
to;1.4464414689262876e-06
import;2.674836925438747e-06
the;1.5620770837739306e-06
BERT;1.1098883913017258e-05
language;1.5340184746791895e-06
model;2.907458602678242e-06
into;3.2074530665620047e-06
python;3.4609461659505106e-06
for;2.4189673753869746e-06
use;1.6320711689087357e-06
with;1.4143625164821391e-06
PyTorch;5.703836525352308e-06
is;8.517077810446386e-06
using;2.488834229619013e-06
the;1.5571946082078289e-06
Hugging;2.558001674280113e-06
Face;1.8310067221391912e-06
Transformer's;1.0609102404065668e-05
library,;4.857951853382577e-06
which;1.6598486317528478e-06
has;1.3674353254683312e-06
built;1.1341598210839375e-06
in;1.813716634584758e-06
methods;1.642008242511055e-06
for;1.62871654939604e-06
pre-training,;5.264337195238678e-06
inference,;3.153393179118003e-06
and;1.3037594333854799e-06
deploying;1.68744919096324e-06
BERT.;6.8568812108048056e-06
â€˜**;4.212726078339019e-06
from;3.18309346765469e-06
transformers;1.9001172090138006e-06
import;2.0241054778519446e-06
AutoTokenizer,;1.4696680057253e-05
BertModel;4.029072172117144e-06
import;2.894456098232141e-06
torch;1.8653873491188634e-06
tokenizer;3.423765582118687e-06
=;6.410031401952076e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0005211180041427402
model;1.870914559916872e-06
=;2.200481829653814e-06
"BertModel.from_pretrained(""bert-base-uncased"")";3.7128445947209825e-05
inputs;2.7693866890887153e-06
=;2.2179216591907093e-06
"tokenizer(""Hello,";1.253662581069759e-05
my;1.1471753405082982e-06
dog;1.2826985824439697e-06
is;1.1356164290434627e-06
"cute"",";1.8581276434493008e-06
"return_tensors=""pt"")";7.4670811865513915e-06
outputs;1.3957678767302627e-06
=;1.356752494343985e-06
model(**inputs);4.084669965400364e-06
last_hidden_states;3.3561855859950645e-06
=;1.4934666199912938e-06
outputs.last_hidden_state;2.122739786840286e-06
***;1.0492671586909546e-06
