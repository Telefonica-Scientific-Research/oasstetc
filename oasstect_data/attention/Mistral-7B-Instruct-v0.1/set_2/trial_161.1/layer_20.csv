text;attention
The;0.9980780636138123
easiest;1.1416369916040798e-05
way;8.961633177870006e-06
to;8.247838894114015e-06
import;1.6435103803287876e-05
the;9.288841377121002e-06
BERT;0.00019670556226205577
language;9.712751277951599e-06
model;1.5619572664991197e-05
into;9.625801538317972e-06
python;1.0875180703431058e-05
for;8.312589553557704e-06
use;9.275998475285267e-06
with;8.180248948675837e-06
PyTorch;2.119700988830902e-05
is;9.978019082449105e-06
using;1.0505078380119798e-05
the;8.934289387934065e-06
Hugging;1.971426167021024e-05
Face;9.928651525319955e-06
Transformer's;4.4728706773286865e-05
library,;1.2762844293000241e-05
which;7.499504508980404e-06
has;7.85940504699205e-06
built;7.953044882012362e-06
in;8.337046754804818e-06
methods;8.12914112084157e-06
for;8.220193824972444e-06
pre-training,;2.3837376372908733e-05
inference,;1.3189723081312806e-05
and;6.963715343495684e-06
deploying;9.506087221655402e-06
BERT.;1.7942485567080652e-05
â€˜**;1.1081269409046935e-05
from;1.1810608286493286e-05
transformers;1.3090269077944701e-05
import;8.669236904936035e-06
AutoTokenizer,;5.061196011828157e-05
BertModel;1.7727711643862537e-05
import;1.0998891399902174e-05
torch;1.5576003203975108e-05
tokenizer;1.9008210344000083e-05
=;9.069425650359652e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0008499642602277433
model;9.297714107350348e-06
=;7.770363961102993e-06
"BertModel.from_pretrained(""bert-base-uncased"")";7.959110014000114e-05
inputs;1.2541118683779197e-05
=;8.648963360898957e-06
"tokenizer(""Hello,";3.910811404504895e-05
my;7.965712496544546e-06
dog;8.25651421608554e-06
is;7.068156078587874e-06
"cute"",";8.760541892195894e-06
"return_tensors=""pt"")";3.464877434017691e-05
outputs;1.073182109478863e-05
=;7.528241185445857e-06
model(**inputs);1.8149620855416918e-05
last_hidden_states;3.082973304337028e-05
=;7.79749749827175e-06
outputs.last_hidden_state;1.879678694426962e-05
***;6.993688659158346e-06
