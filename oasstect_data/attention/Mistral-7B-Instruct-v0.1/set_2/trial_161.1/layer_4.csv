text;attention
The;0.02712419621948827
easiest;0.013175879930817123
way;0.012079697496264159
to;0.01068120243532238
import;0.01614802803840979
the;0.01135325049711371
BERT;0.0380882658295724
language;0.01432123237442864
model;0.014546975733678611
into;0.013726890857079847
python;0.01234183045828608
for;0.011593872544867375
use;0.01026879538142517
with;0.010936435548474377
PyTorch;0.013390229327699005
is;0.011169456977083467
using;0.010919601135768557
the;0.010010280783866144
Hugging;0.02160020786682546
Face;0.013342397243863695
Transformer's;0.019174795364194735
library,;0.01550443133274867
which;0.010949552523272862
has;0.010336762386714873
built;0.009890846594445215
in;0.010131208949365004
methods;0.010717852768161267
for;0.01076335820372829
pre-training,;0.017364857434411376
inference,;0.012438578033826026
and;0.009663737511077712
deploying;0.011637791240708626
BERT.;0.013857178525712678
â€˜**;0.021608563893130865
from;0.020669381211373405
transformers;0.012867340048456126
import;0.012258407762395774
AutoTokenizer,;0.023443922625958177
BertModel;0.015131429139042666
import;0.012660969526629005
torch;0.011276380569973961
tokenizer;0.011338091405246263
=;0.013240731297399319
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.13319247668270046
model;0.010856515214949968
=;0.012455785702755027
"BertModel.from_pretrained(""bert-base-uncased"")";0.04231486085079382
inputs;0.011196845899735595
=;0.01162444919766863
"tokenizer(""Hello,";0.024947702014994737
my;0.00945532670402323
dog;0.009810173364327619
is;0.009005936411200493
"cute"",";0.010064566743111882
"return_tensors=""pt"")";0.019842614590347772
outputs;0.00955689132927166
=;0.009793943974777255
model(**inputs);0.014967057390813639
last_hidden_states;0.016829685320647582
=;0.0099892956326928
outputs.last_hidden_state;0.011534468764003504
***;0.008816509182876936
