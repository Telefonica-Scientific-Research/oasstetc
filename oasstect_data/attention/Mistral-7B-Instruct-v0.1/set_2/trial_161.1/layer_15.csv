text;attention
The;0.9982652382166222
easiest;7.359632559493261e-06
way;6.128632754528331e-06
to;5.921265775414989e-06
import;7.24913254126157e-06
the;6.2747374351453405e-06
BERT;0.00011723013152642234
language;7.839425388521517e-06
model;1.4306667040295405e-05
into;1.053645778556745e-05
python;1.7279150452896996e-05
for;8.158319836066112e-06
use;5.83867645474712e-06
with;5.784768845409245e-06
PyTorch;6.443001264805024e-05
is;7.887076576945307e-06
using;7.085699479269969e-06
the;7.885560629518923e-06
Hugging;2.451824059734783e-05
Face;1.0913487304223692e-05
Transformer's;4.918531071094566e-05
library,;1.3360010256237021e-05
which;5.35940580422845e-06
has;5.326868030736766e-06
built;5.032560374894813e-06
in;5.607137871428336e-06
methods;5.7486077532146195e-06
for;5.385623202172803e-06
pre-training,;2.678705659861794e-05
inference,;1.1909505771650537e-05
and;5.497467332748956e-06
deploying;6.3667279995899135e-06
BERT.;2.040981391624337e-05
â€˜**;8.834557628994731e-06
from;1.0025723566549843e-05
transformers;8.60967731461033e-06
import;8.240847126520735e-06
AutoTokenizer,;0.00010628567170886251
BertModel;2.448118995333769e-05
import;9.155689578946226e-06
torch;1.099856493702926e-05
tokenizer;1.337373667898198e-05
=;8.998254426593035e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0008150643642077186
model;6.865163673860366e-06
=;5.394646829173483e-06
"BertModel.from_pretrained(""bert-base-uncased"")";3.7904334627019726e-05
inputs;7.902612891768351e-06
=;5.502558803041603e-06
"tokenizer(""Hello,";1.922075565513952e-05
my;5.3875644475236995e-06
dog;5.520644472135395e-06
is;4.944389308959681e-06
"cute"",";6.758792927949035e-06
"return_tensors=""pt"")";3.960150484239644e-05
outputs;5.912185023733137e-06
=;5.159680408262187e-06
model(**inputs);1.1831320081858756e-05
last_hidden_states;1.675344369025908e-05
=;5.65204434401306e-06
outputs.last_hidden_state;7.049084091429976e-06
***;4.729608877513064e-06
