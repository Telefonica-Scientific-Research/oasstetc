text;attention
The;0.015622672817106536
easiest;0.014151091844275553
way;0.013956683935447273
to;0.013095056800432775
import;0.01664390362590211
the;0.011934904214636909
BERT;0.025199724615513966
language;0.016048545645040366
model;0.01286902243673406
into;0.012877221781500645
python;0.01723491049716669
for;0.012892015246519856
use;0.013289735520516504
with;0.012210806323586978
PyTorch;0.020600542733051817
is;0.014121554423089716
using;0.014205508977567345
the;0.01199693945293788
Hugging;0.015503150954519637
Face;0.012779281457233026
Transformer's;0.019355700336485086
library,;0.01466129886915422
which;0.012558672067877452
has;0.012421076303892624
built;0.013389412441958595
in;0.012341325242194905
methods;0.013308413592150133
for;0.0125937580490245
pre-training,;0.022766754025707055
inference,;0.015266188058271362
and;0.011466223152634217
deploying;0.012250924606395203
BERT.;0.015890852343590625
â€˜**;0.02191663227462132
from;0.01319376212212943
transformers;0.013477578463260278
import;0.013855765047655393
AutoTokenizer,;0.021594209986033074
BertModel;0.013582138746387605
import;0.013190049808618771
torch;0.013518659387766689
tokenizer;0.013101086777646742
=;0.014366079693867015
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09525437535900617
model;0.01164066880757501
=;0.013496397055613797
"BertModel.from_pretrained(""bert-base-uncased"")";0.03668969461161815
inputs;0.012368621328879918
=;0.013171905871661732
"tokenizer(""Hello,";0.019764489912098622
my;0.011497259091066006
dog;0.012175874769832855
is;0.011257667499247302
"cute"",";0.013240007972441226
"return_tensors=""pt"")";0.018555289264609472
outputs;0.01180648848451442
=;0.012498268105644401
model(**inputs);0.015187858889857629
last_hidden_states;0.018129356672091083
=;0.011719452120075902
outputs.last_hidden_state;0.013618087197772334
***;0.010628402286292004
