text;attention
The;0.9999975910596975
easiest;4.3736478057012744e-08
way;3.358713660105274e-08
to;2.848667537973141e-08
import;3.465464213511953e-08
the;2.44528979365239e-08
BERT;5.8714141417365937e-08
language;2.6207774955832847e-08
model;3.038362155879789e-08
into;2.8933884323805676e-08
python;2.6663254806843937e-08
for;2.787127089555474e-08
use;2.44172121819477e-08
with;2.5883119165012747e-08
PyTorch;4.1023371279195716e-08
is;2.546861448695318e-08
using;2.9340711791490917e-08
the;2.7197690795647697e-08
Hugging;5.06912529623482e-08
Face;2.8170621665660373e-08
Transformer's;6.404760298460955e-08
library,;3.550181805942098e-08
which;2.569866602208841e-08
has;2.540822009003474e-08
built;2.5320955974326522e-08
in;2.5247673599577176e-08
methods;2.57026305223724e-08
for;2.594165525225613e-08
pre-training,;4.9644763757394666e-08
inference,;3.456331002445828e-08
and;2.297126939770939e-08
deploying;2.5494530491390187e-08
BERT.;4.1154563279166426e-08
â€˜**;3.9422496381652386e-08
from;4.206808296601022e-08
transformers;3.1769475657129756e-08
import;2.7755742136279413e-08
AutoTokenizer,;5.3336485599934565e-08
BertModel;3.311119517705879e-08
import;3.6313814665303885e-08
torch;2.9288598019478974e-08
tokenizer;3.4517881250346275e-08
=;2.5543916770619157e-08
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";3.880475823513639e-07
model;2.625789727532182e-08
=;2.3214832234815075e-08
"BertModel.from_pretrained(""bert-base-uncased"")";8.172556910704606e-08
inputs;2.764831964625881e-08
=;2.537840448765415e-08
"tokenizer(""Hello,";5.885882415139667e-08
my;2.3730915611248715e-08
dog;2.4415624414802415e-08
is;2.2686113565813782e-08
"cute"",";3.079558898830003e-08
"return_tensors=""pt"")";5.496342830501862e-08
outputs;2.6423126484736566e-08
=;2.331454988985855e-08
model(**inputs);4.307168879772794e-08
last_hidden_states;4.8231412248270845e-08
=;2.3043556383727324e-08
outputs.last_hidden_state;3.756994340564824e-08
***;2.3853205092497792e-08
