text;attention
The;0.0143657124961255
easiest;0.014750317174219681
way;0.012941173114629433
to;0.010785916679513327
import;0.017790438887473517
the;0.010489292299482296
BERT;0.01640069219717263
language;0.013262336436419248
model;0.010753043317860967
into;0.011482273121951792
python;0.0125510762821944
for;0.010688401782134312
use;0.012324638897348732
with;0.011953091515509762
PyTorch;0.014777383583858177
is;0.011455701069503605
using;0.010677883381674816
the;0.009872742259131343
Hugging;0.015463522024602208
Face;0.012520222499492788
Transformer's;0.016144171409914927
library,;0.01308976486265227
which;0.010110104745026199
has;0.01054637576651971
built;0.010667757311096435
in;0.011789072652711277
methods;0.012318654783149988
for;0.010613361860362184
pre-training,;0.021195544989096513
inference,;0.01336858705993261
and;0.009845992247238117
deploying;0.01263812192841459
BERT.;0.012670651268055265
â€˜**;0.01635389839765738
from;0.010759642287792052
transformers;0.013710856512655079
import;0.011661160535537136
AutoTokenizer,;0.02114094730545536
BertModel;0.01244593829745154
import;0.010468441646981274
torch;0.011865098699180622
tokenizer;0.011834364643106285
=;0.010343747607601454
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1531535258636872
model;0.010152829368229404
=;0.0100516612357947
"BertModel.from_pretrained(""bert-base-uncased"")";0.06706333448247365
inputs;0.011082489580615256
=;0.010104058165335026
"tokenizer(""Hello,";0.02540253437433227
my;0.009765547640544232
dog;0.010215490814280347
is;0.009097958903542654
"cute"",";0.011906701960314356
"return_tensors=""pt"")";0.028365857354911997
outputs;0.010497805180122931
=;0.009801043663895942
model(**inputs);0.018702633081550034
last_hidden_states;0.019434805802618593
=;0.009608350937331516
outputs.last_hidden_state;0.015872448212717594
***;0.008832807539815583
