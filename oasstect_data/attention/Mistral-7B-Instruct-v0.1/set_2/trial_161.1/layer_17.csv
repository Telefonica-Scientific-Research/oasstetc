text;attention
The;0.9315393848235607
easiest;7.042323331338169e-05
way;6.058325043936842e-05
to;5.382354699005181e-05
import;0.00010604529256581491
the;5.853757053916438e-05
BERT;0.0003416823102218993
language;5.515244233511912e-05
model;7.44482860551152e-05
into;7.263278285356517e-05
python;9.216749339158242e-05
for;6.023426273847463e-05
use;5.5033447487069324e-05
with;5.7226607419963216e-05
PyTorch;0.00016309858240641357
is;7.848810746568914e-05
using;7.507707487452696e-05
the;7.185330588276564e-05
Hugging;0.00013227569733859253
Face;7.707279879127878e-05
Transformer's;0.00024167449364701147
library,;9.581827784407191e-05
which;5.5860572438593886e-05
has;5.595123131704692e-05
built;5.042081802670787e-05
in;5.9841116632936875e-05
methods;5.450970035263201e-05
for;6.455968925003751e-05
pre-training,;0.0002689924945844552
inference,;8.883053214347208e-05
and;4.768956475096233e-05
deploying;6.0124151160086024e-05
BERT.;0.00014395018781702843
â€˜**;8.448312321983411e-05
from;8.002442635012483e-05
transformers;6.897859565656464e-05
import;6.790096936144728e-05
AutoTokenizer,;0.0003421442855826721
BertModel;9.671051161786792e-05
import;0.00011639941581648764
torch;7.658427669136625e-05
tokenizer;9.799212291707496e-05
=;8.183076399541433e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.05600685798889307
model;6.483467434837148e-05
=;6.026839406335054e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.00618732007162969
inputs;8.841474426300447e-05
=;6.282208359676902e-05
"tokenizer(""Hello,";0.0003516729744599965
my;4.660050718382865e-05
dog;4.938436147388677e-05
is;4.430308058526267e-05
"cute"",";7.365873688005908e-05
"return_tensors=""pt"")";0.0003492378965797782
outputs;5.8830498376740075e-05
=;5.348220422376229e-05
model(**inputs);0.00026979167192653846
last_hidden_states;0.0002505681061850012
=;6.383398778998168e-05
outputs.last_hidden_state;0.00018238391837721777
***;3.9221861319347845e-05
