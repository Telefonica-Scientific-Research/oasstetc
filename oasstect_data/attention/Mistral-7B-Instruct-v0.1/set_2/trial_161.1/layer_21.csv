text;attention
The;0.9997948477508153
easiest;1.7717548484861442e-06
way;1.546736297401045e-06
to;1.3677909858027444e-06
import;1.669441157290836e-06
the;1.418557684510234e-06
BERT;1.138900948848743e-05
language;1.6992381100963105e-06
model;2.021392407133394e-06
into;1.5871391340956737e-06
python;1.7373042654020704e-06
for;1.4342125764015578e-06
use;1.311361125927563e-06
with;1.427791626854901e-06
PyTorch;3.2028749341389856e-06
is;1.477908427659701e-06
using;1.482345969756407e-06
the;1.4610715756304445e-06
Hugging;2.2107874451035474e-06
Face;1.7390132894071326e-06
Transformer's;4.603656855921142e-06
library,;1.8726019675687084e-06
which;1.2319534068531392e-06
has;1.3374120485053716e-06
built;1.2146454202495744e-06
in;1.4150787274837581e-06
methods;1.3764528712410202e-06
for;1.4806323496046883e-06
pre-training,;3.2225552759147087e-06
inference,;1.8719862287838242e-06
and;1.1560990697959172e-06
deploying;1.5888135769894703e-06
BERT.;2.3474593366541346e-06
â€˜**;1.4991305319892983e-06
from;2.172070915252955e-06
transformers;1.6320802955351673e-06
import;1.3892241521671816e-06
AutoTokenizer,;9.23626726498645e-06
BertModel;2.7053666341099824e-06
import;1.6168417560359944e-06
torch;1.6332348041377715e-06
tokenizer;2.1297051947732247e-06
=;1.4846563442635037e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";6.816710464221496e-05
model;1.3096439696954251e-06
=;1.3021660735360754e-06
"BertModel.from_pretrained(""bert-base-uncased"")";1.1835441828975529e-05
inputs;1.454775534486864e-06
=;1.3599670339910767e-06
"tokenizer(""Hello,";4.648202673993179e-06
my;1.2233479244117425e-06
dog;1.3759006276601219e-06
is;1.1777541102874541e-06
"cute"",";1.5001968844110077e-06
"return_tensors=""pt"")";4.091653203768398e-06
outputs;1.3488622018249187e-06
=;1.2566960113817787e-06
model(**inputs);2.9805796544362046e-06
last_hidden_states;3.009808364686445e-06
=;1.2030481294023605e-06
outputs.last_hidden_state;2.6590744747358133e-06
***;1.0743694625704797e-06
