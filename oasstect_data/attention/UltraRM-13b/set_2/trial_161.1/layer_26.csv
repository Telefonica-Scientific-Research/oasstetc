text;attention
The;0.034610509987039156
easiest;0.01355579125598248
way;0.012754340971078536
to;0.011043555954999684
import;0.01679511020098877
the;0.011001588875166462
BERT;0.025137236697982398
language;0.010307323841509425
model;0.014026988598034307
into;0.010984470192807802
python;0.017960833941863426
for;0.01050386346820561
use;0.011438164400018597
with;0.009608652596519848
PyTorch;0.02249479092155775
is;0.0195214816765901
using;0.0100657446129481
the;0.009513686744879072
Hugging;0.013939890953664593
Face;0.010216391461889204
Transformer's;0.18854727874317095
library,;0.016122771875755344
which;0.010685332203495533
has;0.009632916911726257
built;0.009623014688453219
in;0.00950766116133954
methods;0.010733252669857814
for;0.009188651532609964
pre-training,;0.012565711311424647
inference,;0.011038397489229081
and;0.009119098562450243
deploying;0.010623223405469638
BERT.;0.0245988968372581
â€˜**;0.020588963645027596
from;0.010585039118698856
transformers;0.01140495360635958
import;0.009960405980654747
AutoTokenizer,;0.014478739270989598
BertModel;0.011797532374251754
import;0.010701299307326934
torch;0.010742208449828327
tokenizer;0.012344348940244035
=;0.009835390616805412
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.042176822430814755
model;0.009563914929616852
=;0.009109479572873823
"BertModel.from_pretrained(""bert-base-uncased"")";0.04244759040123405
inputs;0.009740694923571136
=;0.009112204624201604
"tokenizer(""Hello,";0.01217752249987189
my;0.009177775549974662
dog;0.009373586352070133
is;0.009028380902814678
"cute"",";0.010321701096628486
"return_tensors=""pt"")";0.011535632111331759
outputs;0.009106726839273011
=;0.008945432252221488
model(**inputs);0.01112695269943605
last_hidden_states;0.010022111029335859
=;0.00884607032965644
outputs.last_hidden_state;0.009540407709693518
***;0.008741487689227135
