text;attention
The;0.06964723089545308
easiest;0.010466780496589606
way;0.009794125191762232
to;0.009579805155818483
import;0.01411777078531933
the;0.008739112772804532
BERT;0.017535655577675894
language;0.01034601843412025
model;0.010128627399408443
into;0.013039255790737569
python;0.01397943634707875
for;0.009881782543139305
use;0.009251810776041286
with;0.009212187130965287
PyTorch;0.01608967852314847
is;0.01266828001304206
using;0.010521422870547553
the;0.009082976916711947
Hugging;0.009993749566012837
Face;0.015530259253705521
Transformer's;0.01982167750877915
library,;0.01212834207531156
which;0.00903296757918825
has;0.008662163765514711
built;0.007914961389039375
in;0.009154011386433429
methods;0.010003325215337651
for;0.00931464220853468
pre-training,;0.016679373287568754
inference,;0.01055678935312045
and;0.00818189369166982
deploying;0.009907351076725092
BERT.;0.013923601958333599
â€˜**;0.012545468675460752
from;0.013829507305663
transformers;0.014311047328387191
import;0.010834775678523683
AutoTokenizer,;0.02647133654370128
BertModel;0.011688919997557671
import;0.01158635010591636
torch;0.010459766196307584
tokenizer;0.013320303643169096
=;0.010991770976680063
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.18447119695024225
model;0.010051464904914218
=;0.010175400892777013
"BertModel.from_pretrained(""bert-base-uncased"")";0.041789325403705585
inputs;0.010434934377294642
=;0.00947225303998146
"tokenizer(""Hello,";0.028242402241163072
my;0.008284695014450469
dog;0.009832595746002572
is;0.008413408883761797
"cute"",";0.010693579887987607
"return_tensors=""pt"")";0.017830011249632467
outputs;0.009631045141125175
=;0.008395769912069929
model(**inputs);0.016205051295481544
last_hidden_states;0.01595542584759376
=;0.008496542910316679
outputs.last_hidden_state;0.012983406564514388
***;0.0077151763499797524
