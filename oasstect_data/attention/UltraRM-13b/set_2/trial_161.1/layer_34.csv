text;attention
The;0.12013341127709794
easiest;0.0212316335815491
way;0.013841413822459151
to;0.009234348561873256
import;0.015283461443790548
the;0.008880074911457519
BERT;0.040259982101394955
language;0.008865304564907127
model;0.01609303245167818
into;0.01071678057636519
python;0.027269858756129167
for;0.008899820963107657
use;0.011256754528795427
with;0.007563549195564289
PyTorch;0.04676673103023691
is;0.017027486827031896
using;0.008798522326829
the;0.007534290289345228
Hugging;0.015320874502458427
Face;0.008785161019208449
Transformer's;0.0786688434768718
library,;0.014352163116648164
which;0.007346000607480401
has;0.0063667377411575216
built;0.0065825188205380246
in;0.006280811074204789
methods;0.007178542470647634
for;0.005904683860840479
pre-training,;0.015589871059408938
inference,;0.008372756231653377
and;0.005760671231281468
deploying;0.007491071555436981
BERT.;0.03072479322173305
â€˜**;0.016043242689211798
from;0.007451470347216142
transformers;0.009865125909844544
import;0.007037596064019646
AutoTokenizer,;0.015845673618982616
BertModel;0.01023782292506538
import;0.006985881160389807
torch;0.008314060949645055
tokenizer;0.009005162382788524
=;0.0063533513836904355
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.125257872940177
model;0.006119837533978027
=;0.0056479299693485115
"BertModel.from_pretrained(""bert-base-uncased"")";0.05131750070574186
inputs;0.0058501065780714664
=;0.005401647016954754
"tokenizer(""Hello,";0.008571954552424795
my;0.0054114952611430235
dog;0.005566864806657365
is;0.00532697837964641
"cute"",";0.0063087393251383675
"return_tensors=""pt"")";0.008113425585226491
outputs;0.005342640285503962
=;0.005227909968853984
model(**inputs);0.00703586390445891
last_hidden_states;0.0061967832903911165
=;0.005109121160798106
outputs.last_hidden_state;0.005661816274608175
***;0.005010167830841521
