text;attention
The;0.03798598801564141
easiest;0.017445905920788273
way;0.013748545115264691
to;0.012235276749619595
import;0.015140632535627535
the;0.011430390692376866
BERT;0.016441657412540136
language;0.010649517054947693
model;0.014446677231248012
into;0.011306935721588841
python;0.018277636889928135
for;0.011026246009686045
use;0.012522053544462262
with;0.010076809677732706
PyTorch;0.018715482848058716
is;0.024270573939306383
using;0.01089641408395972
the;0.010122308500902228
Hugging;0.01374358684116108
Face;0.010930601185172572
Transformer's;0.09934242432084492
library,;0.01724003904076037
which;0.011540728215079211
has;0.010375262678041636
built;0.01060741651934702
in;0.010288928599918909
methods;0.011242300883655468
for;0.009755222380305777
pre-training,;0.015463890651032892
inference,;0.012338426384094648
and;0.00979848076620779
deploying;0.011828137881715186
BERT.;0.02436815478068906
â€˜**;0.02717542661302462
from;0.011516124212107386
transformers;0.013640028814425248
import;0.011022273200797374
AutoTokenizer,;0.017829960283531933
BertModel;0.012674081655522123
import;0.011418714163948624
torch;0.011759444187237799
tokenizer;0.013645191789165805
=;0.01058099655633478
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07233898760452746
model;0.010250598333836292
=;0.009729287684665711
"BertModel.from_pretrained(""bert-base-uncased"")";0.05200059073277931
inputs;0.010392396360254425
=;0.0096598429992826
"tokenizer(""Hello,";0.013943299414075405
my;0.009726509656523176
dog;0.00987773451894872
is;0.00960130538172558
"cute"",";0.011086461709186316
"return_tensors=""pt"")";0.01353306467823567
outputs;0.009678284296479935
=;0.009445684167297215
model(**inputs);0.012180191171511822
last_hidden_states;0.010940432969035897
=;0.009301803091795827
outputs.last_hidden_state;0.010304766510788259
***;0.009143864171248942
