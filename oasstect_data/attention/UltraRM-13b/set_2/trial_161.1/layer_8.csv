text;attention
The;0.020654277008720065
easiest;0.010548441528865629
way;0.009632252224797671
to;0.009310417071056143
import;0.015182525547033303
the;0.00945746631524566
BERT;0.02558237932885526
language;0.012880764145270519
model;0.016084475101062465
into;0.016854604910287615
python;0.019538744253443274
for;0.011478512396460708
use;0.01066850471355173
with;0.010215877559171646
PyTorch;0.02491307178624479
is;0.01357561461940035
using;0.010660152675900066
the;0.009647620933708597
Hugging;0.011122755523100105
Face;0.022492694258602397
Transformer's;0.031747853353721704
library,;0.01479105455855915
which;0.0094773572380368
has;0.009179127171208088
built;0.008689096420646494
in;0.010017894081075875
methods;0.010991535186840162
for;0.0100474271485401
pre-training,;0.01890792995023295
inference,;0.012839844302480773
and;0.00892424730250295
deploying;0.011044808377488556
BERT.;0.019060707883954798
â€˜**;0.012886279522391363
from;0.013447591224071177
transformers;0.015208001870072315
import;0.012004246021541026
AutoTokenizer,;0.034383942395186565
BertModel;0.014363995492256577
import;0.013892413683973348
torch;0.011593479275910885
tokenizer;0.018737469275881026
=;0.011636682398572679
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.12489169406852928
model;0.011741454869318149
=;0.01097057903479798
"BertModel.from_pretrained(""bert-base-uncased"")";0.04894375853482669
inputs;0.012293389630269772
=;0.010154544313387798
"tokenizer(""Hello,";0.021947289583656288
my;0.0089044662956597
dog;0.010289551522963354
is;0.009206838483187442
"cute"",";0.011145765215974619
"return_tensors=""pt"")";0.016527660793406826
outputs;0.010651253878272482
=;0.008922172498097722
model(**inputs);0.016705615255040846
last_hidden_states;0.013689452670039714
=;0.008963711633285607
outputs.last_hidden_state;0.01128901570657037
***;0.00838764997679222
