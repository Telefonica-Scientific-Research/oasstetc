text;attention
The;0.09642698742849516
easiest;0.023404410989338764
way;0.016468453586097425
to;0.011575289339420272
import;0.014202933003899262
the;0.011190030055275085
BERT;0.02212926565560509
language;0.01033259740665804
model;0.013997970439510713
into;0.011655779056616238
python;0.01772888086577009
for;0.010458289462931194
use;0.012440522768613023
with;0.00935964423352952
PyTorch;0.03214173843445588
is;0.01576035734320378
using;0.009963654950539816
the;0.009150862838936845
Hugging;0.017058844696445377
Face;0.009973649022999359
Transformer's;0.06875850152304101
library,;0.015465935437647804
which;0.009483556489434036
has;0.008611442682603427
built;0.008784561635598575
in;0.008362760790359662
methods;0.009301094705743331
for;0.007944766318574274
pre-training,;0.017290268101018948
inference,;0.010604178654521085
and;0.007850422014446447
deploying;0.00999914250227736
BERT.;0.020942877699944896
â€˜**;0.016320124202574
from;0.009684042919352317
transformers;0.01217637946388418
import;0.009010065879993055
AutoTokenizer,;0.017350385380565277
BertModel;0.011293596637317403
import;0.008961670833725562
torch;0.010069056300722948
tokenizer;0.010689086178340726
=;0.008470774060802719
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1142941695376343
model;0.008028143850062003
=;0.007729106497844977
"BertModel.from_pretrained(""bert-base-uncased"")";0.05306105722986825
inputs;0.007918796439147537
=;0.0074833326101373596
"tokenizer(""Hello,";0.011488070134861451
my;0.007539485081657088
dog;0.007647213805829069
is;0.007414834275557179
"cute"",";0.00875799896990928
"return_tensors=""pt"")";0.011143689074901625
outputs;0.007357209798715394
=;0.0072267126471214
model(**inputs);0.009642217848052909
last_hidden_states;0.008514466330666253
=;0.00707576344861198
outputs.last_hidden_state;0.007909593180530321
***;0.006923287248061804
