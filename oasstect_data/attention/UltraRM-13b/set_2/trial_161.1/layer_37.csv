text;attention
The;0.024819075842292215
easiest;0.0014626049590000193
way;0.0010882038879471306
to;0.00037816108056315745
import;0.0008114060455317348
the;0.0003325168976036238
BERT;0.012387725106102077
language;0.0004347581879171733
model;0.0009886440872089494
into;0.0005199973643137198
python;0.0018072416363612194
for;0.00041621613733326543
use;0.0006385453754201554
with;0.00028656056507398884
PyTorch;0.025451031872133795
is;0.0012442650591932513
using;0.00044207783459193005
the;0.00029888522538012443
Hugging;0.005056915476598228
Face;0.0004074239067186962
Transformer's;0.13943445227309123
library,;0.0013636148366332635
which;0.0002817230676181612
has;0.00020680795557909425
built;0.0002064358636856217
in;0.00018635527261820367
methods;0.0002444168396447738
for;0.00016195390058618484
pre-training,;0.002661507538542748
inference,;0.00037460153740534143
and;0.00015539793375307366
deploying;0.0003142844182759879
BERT.;0.0049152817403732806
â€˜**;0.0013719364736414193
from;0.00025486121376759726
transformers;0.0006521952908013224
import;0.00022934999406541871
AutoTokenizer,;0.002716745380072851
BertModel;0.0006685343896606826
import;0.00022442820271127736
torch;0.0003940385551028207
tokenizer;0.0004416454336661625
=;0.00017284576151980138
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7004974367138872
model;0.00015426071426494467
=;0.00012978336623087225
"BertModel.from_pretrained(""bert-base-uncased"")";0.05971038081345617
inputs;0.00013464508616905285
=;0.00011522195493079121
"tokenizer(""Hello,";0.00040872095457910836
my;0.00011712265587522854
dog;0.00012042717033709829
is;0.00011120099287140294
"cute"",";0.00018084611108739964
"return_tensors=""pt"")";0.0004289602464684815
outputs;0.00011043879442687
=;0.00010517867026687202
model(**inputs);0.0002584108750207701
last_hidden_states;0.00017825326334262626
=;9.800694633692643e-05
outputs.last_hidden_state;0.00014381857329217253
***;9.121567705518137e-05
