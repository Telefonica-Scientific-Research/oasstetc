text;attention
The;0.010519121078273751
easiest;0.006926673311454978
way;0.006732025468527728
to;0.005594267893857045
import;0.0077463039621411415
the;0.005659228159829129
BERT;0.01781546744744729
language;0.0054775242918613165
model;0.008317929709430127
into;0.00924087657469613
python;0.010852434820449482
for;0.0070118582464946625
use;0.005296862490182963
with;0.005147112472840618
PyTorch;0.015358667355164017
is;0.011836649963032738
using;0.007015382151189124
the;0.006896754636270813
Hugging;0.00711250125360529
Face;0.011818043126218198
Transformer's;0.06558338992971964
library,;0.008538978114504428
which;0.0055323637211371094
has;0.005242913581585564
built;0.004746968085143567
in;0.006194329820982192
methods;0.006082468734910955
for;0.005656516618052915
pre-training,;0.012110964205444424
inference,;0.008754475096504111
and;0.005168922900506922
deploying;0.006323555888183723
BERT.;0.015319314462002765
â€˜**;0.00825669816150121
from;0.007714076023604159
transformers;0.008031651782501308
import;0.008332976563526058
AutoTokenizer,;0.02331823906295287
BertModel;0.008912640832036293
import;0.0087227760800013
torch;0.008007077547785842
tokenizer;0.009323398966148031
=;0.008039321484997869
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3329441523828209
model;0.006335036239753339
=;0.006793501024611128
"BertModel.from_pretrained(""bert-base-uncased"")";0.1047498676723901
inputs;0.006472908067904018
=;0.006337358701710677
"tokenizer(""Hello,";0.018232501519473667
my;0.004893832700510031
dog;0.005267805770566536
is;0.005118992561456023
"cute"",";0.007544876787543994
"return_tensors=""pt"")";0.017421247247011572
outputs;0.005543619704722366
=;0.005387878998071543
model(**inputs);0.014793343510285063
last_hidden_states;0.008890716689071869
=;0.005142626371570373
outputs.last_hidden_state;0.007337604880751433
***;0.004502427093075889
