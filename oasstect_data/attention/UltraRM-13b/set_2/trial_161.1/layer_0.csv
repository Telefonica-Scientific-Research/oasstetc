text;attention
The;0.0025319513697440506
easiest;0.00018728946534400082
way;0.00024224775541194807
to;0.0025506977996889047
import;0.0001636676751974713
the;0.00044415955372154075
BERT;0.0009582518096954686
language;8.138537821989329e-05
model;7.46280919463937e-05
into;0.00024471954687640073
python;0.00011582859988381144
for;0.00048355176897629495
use;7.854463700998389e-05
with;0.0003463462123509894
PyTorch;0.001234450657148538
is;0.00029358035796823496
using;6.254674289585155e-05
the;0.00012780768172438725
Hugging;0.000800035975697309
Face;4.513731407773743e-05
Transformer's;0.006257826453774012
library,;0.0006058557474293737
which;6.619009487352612e-05
has;8.995334424227477e-05
built;3.614224425928582e-05
in;0.00015066694021228415
methods;3.5153065646663995e-05
for;0.000115665588816107
pre-training,;0.003715681479816976
inference,;0.00026201739378799274
and;9.714918321845465e-05
deploying;0.00010568042895218801
BERT.;0.0007713911564286633
â€˜**;6.638150205102964e-05
from;6.54592175973967e-05
transformers;8.530706785627834e-05
import;3.196322922496217e-05
AutoTokenizer,;0.0006294259801056863
BertModel;4.919518043086814e-05
import;2.9435533950973512e-05
torch;5.36171798014319e-05
tokenizer;4.99487523400058e-05
=;5.043466275822478e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.9457413208319233
model;2.1500358806474713e-05
=;3.5414489544006966e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.029146276595988445
inputs;1.9638973621303518e-05
=;2.6697687978881796e-05
"tokenizer(""Hello,";0.00011918246157025196
my;1.993807546199759e-05
dog;1.85696042012089e-05
is;2.359640674229551e-05
"cute"",";3.4632970207846866e-05
"return_tensors=""pt"")";0.0001304700716377642
outputs;1.73718039864909e-05
=;2.0009525716376527e-05
model(**inputs);5.1953004097734036e-05
last_hidden_states;2.9173786411084253e-05
=;1.702470067539819e-05
outputs.last_hidden_state;2.5141625084459876e-05
***;1.4717205190954466e-05
