text;attention
The;8.338691353989216e-11
easiest;7.84255797075613e-17
way;5.3673431798853664e-17
to;6.924383843488286e-17
import;6.205458672664224e-17
the;0.06237931575158698
BERT;9.628770680652017e-17
language;5.928862000780529e-17
model;5.788322910301547e-17
into;5.72387451481218e-17
python;7.971623728734869e-17
for;5.779294725856325e-17
use;5.4105826594046584e-17
with;5.505320160766426e-17
PyTorch;1.124115947126228e-16
is;5.689942044433427e-17
using;5.435111173008555e-17
the;5.1520621677346576e-17
Hugging;8.54574642375083e-17
Face;5.613780152866116e-17
Transformer's;9.451996517577853e-17
library,;0.9376206841650222
which;5.537713927566284e-17
has;5.227004220657375e-17
built;5.467059319376203e-17
in;5.092041594911027e-17
methods;5.455292329323065e-17
for;5.1154702891279447e-17
pre-training,;8.378632635949252e-17
inference,;6.525715690953994e-17
and;5.0931574174473656e-17
deploying;6.433885380855813e-17
BERT.;7.601061004526409e-17
â€˜**;7.491289364584468e-17
from;5.2284988796685865e-17
transformers;6.204124633750626e-17
import;5.629489741151168e-17
AutoTokenizer,;8.325386392001078e-17
BertModel;6.225894785621232e-17
import;5.475473764258078e-17
torch;6.18846034157497e-17
tokenizer;5.833844121832175e-17
=;5.520257260717246e-17
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.826386295248251e-16
model;4.980729576565062e-17
=;5.342786113374408e-17
"BertModel.from_pretrained(""bert-base-uncased"")";3.3235304934829446e-16
inputs;5.087454676904521e-17
=;5.259925283888278e-17
"tokenizer(""Hello,";8.502404714145139e-17
my;4.869827348613185e-17
dog;4.9752923562515673e-17
is;4.921355284489853e-17
"cute"",";6.184302416499577e-17
"return_tensors=""pt"")";1.0802660927115542e-16
outputs;5.0215056185135717e-17
=;5.165373738241957e-17
model(**inputs);8.293662015882197e-17
last_hidden_states;6.63201576657592e-17
=;4.906253713583751e-17
outputs.last_hidden_state;7.08473930274497e-17
***;4.6504770237279883e-17
