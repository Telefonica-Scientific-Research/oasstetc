text;attention
The;3.6771082533900946e-17
easiest;3.292005312251904e-21
way;2.7906999181991938e-21
to;2.6090277890164958e-21
import;3.398688471010548e-21
the;0.999644167656562
BERT;4.314808181009354e-21
language;3.0292480263163752e-21
model;2.6221597376590396e-21
into;2.7570683481138943e-21
python;3.24494899813814e-21
for;2.746869458461683e-21
use;2.6691175525368613e-21
with;2.5667606934723104e-21
PyTorch;4.420756399628176e-21
is;2.816191016844825e-21
using;2.6886776612351335e-21
the;2.4203341272908105e-21
Hugging;3.489346126566635e-21
Face;2.8158249421996227e-21
Transformer's;3.865490177954161e-21
library,;0.0003558323434380178
which;2.6127705685065796e-21
has;2.5128037184727963e-21
built;2.457137650658711e-21
in;2.596853901747356e-21
methods;2.662375022464961e-21
for;2.5661375990963573e-21
pre-training,;3.762880568235521e-21
inference,;2.8343210401798206e-21
and;2.3537764760339497e-21
deploying;2.597546132476088e-21
BERT.;3.1404446111589744e-21
â€˜**;3.1962778620653135e-21
from;2.69219737865932e-21
transformers;2.9144911790798855e-21
import;2.945933360111589e-21
AutoTokenizer,;4.279861727407134e-21
BertModel;2.7206760997162922e-21
import;2.7269817803721962e-21
torch;2.8211476482204333e-21
tokenizer;2.7638742494602717e-21
=;2.8075365713131813e-21
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.5777355826022276e-20
model;2.3799577611774757e-21
=;2.4964568962917632e-21
"BertModel.from_pretrained(""bert-base-uncased"")";9.730201137487865e-21
inputs;2.7059199479454994e-21
=;2.446107154432609e-21
"tokenizer(""Hello,";4.742523463839464e-21
my;2.370674363213202e-21
dog;2.497925616370919e-21
is;2.3991159012319477e-21
"cute"",";2.8794782693359406e-21
"return_tensors=""pt"")";4.7163755253689936e-21
outputs;2.453475154649881e-21
=;2.4463220721124937e-21
model(**inputs);4.187490547604933e-21
last_hidden_states;3.2912479273441215e-21
=;2.3853645491438506e-21
outputs.last_hidden_state;3.3880674560342198e-21
***;2.1735253767849946e-21
