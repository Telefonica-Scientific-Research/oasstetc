text;attention
The;0.03181543919917709
easiest;0.017347833457925173
way;0.015124455705991205
to;0.01421629733815701
import;0.01754397126373733
the;0.014058406249740252
BERT;0.02381663924022112
language;0.014077632446054191
model;0.01601450321592087
into;0.014341147156637975
python;0.019527322037057898
for;0.013659328147346167
use;0.014589285050676378
with;0.012923697268465524
PyTorch;0.02213580833492805
is;0.01981640402037638
using;0.013711265797965442
the;0.012989003038010646
Hugging;0.01502008808168076
Face;0.013727385305451688
Transformer's;0.05187196980601975
library,;0.01717130526868024
which;0.01356997914270562
has;0.012873233898780773
built;0.012873519291274206
in;0.013036440015659364
methods;0.013885708936179776
for;0.0125933482348509
pre-training,;0.016083166311019446
inference,;0.013998264610245175
and;0.012519626319031417
deploying;0.013620302164857222
BERT.;0.021430821209842365
â€˜**;0.02814394744653571
from;0.014099130655780551
transformers;0.01450891323788791
import;0.013165834356938418
AutoTokenizer,;0.016539616753768353
BertModel;0.014489009563396672
import;0.013904989444682663
torch;0.013533346346864683
tokenizer;0.015099805137356858
=;0.013453231662617816
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.03019092180505214
model;0.01297333735928172
=;0.012592818189555198
"BertModel.from_pretrained(""bert-base-uncased"")";0.028345989123073838
inputs;0.013145204487332538
=;0.01260824903300124
"tokenizer(""Hello,";0.015029384668449827
my;0.012608290788856141
dog;0.012761483862095819
is;0.012443211237047076
"cute"",";0.013391331812741361
"return_tensors=""pt"")";0.014500483326971883
outputs;0.012602059517739692
=;0.012462085660700551
model(**inputs);0.014350720947354467
last_hidden_states;0.01335771771614478
=;0.012390665646686784
outputs.last_hidden_state;0.013041134051112789
***;0.01228348859630497
