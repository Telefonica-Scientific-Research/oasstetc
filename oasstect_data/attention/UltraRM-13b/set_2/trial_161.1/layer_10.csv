text;attention
The;0.011252105159014994
easiest;0.006774874584276549
way;0.0062793056824402465
to;0.0057113679703026145
import;0.010400943017502735
the;0.005990000806837749
BERT;0.027066695097315664
language;0.006484828381748458
model;0.010643621495447278
into;0.01494272187964771
python;0.013811072709781328
for;0.00799545471460436
use;0.006895479590342926
with;0.006275793525368416
PyTorch;0.02403197636221288
is;0.012833353872941952
using;0.008445698673664095
the;0.00672500026746027
Hugging;0.007624895265463672
Face;0.014884659135142458
Transformer's;0.06605629053244008
library,;0.010401034769695484
which;0.0058100556629544385
has;0.0058125420680555555
built;0.005111997648299887
in;0.006782064670771295
methods;0.006349615420338762
for;0.006157023836411628
pre-training,;0.016684220779191117
inference,;0.009776774702922608
and;0.00552627098979296
deploying;0.007371185176066713
BERT.;0.025479848671618788
â€˜**;0.008691637237166152
from;0.008307490526604172
transformers;0.010213068489942297
import;0.009963016271624978
AutoTokenizer,;0.033145154293812734
BertModel;0.012269037703351226
import;0.009023311017352233
torch;0.008428699403146485
tokenizer;0.013071037040154837
=;0.009214232148884265
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2732312216330766
model;0.0068643868626084405
=;0.007020985291289972
"BertModel.from_pretrained(""bert-base-uncased"")";0.0711112400311256
inputs;0.00767388307830199
=;0.006551077987854743
"tokenizer(""Hello,";0.022284676119576824
my;0.0052633225915558685
dog;0.005398436415827533
is;0.005302615739285588
"cute"",";0.0068409559126104014
"return_tensors=""pt"")";0.015107615060424875
outputs;0.00637169257734817
=;0.005819429937078335
model(**inputs);0.014554957343141172
last_hidden_states;0.008012226234449157
=;0.0058467302245491185
outputs.last_hidden_state;0.007060149455136525
***;0.004968940252644094
