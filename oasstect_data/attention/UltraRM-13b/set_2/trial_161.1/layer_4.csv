text;attention
The;0.5681953168133632
easiest;0.005622537488122247
way;0.005289214329007196
to;0.005046780363371892
import;0.006054641596857356
the;0.005293816471704608
BERT;0.008241687136326744
language;0.005681350680029765
model;0.00576564832378984
into;0.005952552729046172
python;0.005869662939276781
for;0.005211171512457861
use;0.00508560904967565
with;0.00520706205463048
PyTorch;0.007855074337004515
is;0.006487553787537901
using;0.005201858359834626
the;0.004712275283999051
Hugging;0.00628196848889601
Face;0.00592562543992045
Transformer's;0.007707837490760556
library,;0.005622884106335912
which;0.005017132942351708
has;0.004701651125894254
built;0.0048813980067940665
in;0.005413901056330487
methods;0.005196963037194282
for;0.0049240431430292485
pre-training,;0.009511188662525516
inference,;0.00536308170333765
and;0.004588416424339132
deploying;0.005014438042535526
BERT.;0.006149401435346506
â€˜**;0.006554155865509719
from;0.006306602969917231
transformers;0.005943095787407281
import;0.005774116978531913
AutoTokenizer,;0.009377264133663897
BertModel;0.006031277551346097
import;0.005402574837070736
torch;0.005243326898659597
tokenizer;0.005707389263052098
=;0.005702070326796898
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.06592521072149353
model;0.005434483729042011
=;0.0054522604240460825
"BertModel.from_pretrained(""bert-base-uncased"")";0.018293507411549812
inputs;0.005529547527524699
=;0.005566197536688922
"tokenizer(""Hello,";0.008860369177088547
my;0.004738782140247541
dog;0.004808308444467839
is;0.00470927928151807
"cute"",";0.005152712835509595
"return_tensors=""pt"")";0.008082542475630563
outputs;0.005142959054518619
=;0.005109719747521892
model(**inputs);0.008183505058690518
last_hidden_states;0.008479045202028041
=;0.004797757378789894
outputs.last_hidden_state;0.006272190494968121
***;0.004346002385092769
