text;attention
The;0.021229720337780453
easiest;0.015142724865716526
way;0.013303333791530782
to;0.012380135311630274
import;0.015485132740444411
the;0.012128102594468481
BERT;0.0290417842419874
language;0.01156672338015178
model;0.014859187856248745
into;0.012176537162627324
python;0.017926553378435347
for;0.01168271399164282
use;0.012075365742127303
with;0.011035351106619098
PyTorch;0.028946232526514064
is;0.015648745759634857
using;0.01150793988495807
the;0.011135894389798945
Hugging;0.01511358763372051
Face;0.012586190384591571
Transformer's;0.10819773824146861
library,;0.01644817504603745
which;0.011877351525746102
has;0.010974563340010542
built;0.011399555701171429
in;0.011283861291739384
methods;0.012156589539426395
for;0.010775621576699397
pre-training,;0.014808494202736569
inference,;0.013042429122936195
and;0.010796354813358524
deploying;0.012138277352587289
BERT.;0.021728754056130083
â€˜**;0.018956669002652703
from;0.012624310848941984
transformers;0.01444304041388353
import;0.011526202238886729
AutoTokenizer,;0.017387529873338702
BertModel;0.014838235576770685
import;0.012206633123355685
torch;0.01256309882299229
tokenizer;0.01458768560315593
=;0.011346236671129458
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04860188090822648
model;0.011606747201703101
=;0.0107510886293258
"BertModel.from_pretrained(""bert-base-uncased"")";0.04245712673763409
inputs;0.011681276303801716
=;0.010813321317165243
"tokenizer(""Hello,";0.014785387734819706
my;0.010917050568842531
dog;0.011071732765120811
is;0.01065044167756093
"cute"",";0.012341712888832642
"return_tensors=""pt"")";0.013987879638486047
outputs;0.010921834027692489
=;0.010604096923343444
model(**inputs);0.01334855356476558
last_hidden_states;0.012146353458588811
=;0.010498247586080811
outputs.last_hidden_state;0.011358259709554367
***;0.010377643292671275
