text;attention
The;0.02684655375294845
easiest;0.017197287143396515
way;0.015124032366733368
to;0.014582187358670583
import;0.016967147440825694
the;0.014043443170264383
BERT;0.024599176311241658
language;0.013559462430040269
model;0.016166655355927245
into;0.014140208719480478
python;0.018394840282329322
for;0.01360287024350045
use;0.014617010727321833
with;0.013003079428562563
PyTorch;0.026586289825745974
is;0.017060461810489192
using;0.013428264736167723
the;0.013106847366564202
Hugging;0.016478027267001056
Face;0.013940961469310297
Transformer's;0.05735482964129845
library,;0.016477379049556453
which;0.013654420883573208
has;0.01302147666663493
built;0.013333872760988465
in;0.012967015971955523
methods;0.013618889814849508
for;0.012679213946364962
pre-training,;0.0160563847774373
inference,;0.014519135042542756
and;0.01262151221134417
deploying;0.013861097255938053
BERT.;0.01937138446233616
â€˜**;0.016806139802494048
from;0.013440838495288977
transformers;0.016269801884956463
import;0.013442457088559685
AutoTokenizer,;0.01790394903738225
BertModel;0.01578151965285096
import;0.01341963263728893
torch;0.014155306766681058
tokenizer;0.014663230272829638
=;0.013008256783656314
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.03607027872107579
model;0.013002956003402438
=;0.012606705280588125
"BertModel.from_pretrained(""bert-base-uncased"")";0.030184696859556215
inputs;0.01292385351208802
=;0.012509680292460775
"tokenizer(""Hello,";0.015106616398006097
my;0.012548789274028973
dog;0.012686581197730981
is;0.01248424400902172
"cute"",";0.013428960615614386
"return_tensors=""pt"")";0.014844132754044186
outputs;0.012534702793038524
=;0.012378656294765693
model(**inputs);0.014102030625762211
last_hidden_states;0.013333432582562284
=;0.012285503865525182
outputs.last_hidden_state;0.012923613155899003
***;0.012172013651499865
