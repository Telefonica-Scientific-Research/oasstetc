text;attention
The;0.0056613588506847215
easiest;0.006214683594752714
way;0.0050289831244928205
to;0.004254888557402636
import;0.006718611050043659
the;0.004330390112232849
BERT;0.03181323189946674
language;0.004772638601791605
model;0.009890481556058536
into;0.006058653746036469
python;0.007831837114106233
for;0.004589755112550321
use;0.004491763663853079
with;0.004187545838677521
PyTorch;0.026320038644851548
is;0.00631149305018569
using;0.004953685751431534
the;0.0049907855608513395
Hugging;0.006093479218417473
Face;0.008615233171990784
Transformer's;0.3122683435990604
library,;0.005413276229586474
which;0.004091987554388528
has;0.004041367125881571
built;0.0040216042609786725
in;0.004123527136306672
methods;0.004240518292104262
for;0.004031696132329939
pre-training,;0.00765796466938258
inference,;0.005722630958122467
and;0.003943920504063521
deploying;0.004431603197578097
BERT.;0.014059628435508688
â€˜**;0.0136064117208851
from;0.005528992647897307
transformers;0.006261781429419508
import;0.005570323430529174
AutoTokenizer,;0.020579853197708477
BertModel;0.010737571271840494
import;0.005918615795163984
torch;0.006018671348450347
tokenizer;0.008945679063223144
=;0.005887132461930389
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1109101452780522
model;0.004939068384440083
=;0.004500735877840523
"BertModel.from_pretrained(""bert-base-uncased"")";0.1612024489351985
inputs;0.005282146270126596
=;0.00443009383792688
"tokenizer(""Hello,";0.011026512503809186
my;0.003973986527753956
dog;0.004451813277296021
is;0.004106973160515102
"cute"",";0.0051814907688034735
"return_tensors=""pt"")";0.0126173175851436
outputs;0.004599214526947985
=;0.004109135021501601
model(**inputs);0.008406936849756005
last_hidden_states;0.006987165501667484
=;0.00403069498409972
outputs.last_hidden_state;0.005184577510486653
***;0.0038269045164162296
