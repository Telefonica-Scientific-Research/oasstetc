text;attention
The;8.630483408257389e-05
easiest;2.7612442859342774e-05
way;1.4422978076247292e-05
to;7.511490709202318e-06
import;4.569872818199271e-05
the;8.173779380858528e-06
BERT;6.644457135669465e-05
language;1.701254654685631e-05
model;2.2192220619589526e-05
into;1.920377439608029e-05
python;3.5478975098040345e-05
for;1.0274318274915278e-05
use;1.3040097513019754e-05
with;1.0567432528943675e-05
PyTorch;0.0002906932760466807
is;2.751049789685349e-05
using;1.737593901835891e-05
the;1.2956638767395856e-05
Hugging;0.0001123528393578714
Face;1.5337720659166537e-05
Transformer's;0.0022450816545458714
library,;2.9414863662353396e-05
which;9.484824911417727e-06
has;7.3753355269510925e-06
built;8.267147567712575e-06
in;7.3504984783181404e-06
methods;8.591550632294189e-06
for;6.5282756845752964e-06
pre-training,;0.00014023339799297175
inference,;1.6413306103979844e-05
and;6.29953552989846e-06
deploying;1.3513011846299712e-05
BERT.;0.00011845015654283638
â€˜**;3.734783740142478e-05
from;1.0243950378542142e-05
transformers;2.7348024150794997e-05
import;1.1198778749930839e-05
AutoTokenizer,;0.00014859504753668123
BertModel;2.8316681155199385e-05
import;1.1028779272652136e-05
torch;1.7092692781522306e-05
tokenizer;1.932284976639401e-05
=;8.040016696127085e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.8825041628139927
model;7.283907013424714e-06
=;6.268577171904118e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.11319864667246145
inputs;7.2625510535784235e-06
=;5.823777697877439e-06
"tokenizer(""Hello,";6.608183556053367e-05
my;6.276622984461742e-06
dog;6.512635272958829e-06
is;5.899964771647789e-06
"cute"",";1.823311175646315e-05
"return_tensors=""pt"")";0.00018923113479763637
outputs;6.103952940409421e-06
=;5.509411501465991e-06
model(**inputs);7.015119268133252e-05
last_hidden_states;4.00906322167847e-05
=;5.303957313968962e-06
outputs.last_hidden_state;5.0816772090286497e-05
***;4.637158436125072e-06
