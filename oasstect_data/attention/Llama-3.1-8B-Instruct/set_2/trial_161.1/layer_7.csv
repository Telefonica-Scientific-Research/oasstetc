text;attention
The;0.009308951478565312
easiest;0.007360012436922756
way;0.007376837889091847
to;0.0067982143387486545
import;0.011698396651344886
the;0.007812072942685759
BERT;0.03379310397207711
language;0.008447007597265934
model;0.020026196167444468
into;0.01577837620383901
python;0.02060754551001714
for;0.00988038209949676
use;0.007349568700389205
with;0.00697773861064841
PyTorch;0.020323829404705427
is;0.017033673908398952
using;0.008627453329652155
the;0.007408921342244806
Hugging;0.008365604975351115
Face;0.009824849023046296
Transformer's;0.01953320220658206
library,;0.019416862294899556
which;0.007867884815340974
has;0.006863354840439649
built;0.005834856515374103
in;0.007148321324344631
methods;0.00759014005775554
for;0.006996371478251726
pre-training,;0.012834544116913111
inference,;0.009916354557639335
and;0.006178348673280935
deploying;0.006766910585160137
BERT.;0.024570361188163248
â€˜**;0.020452797950895173
from;0.009317981710352462
transformers;0.010180902537684649
import;0.009500686429546042
AutoTokenizer,;0.02455978679002653
BertModel;0.014022087031148394
import;0.011966478088494572
torch;0.008715279463022918
tokenizer;0.009043702203230787
=;0.010716847670455535
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.25182879903732674
model;0.008372394691996474
=;0.007699377052309299
"BertModel.from_pretrained(""bert-base-uncased"")";0.06237525933263768
inputs;0.009399670861727931
=;0.00729421274387989
"tokenizer(""Hello,";0.03710787107751771
my;0.006275901706236844
dog;0.006543292920726473
is;0.005978800650844766
"cute"",";0.007577465143429991
"return_tensors=""pt"")";0.016246471651535716
outputs;0.008237360374329528
=;0.006415408684986889
model(**inputs);0.014182003875001377
last_hidden_states;0.010107486126531727
=;0.006268524371780225
outputs.last_hidden_state;0.007767854576100466
***;0.0055290460101621265
