text;attention
The;0.01542375722813849
easiest;0.013530779127839527
way;0.013161265780929668
to;0.012609310049876804
import;0.017504502829714417
the;0.013021467634559998
BERT;0.025133901857109328
language;0.012768383152792016
model;0.015491785362334866
into;0.014423171425756802
python;0.01636083285135024
for;0.013750163476951974
use;0.013147205937894143
with;0.012803167277775132
PyTorch;0.018658991544327652
is;0.01399797290429689
using;0.013611235032286546
the;0.013069829828732437
Hugging;0.013286260889512227
Face;0.014061084166898302
Transformer's;0.017951550502293846
library,;0.014833000175694576
which;0.01285155706998186
has;0.012880250752571277
built;0.01180813037114281
in;0.013170773098388081
methods;0.013284627550026743
for;0.012901446766049736
pre-training,;0.0157393388444533
inference,;0.013905590550945814
and;0.01191562896629297
deploying;0.012204869649225354
BERT.;0.016606047910251255
â€˜**;0.017167898671542645
from;0.014744340739863278
transformers;0.013782917345911994
import;0.014474061247382769
AutoTokenizer,;0.027863134343708018
BertModel;0.020354498653297776
import;0.013974594725088474
torch;0.01414116292140828
tokenizer;0.015118685937659655
=;0.013320668184656569
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08558270459739466
model;0.014319594657384173
=;0.013096655892576185
"BertModel.from_pretrained(""bert-base-uncased"")";0.03045755847599228
inputs;0.015266815117788485
=;0.012812723490849864
"tokenizer(""Hello,";0.019389413634786225
my;0.011892325094141835
dog;0.01240254990504117
is;0.011645367115207254
"cute"",";0.012820812696454336
"return_tensors=""pt"")";0.01922759329131303
outputs;0.014420893432443868
=;0.01279084049098681
model(**inputs);0.017381530394546454
last_hidden_states;0.01776922401316536
=;0.012606322899311892
outputs.last_hidden_state;0.013651300203349113
***;0.01165593126035248
