text;attention
The;0.015628625319184394
easiest;0.013571873830377662
way;0.01377071432119022
to;0.013011921276088783
import;0.016789594900408693
the;0.013536887565062626
BERT;0.026988537456311922
language;0.01299018275125592
model;0.014561290806321148
into;0.013814477075552452
python;0.015334584345499502
for;0.013477013784935937
use;0.012969442362270217
with;0.013248422906135718
PyTorch;0.02055018009547181
is;0.014070821780493811
using;0.013469096048100078
the;0.013769991421855265
Hugging;0.016475433874527802
Face;0.015034032978575716
Transformer's;0.016975832926107968
library,;0.014704961538611637
which;0.0130023743025873
has;0.01281224891802845
built;0.01298012542391319
in;0.013564939895342869
methods;0.013396894002576059
for;0.012719809227889359
pre-training,;0.014983010402694863
inference,;0.01407601803464085
and;0.012507639003039566
deploying;0.013012998782636084
BERT.;0.01809212749378534
â€˜**;0.01871640396797701
from;0.01454632210203004
transformers;0.014021988030692113
import;0.014676051669510146
AutoTokenizer,;0.019879894517354342
BertModel;0.01961189206820049
import;0.014433259801121609
torch;0.01480835810419238
tokenizer;0.013990391665391308
=;0.013505321481562312
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07100168909622626
model;0.013649138230435613
=;0.013463904404719815
"BertModel.from_pretrained(""bert-base-uncased"")";0.03495516796005676
inputs;0.015748414493815328
=;0.013753606997535753
"tokenizer(""Hello,";0.021347895690395736
my;0.01243529914002459
dog;0.012652402132358622
is;0.012323950373953665
"cute"",";0.013772539590289972
"return_tensors=""pt"")";0.019239886439999587
outputs;0.014994288764598757
=;0.012730004209889113
model(**inputs);0.017691393685856566
last_hidden_states;0.015919869561427283
=;0.012630800381648004
outputs.last_hidden_state;0.015009818867858961
***;0.012597941719404745
