text;attention
The;0.014350700677234518
easiest;0.011866545078969044
way;0.011411954908998628
to;0.011181061258780592
import;0.01332758160592414
the;0.011949277473646167
BERT;0.0201510993173538
language;0.012009488555165634
model;0.013249491199822218
into;0.011990374888892855
python;0.015431729498715727
for;0.011587618290862852
use;0.011248562380258079
with;0.011466125913381288
PyTorch;0.019865972758065006
is;0.013464827252261058
using;0.011896866499187368
the;0.012126566388713959
Hugging;0.01521810488300781
Face;0.013246933112524147
Transformer's;0.015592878211240502
library,;0.014744040937886976
which;0.011353124580648749
has;0.01106822822140696
built;0.010902454749207207
in;0.011511550418011992
methods;0.011830969805450769
for;0.01147849214580967
pre-training,;0.015470996543073984
inference,;0.013674962834552726
and;0.010698086686398281
deploying;0.01102094634087868
BERT.;0.017924181751356398
â€˜**;0.017725985218251954
from;0.012600220122945198
transformers;0.012463578511692813
import;0.012020803142474846
AutoTokenizer,;0.025184250625631715
BertModel;0.01873977055611001
import;0.011936194343108154
torch;0.01449559490691195
tokenizer;0.015553248281179845
=;0.0116761085332479
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1495692952680602
model;0.011723623699479756
=;0.011243026443398984
"BertModel.from_pretrained(""bert-base-uncased"")";0.04788803635963035
inputs;0.012829101414060896
=;0.011661637566100771
"tokenizer(""Hello,";0.019055979598092687
my;0.01072877963043213
dog;0.012023854189180222
is;0.010594720212584702
"cute"",";0.012828530406434875
"return_tensors=""pt"")";0.01756109029400541
outputs;0.012015001644205199
=;0.010606532464466993
model(**inputs);0.015310050907409955
last_hidden_states;0.014379561562842135
=;0.010495024616832773
outputs.last_hidden_state;0.012127990342509266
***;0.010650613971030606
