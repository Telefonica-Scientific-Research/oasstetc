text;attention
The;0.01787279560825408
easiest;0.016800528524389558
way;0.015505351243250797
to;0.014592002953815787
import;0.019685656511092883
the;0.014916384830769628
BERT;0.019347437721868966
language;0.016365956117509267
model;0.015788803807221304
into;0.015580597876186195
python;0.01616498376955743
for;0.014755130432393083
use;0.014671845334736965
with;0.014319330779525193
PyTorch;0.017605335263082052
is;0.015581213368363192
using;0.015394758667741596
the;0.014097357966594911
Hugging;0.01576409224810392
Face;0.015060668876508896
Transformer's;0.01568119760277237
library,;0.015887352649084467
which;0.014365464309340055
has;0.014172128500755166
built;0.014591316736292234
in;0.014580739568186552
methods;0.014540185925478797
for;0.014235210074544294
pre-training,;0.0185018947692335
inference,;0.014918723532065796
and;0.01348580322055904
deploying;0.014116210735826813
BERT.;0.016695446943572032
â€˜**;0.017593596449239003
from;0.015486582128010155
transformers;0.015872767877607327
import;0.01572765781383262
AutoTokenizer,;0.019060587028403093
BertModel;0.0152660140948316
import;0.015004674148025992
torch;0.014315303365929604
tokenizer;0.014858393648151301
=;0.016041614839077848
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.040475920915007756
model;0.015459588049798806
=;0.015531586935322311
"BertModel.from_pretrained(""bert-base-uncased"")";0.02477070302416391
inputs;0.01489459792800542
=;0.016228958581841903
"tokenizer(""Hello,";0.022301538083374373
my;0.014039847283812168
dog;0.01372037520966756
is;0.01346528409797056
"cute"",";0.01438494597368932
"return_tensors=""pt"")";0.018602258576546134
outputs;0.014095229728378995
=;0.014641715771042368
model(**inputs);0.016097647374893987
last_hidden_states;0.015328926059258617
=;0.013678215004722862
outputs.last_hidden_state;0.014182954104270677
***;0.013230609436446772
