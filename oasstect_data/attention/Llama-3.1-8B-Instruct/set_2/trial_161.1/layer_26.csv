text;attention
The;0.012928766650611734
easiest;0.011881907146140052
way;0.012303168220226108
to;0.010881604046355823
import;0.013045448613854593
the;0.011691643230622988
BERT;0.025288947055255903
language;0.011552600326424604
model;0.012929038680407425
into;0.011969404092039697
python;0.014174266980387712
for;0.01145769831984275
use;0.01129145338648273
with;0.011431561975694832
PyTorch;0.020455366330675004
is;0.01298834950198812
using;0.012637486282062127
the;0.01264606535340799
Hugging;0.012578082732280808
Face;0.014393753143277831
Transformer's;0.01566287379059151
library,;0.013376776917101523
which;0.011499590989722835
has;0.011211584023487913
built;0.011010689261723276
in;0.011436965868507017
methods;0.01209686555884876
for;0.011272858112946024
pre-training,;0.014118009749309093
inference,;0.012730707235428247
and;0.010369654772196236
deploying;0.010856370687664122
BERT.;0.018916464750402977
â€˜**;0.026676295505228396
from;0.012375675062725175
transformers;0.011855397575530864
import;0.01237541688871756
AutoTokenizer,;0.02139800751796089
BertModel;0.019407352553455038
import;0.01196396469532364
torch;0.012778936749270987
tokenizer;0.012838610771958697
=;0.011628771370511787
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.14174864651078736
model;0.011979863471351427
=;0.011144076718884411
"BertModel.from_pretrained(""bert-base-uncased"")";0.052759919186086295
inputs;0.012620714946440443
=;0.011055219129161343
"tokenizer(""Hello,";0.01956258796211341
my;0.010740567319537467
dog;0.011333229472911916
is;0.010341530569838216
"cute"",";0.012444664187665436
"return_tensors=""pt"")";0.019037053496216705
outputs;0.01264577138877378
=;0.010421977747990668
model(**inputs);0.015253809032965025
last_hidden_states;0.01667113513697608
=;0.010477264852583082
outputs.last_hidden_state;0.012280656575191815
***;0.011096859817873674
