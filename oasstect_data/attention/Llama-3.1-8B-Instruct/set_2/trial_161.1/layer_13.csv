text;attention
The;0.01251397127420637
easiest;0.005550776278026567
way;0.005171322319298346
to;0.00511897759315491
import;0.006652734835615864
the;0.0056578225492879995
BERT;0.09445505129073345
language;0.006870214163541456
model;0.014347814231813536
into;0.008145759553935369
python;0.0146453108644501
for;0.0074870745362028025
use;0.006257230095674934
with;0.005896470490203914
PyTorch;0.060375047142933805
is;0.007550218420189663
using;0.006738431597623058
the;0.007502675213089272
Hugging;0.00887349559511938
Face;0.009352894371445562
Transformer's;0.02127444632356916
library,;0.007161381939262162
which;0.004867414063334264
has;0.004869027473108492
built;0.005987965513055074
in;0.00513468890545476
methods;0.005196967958747701
for;0.005051560908745041
pre-training,;0.010066452963424647
inference,;0.008015307305682227
and;0.005055056643634405
deploying;0.004894222075700095
BERT.;0.011202106676286983
â€˜**;0.010629449314228212
from;0.0074982609157749555
transformers;0.006598786449941964
import;0.00652234288053675
AutoTokenizer,;0.031566841478835296
BertModel;0.03741902401803139
import;0.009053135847819157
torch;0.007968455002688118
tokenizer;0.007546718463167244
=;0.008130544315758602
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2907589372019699
model;0.007473572945591762
=;0.005555112960177754
"BertModel.from_pretrained(""bert-base-uncased"")";0.05022755187153681
inputs;0.008045810986546723
=;0.005738480065173861
"tokenizer(""Hello,";0.016818004277611154
my;0.004826075596803426
dog;0.00530309405910194
is;0.004504509102761145
"cute"",";0.005562194253799019
"return_tensors=""pt"")";0.017328759223249494
outputs;0.007464234377637542
=;0.004941058881547552
model(**inputs);0.009628347980468176
last_hidden_states;0.009825501403111262
=;0.004937072088345503
outputs.last_hidden_state;0.005828954333851756
***;0.004359278537382104
