text;attention
The;0.014597910556144228
easiest;0.01720297164725568
way;0.015546736463789193
to;0.014266881250637464
import;0.020527409971457165
the;0.014640825665107354
BERT;0.02008391354973031
language;0.015607346983990796
model;0.01566815737847322
into;0.015350837511051103
python;0.016691323752633637
for;0.014375349813777402
use;0.013539977125546178
with;0.013508615999911107
PyTorch;0.018201691743631874
is;0.01479985143272247
using;0.014079290277221452
the;0.013089564107786862
Hugging;0.01495685005027213
Face;0.016852673314912196
Transformer's;0.018892232677545785
library,;0.017752159722490418
which;0.013727599988357946
has;0.01337562347628312
built;0.013497888469911665
in;0.013814869267826034
methods;0.0149877843161551
for;0.013635801925109256
pre-training,;0.017169848395393136
inference,;0.01427193681279005
and;0.012751473109658473
deploying;0.01348059688400425
BERT.;0.01655046656417566
â€˜**;0.016790848669097195
from;0.01573856266873818
transformers;0.019230458062472693
import;0.015957481914272015
AutoTokenizer,;0.018970240116324772
BertModel;0.015149584573474557
import;0.014679060759445672
torch;0.013875574202533726
tokenizer;0.014286930523763324
=;0.015440810864203928
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.04607367538972091
model;0.013559739029117782
=;0.014375538319289555
"BertModel.from_pretrained(""bert-base-uncased"")";0.03417788792345683
inputs;0.014041807229634242
=;0.013755520328474009
"tokenizer(""Hello,";0.023087002151001308
my;0.013031665542310888
dog;0.014334665934891437
is;0.012699649063558293
"cute"",";0.013727759901865005
"return_tensors=""pt"")";0.021070242748436857
outputs;0.013574363630693085
=;0.013202825837327285
model(**inputs);0.016543327188494654
last_hidden_states;0.015605246342299926
=;0.012977412656699982
outputs.last_hidden_state;0.014074710219070238
***;0.01247094800357912
