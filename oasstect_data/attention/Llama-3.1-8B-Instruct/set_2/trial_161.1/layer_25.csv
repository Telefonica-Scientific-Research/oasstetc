text;attention
The;0.015809202999484875
easiest;0.014074345534856663
way;0.013922256700699331
to;0.013358670114231835
import;0.016437112872565638
the;0.014390796558088705
BERT;0.01919115291333592
language;0.013669491980905802
model;0.014690250207530207
into;0.013955923448881438
python;0.015698352591323628
for;0.013868582807175796
use;0.0133511727443736
with;0.013696757655068826
PyTorch;0.020672483297872874
is;0.015450742215508273
using;0.013914880730113295
the;0.01449698403836622
Hugging;0.015156819163450758
Face;0.015315883172465749
Transformer's;0.016495581233023904
library,;0.015514758647907793
which;0.013502517416257695
has;0.013434521159017507
built;0.013546960933396726
in;0.013522292752640892
methods;0.014512151087407799
for;0.01357597347579028
pre-training,;0.015509216040859987
inference,;0.014548978260784724
and;0.012991198245450026
deploying;0.01374053517088164
BERT.;0.02058371210585569
â€˜**;0.018482763093812332
from;0.015136940440324797
transformers;0.013861163910536985
import;0.014265887151354005
AutoTokenizer,;0.019826730188878997
BertModel;0.01639944916502649
import;0.014673649108407164
torch;0.01556028355876556
tokenizer;0.014929894588078549
=;0.014140205286920065
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.054826716256990815
model;0.013967001268775389
=;0.013933991202979
"BertModel.from_pretrained(""bert-base-uncased"")";0.0393976770835654
inputs;0.015204713544435542
=;0.013855474077106444
"tokenizer(""Hello,";0.022089989550375493
my;0.013129578252018766
dog;0.014272236363972408
is;0.012879732503657143
"cute"",";0.014728937005527958
"return_tensors=""pt"")";0.020495669970796108
outputs;0.013952892664843232
=;0.013132770603922343
model(**inputs);0.016053749847538896
last_hidden_states;0.01617398952215146
=;0.01307684656786159
outputs.last_hidden_state;0.01598920718388368
***;0.012961601761919198
