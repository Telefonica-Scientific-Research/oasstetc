text;attention
The;0.004941345486862717
easiest;0.004567574262102382
way;0.004313839049260529
to;0.003972199562596681
import;0.005430034080928195
the;0.004571822907303038
BERT;0.007606387372614025
language;0.004376205350166858
model;0.005194146046635998
into;0.004704055436564935
python;0.005098323093921212
for;0.004727949499694601
use;0.004506157932732752
with;0.0043922260542554525
PyTorch;0.008896950998732974
is;0.004607799353703737
using;0.005376433149246814
the;0.005183579024637046
Hugging;0.005286857963439816
Face;0.005120649859402913
Transformer's;0.00778949580620586
library,;0.006163985872450056
which;0.004334964431420421
has;0.004604341482374514
built;0.004307577208837206
in;0.004497465421176346
methods;0.004590959369042423
for;0.004760437078129861
pre-training,;0.008637028509161855
inference,;0.006015433926070938
and;0.003967014970917932
deploying;0.00461913556707548
BERT.;0.00964461130478014
â€˜**;0.010182001636588882
from;0.004937640105488891
transformers;0.004963412727848786
import;0.00547160245337196
AutoTokenizer,;0.010913075597921729
BertModel;0.00847124216422974
import;0.004876223739623372
torch;0.005215669150198032
tokenizer;0.005375688450884501
=;0.004603997072882455
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4464913275372369
model;0.00480349029546204
=;0.004532389214769865
"BertModel.from_pretrained(""bert-base-uncased"")";0.2064153296457739
inputs;0.005075775091952192
=;0.004346882080334268
"tokenizer(""Hello,";0.012663022886244026
my;0.004276848005796175
dog;0.004331144228535132
is;0.003961753788350236
"cute"",";0.006046313425085283
"return_tensors=""pt"")";0.013581466936135543
outputs;0.004288353463364871
=;0.0038901194700504634
model(**inputs);0.010567236518359632
last_hidden_states;0.0075699706901744225
=;0.0038735125497119643
outputs.last_hidden_state;0.007437769469285389
***;0.004029754171893341
