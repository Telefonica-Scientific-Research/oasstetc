text;attention
The;0.010717763330407097
easiest;0.00860921411324717
way;0.00851284318265935
to;0.007637981383612952
import;0.01334277579546967
the;0.008957517327380813
BERT;0.0409235610296923
language;0.008916055125396883
model;0.013516611095297492
into;0.009733926734004729
python;0.015177702457599385
for;0.008868741206664376
use;0.008784682135205986
with;0.008538591239507459
PyTorch;0.025768070386767088
is;0.010685386994131315
using;0.010894861902889765
the;0.010043974621841452
Hugging;0.01072814596504114
Face;0.013457907756779056
Transformer's;0.017788053824884475
library,;0.011079412834099914
which;0.008208896649621713
has;0.008543978538192614
built;0.007475876902301029
in;0.008870112548207229
methods;0.008882981706966326
for;0.008829679342895882
pre-training,;0.013965579843239116
inference,;0.01071839658784125
and;0.007377098040120822
deploying;0.008129083888237581
BERT.;0.013152470708616611
â€˜**;0.013771482697570031
from;0.011077246654644955
transformers;0.011083810876753166
import;0.01106020538054082
AutoTokenizer,;0.03451829919383457
BertModel;0.021222707884142968
import;0.011999125482211171
torch;0.009830675136977177
tokenizer;0.010489954808270703
=;0.009041640532861047
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.25928013900358116
model;0.009118862344088142
=;0.00824140531727859
"BertModel.from_pretrained(""bert-base-uncased"")";0.039774359018805217
inputs;0.010338059089237295
=;0.007814952920542226
"tokenizer(""Hello,";0.01815121902368521
my;0.007295623718058758
dog;0.007813401353375065
is;0.007067535804149761
"cute"",";0.008330519572954971
"return_tensors=""pt"")";0.02048430287856165
outputs;0.008793204318935122
=;0.007357824200653992
model(**inputs);0.012921314140801964
last_hidden_states;0.012430692624433158
=;0.007373070742983191
outputs.last_hidden_state;0.009288463481803313
***;0.007191966599444626
