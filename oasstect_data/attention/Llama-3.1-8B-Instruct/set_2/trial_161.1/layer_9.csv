text;attention
The;0.012428073369932829
easiest;0.007315519624697376
way;0.0073179869310298
to;0.0065757968932769854
import;0.009749248484249013
the;0.007721946610015682
BERT;0.034821898022034184
language;0.007659948447401517
model;0.01646851288228546
into;0.014646054128882877
python;0.026438386415526836
for;0.011081853568833655
use;0.008446694149258995
with;0.007779416469216701
PyTorch;0.041249960054762586
is;0.013283471102509577
using;0.009301075805444564
the;0.007600065318745547
Hugging;0.011527202232260775
Face;0.012944752033795682
Transformer's;0.020596228465610357
library,;0.013766218006556015
which;0.0073531018820072805
has;0.006305022243780542
built;0.006480314317064046
in;0.007464448574466153
methods;0.008219354516233197
for;0.006789345093817497
pre-training,;0.016119052593094648
inference,;0.010982744668748348
and;0.006574475671203138
deploying;0.006972605282429883
BERT.;0.016883840855067576
â€˜**;0.01621689923559741
from;0.008925323186488246
transformers;0.011024001327136542
import;0.00964852034032593
AutoTokenizer,;0.040785994986805074
BertModel;0.022181657080046237
import;0.012287239787227852
torch;0.010527066065759164
tokenizer;0.00799135509287282
=;0.011360065636498036
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2360449549239442
model;0.007821084663389752
=;0.007900077440373316
"BertModel.from_pretrained(""bert-base-uncased"")";0.05063549090441074
inputs;0.00823220783602991
=;0.007001594230813807
"tokenizer(""Hello,";0.02168317320232934
my;0.006012452643710613
dog;0.006512457336723222
is;0.005873311031558398
"cute"",";0.007584959803321968
"return_tensors=""pt"")";0.01982788334656975
outputs;0.007498280553591412
=;0.0064395978463243145
model(**inputs);0.012473128524484334
last_hidden_states;0.008911469473261109
=;0.006394955934854181
outputs.last_hidden_state;0.007701058995678413
***;0.005639123855634469
