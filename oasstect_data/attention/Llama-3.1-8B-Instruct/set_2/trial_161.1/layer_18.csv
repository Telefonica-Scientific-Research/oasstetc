text;attention
The;0.01422718394962305
easiest;0.011818494645626773
way;0.011877725128376888
to;0.011254628345075004
import;0.01587291354640008
the;0.012460884863605838
BERT;0.04509767975182256
language;0.012271557972691544
model;0.020759466869848885
into;0.013688686635162352
python;0.01657971722763525
for;0.012688471625530912
use;0.011877796987304338
with;0.011746156384771899
PyTorch;0.02661176588860711
is;0.012356850767010279
using;0.012385703545611472
the;0.012620942854508812
Hugging;0.013715655409721804
Face;0.016099888070263742
Transformer's;0.020763266533409243
library,;0.014460860991216982
which;0.011848943827559029
has;0.012022184582536413
built;0.011173421392842333
in;0.011735857156010715
methods;0.011528300811173387
for;0.01169885060415104
pre-training,;0.01506424093918462
inference,;0.01291611451085203
and;0.010810684870744965
deploying;0.010716546147707017
BERT.;0.016721684043706206
â€˜**;0.014411404777492032
from;0.012836848516093765
transformers;0.015608763782822511
import;0.012426480144738435
AutoTokenizer,;0.03260058903896992
BertModel;0.02314149122893573
import;0.01257365240610606
torch;0.013270166833703342
tokenizer;0.013891353892854577
=;0.011849088080426047
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.08980940866421425
model;0.013062993395680098
=;0.011721775349029453
"BertModel.from_pretrained(""bert-base-uncased"")";0.030396191355739777
inputs;0.01526570526333942
=;0.01135618557738878
"tokenizer(""Hello,";0.02216590483322852
my;0.011247220062225661
dog;0.01201021831416942
is;0.010488147713808208
"cute"",";0.011886430433261775
"return_tensors=""pt"")";0.021169416087873597
outputs;0.013306810076305828
=;0.010912113776282283
model(**inputs);0.015580820846813784
last_hidden_states;0.015571872593861379
=;0.01086982415131454
outputs.last_hidden_state;0.012527895779026726
***;0.010568100144001683
