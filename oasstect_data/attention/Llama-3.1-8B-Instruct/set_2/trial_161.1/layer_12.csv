text;attention
The;0.008253518110510862
easiest;0.0043979795296782784
way;0.004023270237739883
to;0.0036023072827297883
import;0.005187878815250508
the;0.004416495730355715
BERT;0.021138716807761086
language;0.00400470298067125
model;0.007205267554793853
into;0.006093589045909444
python;0.009850257673599126
for;0.004945432700745882
use;0.004147243642637582
with;0.003563901949180398
PyTorch;0.01423350941851398
is;0.006565034160366032
using;0.004819395762777883
the;0.004178518669872582
Hugging;0.0063538208337078845
Face;0.00584643620508418
Transformer's;0.011248781461806528
library,;0.0064741947140334825
which;0.003894565078798476
has;0.0036839682058735883
built;0.0038630449630285726
in;0.0041037153514842965
methods;0.0037493780814936376
for;0.0037708662719602797
pre-training,;0.010029735649920095
inference,;0.006020946064092979
and;0.003699183168491141
deploying;0.003558644803950592
BERT.;0.008835306745659648
â€˜**;0.012700303135256974
from;0.00681287386680286
transformers;0.005100489172300709
import;0.005745623602792406
AutoTokenizer,;0.021679400526569605
BertModel;0.009289685503868664
import;0.007390943803830022
torch;0.004533311192277006
tokenizer;0.0059317742137426814
=;0.006327661540086429
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5060306644612917
model;0.005260220959894621
=;0.004939826982220089
"BertModel.from_pretrained(""bert-base-uncased"")";0.09035171167073534
inputs;0.005264247644695988
=;0.004962839409447656
"tokenizer(""Hello,";0.02337741854236714
my;0.003508260852828991
dog;0.003729898354107762
is;0.003235466065615509
"cute"",";0.004161742968325046
"return_tensors=""pt"")";0.014631762607351466
outputs;0.004374712275735525
=;0.003768705700375711
model(**inputs);0.012446144758682675
last_hidden_states;0.006948816784400323
=;0.0038589536501879894
outputs.last_hidden_state;0.004813586342486486
***;0.003063345739243127
