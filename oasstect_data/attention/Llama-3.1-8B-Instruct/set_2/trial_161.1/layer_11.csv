text;attention
The;0.012337396141412273
easiest;0.005571768707959524
way;0.005610246770953681
to;0.004833928579116802
import;0.00676521149321627
the;0.0062834610907606695
BERT;0.033009431501812854
language;0.004983777671606512
model;0.01048943180283661
into;0.010118124361164625
python;0.01393513011155841
for;0.007537321481729136
use;0.005958544695709535
with;0.004890863485226867
PyTorch;0.030594502000077486
is;0.00996636720782541
using;0.006900041936830628
the;0.006528959822107126
Hugging;0.007343058781834215
Face;0.00793577459605459
Transformer's;0.013934228941347514
library,;0.01016622477504939
which;0.005374541096322195
has;0.004689737416027964
built;0.006150568196178583
in;0.0052120975416831165
methods;0.005196271477986396
for;0.004919141433059418
pre-training,;0.011038133844428281
inference,;0.008778360539612149
and;0.0047306647330116955
deploying;0.004602277480135586
BERT.;0.015480554352014168
â€˜**;0.015482282228422168
from;0.007245291428135272
transformers;0.0062216931725334245
import;0.0071287872190217766
AutoTokenizer,;0.04071638460473695
BertModel;0.016395367943297632
import;0.011172925750051094
torch;0.006824086524976151
tokenizer;0.0075907806526597215
=;0.008685306163458653
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4071134648699325
model;0.00594129080682688
=;0.00619471339250717
"BertModel.from_pretrained(""bert-base-uncased"")";0.04748040631789996
inputs;0.005829785207733841
=;0.005889733192961177
"tokenizer(""Hello,";0.02094454160296926
my;0.004573610130367749
dog;0.004640901339631532
is;0.004100618305619271
"cute"",";0.0049860990015082
"return_tensors=""pt"")";0.012434877162000037
outputs;0.005473013580861406
=;0.004510410084023851
model(**inputs);0.010465531166438571
last_hidden_states;0.006948114810682421
=;0.004373168320977252
outputs.last_hidden_state;0.004895885202896313
***;0.003874785750220163
