text;attention
The;0.01241676714468318
easiest;0.01338449888941239
way;0.01086370122367456
to;0.010787327059980402
import;0.01823506304314553
the;0.01167115241132571
BERT;0.023324330155815413
language;0.011813610858579418
model;0.015567047325121326
into;0.02101925509035247
python;0.01602574397028629
for;0.013065705729724283
use;0.010464610822424142
with;0.010024834051020145
PyTorch;0.015295674379137683
is;0.01863862476258361
using;0.011414653681466401
the;0.010212115327126117
Hugging;0.01133461023109285
Face;0.013113868950049021
Transformer's;0.02008882490437464
library,;0.0171126759462403
which;0.010454504266356687
has;0.010419238244358855
built;0.008677968646480797
in;0.010689904702915724
methods;0.010559384232720126
for;0.010635451948015306
pre-training,;0.016996410271371377
inference,;0.01187304668505508
and;0.009098433739341695
deploying;0.010009826611975619
BERT.;0.017464546318640698
â€˜**;0.015737248553743498
from;0.011725058293119603
transformers;0.01312700016313293
import;0.014312725066390026
AutoTokenizer,;0.02892591164401622
BertModel;0.014328080654753255
import;0.016783945585245756
torch;0.010145927593916776
tokenizer;0.012618245645518396
=;0.013636791169803358
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1323511638981663
model;0.012424488015706284
=;0.011005416191870732
"BertModel.from_pretrained(""bert-base-uncased"")";0.05407589767372532
inputs;0.011976708218143452
=;0.009855334560206735
"tokenizer(""Hello,";0.032205145144715196
my;0.009511002064379541
dog;0.00961760348260239
is;0.009049127149604486
"cute"",";0.011685190205927258
"return_tensors=""pt"")";0.022762376357458466
outputs;0.011259614322586802
=;0.00912849303322001
model(**inputs);0.01736150043559199
last_hidden_states;0.012980868290040144
=;0.009183556010648934
outputs.last_hidden_state;0.011156229204029828
***;0.00831593974688846
