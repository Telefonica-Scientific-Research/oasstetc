text;attention
The;1.575814570624288e-11
easiest;1.8633341922679313e-11
way;1.4879223372798914e-11
to;1.6325239997159005e-11
import;1.90603116318374e-11
the;1.8302511975860033e-11
BERT;1.6515285509390743e-11
language;2.284009726586406e-11
model;1.6492717611784562e-11
into;1.6621553443640587e-11
python;2.6233764479520253e-11
for;1.543478648025614e-11
use;1.574717053531313e-11
with;1.8290466242935204e-11
PyTorch;0.002211195260936202
is;1.7642597890762293e-11
using;1.7873923231439427e-11
the;1.8244133339732633e-11
Hugging;1.5888778330976772e-11
Face;1.5534096829462e-11
Transformer's;6.316238400796137e-11
library,;2.7702152616567016e-11
which;1.657638500961793e-11
has;1.5722599225912592e-11
built;1.5709387358321576e-11
in;1.539433471335796e-11
methods;1.76328965715836e-11
for;1.602685546548788e-11
pre-training,;0.0002489679289765724
inference,;2.440297836553941e-11
and;1.6407472951206377e-11
deploying;2.1522294873748092e-11
BERT.;1.4625726663519204e-07
â€˜**;2.972476301502303e-11
from;1.691651643170191e-11
transformers;1.986986877130418e-11
import;1.6373452713147076e-11
AutoTokenizer,;0.9791794799428479
BertModel;1.5024927744212564e-05
import;1.6981726744295962e-11
torch;1.869241557043736e-11
tokenizer;2.4045112313026692e-11
=;1.7352742143881854e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.006820326853671057
model;2.0217285568848503e-11
=;1.5026011209502325e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.011524857517659559
inputs;1.8428636480038984e-11
=;1.5491319798834773e-11
"tokenizer(""Hello,";1.0420119909797905e-10
my;1.651924076062717e-11
dog;1.6380724058367968e-11
is;1.6968678054515477e-11
"cute"",";2.8572481472019048e-11
"return_tensors=""pt"")";1.2406613456904259e-10
outputs;1.630030638970318e-11
=;1.502460022634867e-11
model(**inputs);3.609937594264832e-11
last_hidden_states;3.5484651347161434e-11
=;1.668862453915466e-11
outputs.last_hidden_state;4.740979596037309e-11
***;2.1484012310234495e-11
