text;attention
The;8.942863668668213e-06
easiest;3.3454456185474206e-07
way;3.6681285487973807e-07
to;1.2618741852184314e-06
import;4.1431773891937996e-07
the;7.708620158623563e-06
BERT;3.653054053405039e-07
language;6.010775262646848e-07
model;4.4041960740463767e-07
into;5.474176515337139e-07
python;3.736383113264652e-07
for;7.176390574580491e-07
use;4.368066389441005e-07
with;7.148175583423366e-07
PyTorch;1.391440469076089e-06
is;1.0389443451034326e-06
using;5.208450841472423e-07
the;5.160740205696757e-06
Hugging;3.461178857943103e-07
Face;4.278035542537926e-07
Transformer's;9.600429405425113e-06
library,;5.057339464716812e-06
which;4.688479568237294e-07
has;5.022674180663353e-07
built;3.9203209736203436e-07
in;6.445416846187798e-07
methods;3.8914598634632124e-07
for;6.59152721449116e-07
pre-training,;1.230766445381984e-05
inference,;2.9231611023787025e-06
and;1.0790268584104425e-06
deploying;6.603120255293712e-07
BERT.;1.6895620880270123e-05
â€˜**;5.866622008778071e-06
from;6.341239765500073e-07
transformers;4.963201743876792e-07
import;4.3624371950956415e-07
AutoTokenizer,;2.6261495217470897e-05
BertModel;6.164158212779886e-07
import;4.1941977301729515e-07
torch;4.0277546660254454e-07
tokenizer;5.431562214610156e-07
=;5.592823231098284e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6212367471100471
model;4.1210416197261924e-07
=;5.360082479379442e-07
"BertModel.from_pretrained(""bert-base-uncased"")";0.37838611664629995
inputs;3.7317129987387447e-07
=;5.182551143592765e-07
"tokenizer(""Hello,";4.005643427338444e-05
my;5.047682530051134e-07
dog;3.7826794946068347e-07
is;7.888952711141893e-07
"cute"",";1.0604176415839069e-05
"return_tensors=""pt"")";0.00011044290673619893
outputs;3.3485258349536293e-07
=;4.764633181938155e-07
model(**inputs);6.881793620928673e-06
last_hidden_states;2.1770774012408576e-06
=;4.7560359213610883e-07
outputs.last_hidden_state;8.166215011239829e-05
***;5.858720754107036e-07
