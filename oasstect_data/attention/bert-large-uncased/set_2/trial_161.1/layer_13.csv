text;attention
The;0.00028329433223448397
easiest;0.00034856994721843185
way;0.00042869059422638307
to;0.00027847022543867387
import;0.00047419787798163644
the;0.00035152369469215196
BERT;0.0003861477465966287
language;0.0003632604114290917
model;0.00044149527271553887
into;0.00040545979521479776
python;0.0003949413228475227
for;0.00042485460015365443
use;0.00035903574663808904
with;0.0003912861904605868
PyTorch;0.0010402126876801498
is;0.0005912468714281628
using;0.000505004130027315
the;0.000349514253875719
Hugging;0.0003526778650294595
Face;0.00033161813277249764
Transformer's;0.0009556958707142303
library,;0.0009980123207744976
which;0.00036378228537606837
has;0.00046711413666561814
built;0.00034440029270454785
in;0.0003048253588618086
methods;0.0004888800891057204
for;0.0004048776369240982
pre-training,;0.002063463830363159
inference,;0.0005436434797479029
and;0.0003397912131876364
deploying;0.0010334319552105897
BERT.;0.0010624846562105393
â€˜**;0.0011929677957657914
from;0.0005516639838761442
transformers;0.000430000801114742
import;0.0005362492622208883
AutoTokenizer,;0.001543302921691959
BertModel;0.0006986100434894232
import;0.0007269498988222986
torch;0.00045020634100747186
tokenizer;0.0004059276657049794
=;0.00047114849971604644
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5190140904069765
model;0.0003686523588750697
=;0.00047722816347883556
"BertModel.from_pretrained(""bert-base-uncased"")";0.4184501883480732
inputs;0.0005472245933221227
=;0.00045435943216067405
"tokenizer(""Hello,";0.006411337868011125
my;0.0003644999879477036
dog;0.00039877656110357825
is;0.00038911823746957155
"cute"",";0.0014280072129306879
"return_tensors=""pt"")";0.016302042517398592
outputs;0.00040072594198312827
=;0.00041305167738897006
model(**inputs);0.005303344917086556
last_hidden_states;0.0012005976698712497
=;0.0006623226607104383
outputs.last_hidden_state;0.0019146827314218192
***;0.0006208166758730115
