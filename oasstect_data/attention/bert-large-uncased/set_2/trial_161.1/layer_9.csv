text;attention
The;8.51891276625075e-06
easiest;1.1522855372067594e-06
way;1.1217932514166462e-06
to;1.8790699000978357e-06
import;1.4663929709403651e-06
the;1.7310102043668595e-05
BERT;1.0761273354366367e-06
language;1.203841910637176e-06
model;1.1758765120593384e-06
into;1.2181962519317804e-06
python;1.5097815797684744e-06
for;1.3070081955465733e-06
use;1.1231823094748935e-06
with;1.2940574044634673e-06
PyTorch;4.456622851349518e-06
is;1.7769810135789743e-06
using;1.255610029768511e-06
the;3.086255916914724e-05
Hugging;1.09050858713796e-06
Face;1.1445870280326054e-06
Transformer's;3.3707780793501584e-05
library,;9.674538640375401e-06
which;1.3012707490480312e-06
has;1.3253955735750193e-06
built;1.472477766982462e-06
in;1.2201012514022756e-06
methods;1.2524607441427017e-06
for;1.4160994283023365e-06
pre-training,;7.31672416138902e-06
inference,;4.248067693545869e-06
and;2.088553686621108e-06
deploying;2.2631992584163542e-06
BERT.;2.4155040467633745e-05
â€˜**;0.0008627525181178471
from;1.5382350216780503e-06
transformers;1.3227582668958187e-06
import;1.8678921726080823e-06
AutoTokenizer,;1.3101799190557318e-05
BertModel;2.68771277806121e-06
import;1.7908311584246204e-06
torch;1.0837774416799052e-06
tokenizer;2.1180197758676423e-06
=;1.5940922622986282e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6550922105701897
model;1.3379155174984e-06
=;1.8028326869666938e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.3433544248163562
inputs;1.3377515698111131e-06
=;1.7319701605593281e-06
"tokenizer(""Hello,";0.00010413253801465519
my;1.4829565467255847e-06
dog;1.194171064337232e-06
is;1.7372339654994762e-06
"cute"",";1.5324005426797142e-05
"return_tensors=""pt"")";0.00020942829851594764
outputs;1.5298137897540502e-06
=;1.5498069230174116e-06
model(**inputs);1.630155236387132e-05
last_hidden_states;7.650575914974326e-06
=;1.3687473507273162e-06
outputs.last_hidden_state;0.00011890221424621435
***;3.3093863480743924e-06
