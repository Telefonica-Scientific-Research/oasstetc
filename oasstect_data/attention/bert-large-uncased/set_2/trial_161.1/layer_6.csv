text;attention
The;0.0009107851263956778
easiest;7.474111961618232e-07
way;6.871077427938594e-07
to;8.167549889639028e-06
import;8.689325649446762e-07
the;0.011646470217212258
BERT;8.692356034130927e-07
language;1.1155980689283146e-06
model;8.406927210198026e-07
into;9.286249480091037e-07
python;1.424980929566386e-06
for;9.331607986229187e-07
use;8.73697119867655e-07
with;9.516262092433864e-07
PyTorch;1.2108306064400082e-06
is;1.847143400263984e-06
using;9.448346119788006e-07
the;0.22948466361119307
Hugging;8.915738161135949e-07
Face;9.670979255436838e-07
Transformer's;0.00018732785953735228
library,;1.5421304913989555e-05
which;6.744827253852176e-07
has;1.0600933498341554e-06
built;8.777521870165758e-07
in;8.706835638974755e-07
methods;8.320651987415218e-07
for;9.450011546384235e-07
pre-training,;1.9438068148116784e-05
inference,;1.2303797205976401e-05
and;1.2840662476207788e-06
deploying;1.1516795028649734e-06
BERT.;0.0004649325370905279
â€˜**;0.03660915410837072
from;1.3585245158224147e-06
transformers;9.745031210995377e-07
import;9.263449367000772e-07
AutoTokenizer,;1.1822819962682225e-05
BertModel;1.1729343149375933e-06
import;9.2191667115075e-07
torch;7.71319966806283e-07
tokenizer;1.3091766401309192e-06
=;1.3440075719509177e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.22679803408480298
model;9.053191955694862e-07
=;1.4132556790697694e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.4933057622866825
inputs;7.798082403096242e-07
=;1.2603930114606514e-06
"tokenizer(""Hello,";5.171364409357196e-05
my;8.075711334757353e-07
dog;7.360371389539787e-07
is;1.2542226498362078e-06
"cute"",";0.0001446457081627084
"return_tensors=""pt"")";1.9311631631126656e-05
outputs;7.817772757973476e-07
=;8.697381548514378e-07
model(**inputs);2.806401375053054e-06
last_hidden_states;2.826475689546869e-06
=;7.832148491171057e-07
outputs.last_hidden_state;0.00026119814735624204
***;1.0461830261404341e-06
