text;attention
The;0.0003761227824612732
easiest;3.309727589251123e-07
way;2.831515442753417e-07
to;1.371784723775164e-06
import;3.786327380423447e-07
the;0.0013466706825314841
BERT;2.7211702746016705e-07
language;4.6742260306648616e-07
model;2.9972102514196976e-07
into;3.453575608486152e-07
python;4.925292174354514e-07
for;2.805647780749667e-07
use;3.408086498856882e-07
with;2.726471849532015e-07
PyTorch;4.181579369334146e-07
is;5.527568520375303e-07
using;3.324790081458767e-07
the;0.75211472489278
Hugging;3.2127693842623e-07
Face;2.843081916465647e-07
Transformer's;0.00036303557751143754
library,;6.809995359549076e-06
which;2.7068544288025584e-07
has;3.793366640010944e-07
built;3.435521311385058e-07
in;2.9552637871286827e-07
methods;3.638524034046395e-07
for;3.140888743015876e-07
pre-training,;4.374466838466986e-06
inference,;4.260080186390556e-06
and;3.8442305744457675e-07
deploying;5.278356340680112e-07
BERT.;6.709798249202206e-05
â€˜**;0.13804455442762661
from;4.2863319434352267e-07
transformers;4.1914778246226505e-07
import;3.60216040492684e-07
AutoTokenizer,;2.1830256602245535e-06
BertModel;3.593948545188781e-07
import;3.4958774782077284e-07
torch;2.686190844901175e-07
tokenizer;7.260804887910401e-07
=;7.229628276948834e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.042039432582876006
model;3.237117402695857e-07
=;1.031763304355134e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.0655216531394962
inputs;4.042590716501702e-07
=;6.508581649176067e-07
"tokenizer(""Hello,";1.615212171621903e-05
my;3.118832899313585e-07
dog;3.42850548667672e-07
is;3.5826669826939187e-07
"cute"",";1.5776691028194396e-05
"return_tensors=""pt"")";1.2186083432174457e-05
outputs;3.9670695668593637e-07
=;4.324622924102388e-07
model(**inputs);1.1716692730988326e-06
last_hidden_states;1.0507689123973136e-06
=;3.7257370650715653e-07
outputs.last_hidden_state;4.419353044650256e-05
***;3.6553225248846705e-07
