text;attention
The;0.0004804438210065493
easiest;0.0001535995659315462
way;0.00016349965456335946
to;0.00020513235335828747
import;0.00019598349673661055
the;0.00030414538970800124
BERT;0.00019872993626181596
language;0.00038763233053765926
model;0.00019309317900375746
into;0.00018817522765394464
python;0.00017693183247736137
for;0.0001782288652634195
use;0.00020304817650206562
with;0.00016654623678154405
PyTorch;0.0006206426271612632
is;0.00018844460148951495
using;0.00022552589985571308
the;0.00030228468729325285
Hugging;0.00014892570800485478
Face;0.00018071095987224839
Transformer's;0.0006215699985586794
library,;0.0005676941160394437
which;0.00016534117020400134
has;0.00018058896323188816
built;0.0002206685338679042
in;0.0001760213513390536
methods;0.00017514451205980684
for;0.00016439009013421925
pre-training,;0.0015514385098104751
inference,;0.00040799956677481895
and;0.00018588551537921077
deploying;0.00027796622414697403
BERT.;0.0011261435829728785
â€˜**;0.00045938993584417867
from;0.00016318376771794465
transformers;0.00020178140634850036
import;0.00020799447355609733
AutoTokenizer,;0.0020761334519870185
BertModel;0.00045405414599330286
import;0.0002033556945168486
torch;0.00022434755068047817
tokenizer;0.00026423894333426094
=;0.00023317357905247344
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.556494718048609
model;0.00017538303775768287
=;0.00025367938637904327
"BertModel.from_pretrained(""bert-base-uncased"")";0.40515261570797945
inputs;0.00015237142085333398
=;0.00022785333309497492
"tokenizer(""Hello,";0.0020785705044922944
my;0.00022097760579493185
dog;0.00023787073628379554
is;0.00018158515338366764
"cute"",";0.0005630253261657715
"return_tensors=""pt"")";0.005859013285980624
outputs;0.0001517982145529041
=;0.0002164620686316306
model(**inputs);0.0013122486230664064
last_hidden_states;0.0008238675379400592
=;0.00027979128161734424
outputs.last_hidden_state;0.010073858358021989
***;0.00037408073638214715
