text;attention
The;1.1510335205440745e-10
easiest;9.370563472022822e-11
way;9.162406526642272e-11
to;2.572940794211639e-09
import;1.1064278256351654e-10
the;1.1786541597258896e-10
BERT;9.287202907079425e-11
language;1.9472092239580342e-10
model;1.0373945657698124e-10
into;9.136869200349357e-11
python;2.3374029780696475e-10
for;8.944045584697577e-11
use;8.782749582682925e-11
with;9.397720453407433e-11
PyTorch;9.142235007191198e-08
is;9.27631439542501e-11
using;9.450435380016713e-11
the;1.1357374419990761e-10
Hugging;9.813829937014681e-11
Face;1.097466467790865e-10
Transformer's;3.629304631199194e-10
library,;1.2396294295959458e-08
which;1.0125669079438383e-10
has;9.863324975038531e-11
built;9.116559443638714e-11
in;9.070721991637145e-11
methods;9.815544867313381e-11
for;9.244039178282315e-11
pre-training,;4.317236098070392e-06
inference,;9.450294318037388e-09
and;8.927401906477768e-11
deploying;1.2858701305911197e-10
BERT.;3.6348697189646405e-08
â€˜**;2.792791102658365e-10
from;9.586580807306057e-11
transformers;1.7724759477463906e-10
import;9.951824695279469e-11
AutoTokenizer,;0.001840457554378932
BertModel;6.656795007009864e-08
import;1.0271667607074502e-10
torch;1.4731574818059513e-10
tokenizer;1.338600175758855e-10
=;9.123231444028112e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.9568519946840562
model;1.1071100395706667e-10
=;9.104834942946525e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.0413026693318171
inputs;1.164930477297509e-10
=;9.259599202940627e-11
"tokenizer(""Hello,";3.0435691708011064e-08
my;9.832546854556015e-11
dog;1.0725657367763199e-10
is;9.468287102658459e-11
"cute"",";2.5243470073686164e-07
"return_tensors=""pt"")";5.2956085206361074e-08
outputs;8.734485655012326e-11
=;9.317541241547298e-11
model(**inputs);3.322415268838045e-10
last_hidden_states;2.6356954297612137e-10
=;9.169444644918697e-11
outputs.last_hidden_state;5.27962696423053e-10
***;1.960040439625289e-10
