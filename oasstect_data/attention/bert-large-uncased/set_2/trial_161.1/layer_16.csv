text;attention
The;0.0009391903210471182
easiest;0.0009970166883072676
way;0.0008505207928958366
to;0.0007846351675055254
import;0.0011853935769725453
the;0.0009964896522860532
BERT;0.0008110477823332095
language;0.000879722342135881
model;0.001057847874202143
into;0.0008169487433834147
python;0.0011441078560109124
for;0.0008102660064887181
use;0.0007548701533380986
with;0.000994717741919934
PyTorch;0.0022545983278281275
is;0.0010952987663214923
using;0.0011832785726888927
the;0.0011554547951311172
Hugging;0.0007431369662910613
Face;0.0009728433676849354
Transformer's;0.25915286749494043
library,;0.001843170820306657
which;0.0008724492214302835
has;0.0010379943135861616
built;0.0008320754418340959
in;0.0007202086724112905
methods;0.0009284690409999244
for;0.0008254458232725952
pre-training,;0.011171518753589812
inference,;0.0011966054064001855
and;0.0007775329923260365
deploying;0.0010028612058923117
BERT.;0.0034495524753012973
â€˜**;0.0024525629028209015
from;0.0012493766139655216
transformers;0.0009160707202519622
import;0.0009805651121987367
AutoTokenizer,;0.003488026107422248
BertModel;0.0011843719051474425
import;0.0008255944464744811
torch;0.0009202558352776998
tokenizer;0.0011968601287985238
=;0.0012241820704424766
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.33051284515534923
model;0.0009887020830453405
=;0.0012818385290095688
"BertModel.from_pretrained(""bert-base-uncased"")";0.27961346104981616
inputs;0.001285516589883523
=;0.0010290327885139536
"tokenizer(""Hello,";0.010268664785439939
my;0.0007042239712840246
dog;0.0006403570177044124
is;0.0007107941387431064
"cute"",";0.00206295691170349
"return_tensors=""pt"")";0.03246230961070538
outputs;0.0008459422228373277
=;0.0009379998195706867
model(**inputs);0.008589675653456452
last_hidden_states;0.0023621594266644967
=;0.0012040258592610972
outputs.last_hidden_state;0.004577060848496684
***;0.001246430538651609
