text;attention
The;9.83354966981778e-11
easiest;1.0784983813802327e-10
way;1.1263550637970806e-10
to;1.0859289042700529e-10
import;1.3372932292536987e-10
the;1.0982770848489678e-10
BERT;9.98052823094953e-11
language;1.5180488179728145e-10
model;1.2322843386294053e-10
into;9.781834274139405e-11
python;1.8838776255736181e-10
for;1.0159080062859452e-10
use;1.0311618093729696e-10
with;1.3095467993273032e-10
PyTorch;0.49764682853983894
is;1.2497430474196892e-10
using;1.0279873669093892e-10
the;1.0821719014643397e-10
Hugging;1.0121496789620868e-10
Face;1.0682080737261729e-10
Transformer's;1.7081514699091742e-09
library,;1.9595763400852343e-10
which;1.0004273937707507e-10
has;1.1201320944801473e-10
built;1.0844580228327992e-10
in;9.620679464900592e-11
methods;1.5214380988685458e-10
for;1.0691883115072267e-10
pre-training,;0.0006040758089237703
inference,;1.3400628685776965e-10
and;9.66127865577732e-11
deploying;1.4891198192973512e-10
BERT.;1.673581968541679e-08
â€˜**;2.741417906329763e-10
from;1.333506688966525e-10
transformers;1.2916633519897407e-10
import;1.2016086722137749e-10
AutoTokenizer,;0.0008376710036061069
BertModel;6.50053830770981e-07
import;1.1490833077059881e-10
torch;1.1672646579933168e-10
tokenizer;2.124323313739019e-10
=;1.3590121461836094e-10
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.021627091633612133
model;1.9446540415375844e-10
=;1.266020622820679e-10
"BertModel.from_pretrained(""bert-base-uncased"")";0.47928365420908225
inputs;1.6348570342583886e-10
=;1.1936126461002665e-10
"tokenizer(""Hello,";1.3247467765081562e-09
my;1.1065955783817872e-10
dog;1.1864205264713308e-10
is;1.0345625676244114e-10
"cute"",";2.1169300108849756e-10
"return_tensors=""pt"")";1.551816110625854e-09
outputs;1.1075293141229861e-10
=;1.0480510293374157e-10
model(**inputs);3.8761903098300937e-10
last_hidden_states;2.6375800836470333e-10
=;1.1351122463416157e-10
outputs.last_hidden_state;3.939046896443984e-10
***;2.0810494381857204e-10
