text;attention
The;8.291790313060084e-05
easiest;0.00010188511690019831
way;9.592771467317905e-05
to;0.00010485061564384598
import;0.00015184301239875537
the;0.00013434870079411445
BERT;0.00015549850274862868
language;0.00019284655492547942
model;0.0001658417127213754
into;9.169593638892699e-05
python;0.0001777193219387846
for;7.938156605774018e-05
use;8.313690308503259e-05
with;0.00010171426774001227
PyTorch;0.0006909173393514791
is;0.00011144623540656618
using;9.676448608581053e-05
the;0.00010705993068521769
Hugging;7.462656931946809e-05
Face;9.700244421554384e-05
Transformer's;0.32057535866322073
library,;0.0002205687060295004
which;9.122537386267326e-05
has;0.00010096794337686733
built;0.0001067635425004348
in;7.321493874259166e-05
methods;0.00012018222018923392
for;0.00010031189608484974
pre-training,;0.004702584626507327
inference,;0.00011895442117417942
and;0.00012083010779556747
deploying;0.00012090865350757236
BERT.;0.0006081200875038108
â€˜**;0.00028016421553853126
from;0.0001693623897873737
transformers;0.0001423777948782134
import;0.00011954764657955902
AutoTokenizer,;0.0007745461842908433
BertModel;0.00021884978741298848
import;9.067384647973363e-05
torch;0.00010767045712733366
tokenizer;0.0002211160197140834
=;0.00011035055425843152
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5206102591935211
model;0.00021620736857394754
=;0.00010931300966989458
"BertModel.from_pretrained(""bert-base-uncased"")";0.13768198459368616
inputs;0.00015323016144767155
=;9.205743867936944e-05
"tokenizer(""Hello,";0.0008777025955068029
my;8.020306138854319e-05
dog;8.561570253996915e-05
is;7.98106963628593e-05
"cute"",";0.00022573705047663242
"return_tensors=""pt"")";0.00601427538054093
outputs;9.192652850704752e-05
=;7.976725932234395e-05
model(**inputs);0.0003821017899886163
last_hidden_states;0.00029043320508473683
=;9.301816824497318e-05
outputs.last_hidden_state;0.0004489062974016026
***;0.00019537558828350743
