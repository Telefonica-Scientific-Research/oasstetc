text;attention
The;6.118338570790405e-05
easiest;4.077334798630653e-06
way;3.3705571126829947e-06
to;7.968748701137031e-06
import;3.705643550168072e-06
the;0.00014906279196174983
BERT;3.3018983005445374e-06
language;4.52006500333375e-06
model;3.8140812415749635e-06
into;4.1857243668631275e-06
python;1.0458594494810341e-05
for;3.959717607810805e-06
use;3.610111624260065e-06
with;3.4348270640519448e-06
PyTorch;6.548974953350107e-06
is;6.353266891354981e-06
using;3.961510094744242e-06
the;0.0012791571986217835
Hugging;3.3085433279742e-06
Face;3.4996164624999258e-06
Transformer's;0.00027984244808388134
library,;4.040412898051455e-05
which;3.817050053054006e-06
has;4.068096603076874e-06
built;5.1809831994596684e-06
in;4.143898653419004e-06
methods;4.970389516717556e-06
for;4.683010582453473e-06
pre-training,;2.902150378359578e-05
inference,;1.884626276828489e-05
and;5.4735257446367625e-06
deploying;6.2932559275294806e-06
BERT.;0.0001904339219978207
â€˜**;0.11711233961202303
from;4.9364554369637335e-06
transformers;4.374511958166296e-06
import;4.516260553191532e-06
AutoTokenizer,;4.1457660611084654e-05
BertModel;5.423048506896416e-06
import;4.121534243055795e-06
torch;3.3594754966796815e-06
tokenizer;6.568349463519522e-06
=;1.027572874045463e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7081726783605649
model;3.6911668035221686e-06
=;1.1636208272430081e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.1713272910185651
inputs;4.80593105979935e-06
=;9.531230918430183e-06
"tokenizer(""Hello,";0.00015245429052532545
my;3.298241165759227e-06
dog;3.215444994049515e-06
is;4.06694402857035e-06
"cute"",";3.1070274892699524e-05
"return_tensors=""pt"")";0.000549255600615107
outputs;5.086045719888878e-06
=;6.5493791725511065e-06
model(**inputs);1.9513889070712902e-05
last_hidden_states;1.4313994028682822e-05
=;5.493839960249422e-06
outputs.last_hidden_state;0.0003057905379318784
***;6.223896895555947e-06
