text;attention
The;6.085393035826388e-08
easiest;1.3387414273880232e-07
way;7.870573685797869e-08
to;6.082861430368043e-08
import;9.279588081545306e-08
the;6.214954464140171e-08
BERT;8.675013330803878e-08
language;8.57837930960635e-08
model;7.88807450431091e-08
into;7.09332181285449e-08
python;1.4559035369522784e-07
for;6.427993525257491e-08
use;8.779134311959418e-08
with;6.508081481658668e-08
PyTorch;1.241331138584786e-06
is;6.309541740689868e-08
using;8.274959470259085e-08
the;6.051653335058707e-08
Hugging;9.040874094375413e-08
Face;7.07769437206437e-08
Transformer's;1.3546236197288985e-06
library,;1.7934650875878203e-07
which;7.554271646633695e-08
has;6.894951319425753e-08
built;9.462443481609341e-08
in;6.594015529424535e-08
methods;1.0065970233717003e-07
for;6.393493271240192e-08
pre-training,;1.005576409777431e-06
inference,;1.8288380168743442e-07
and;5.989423559901938e-08
deploying;2.98981448201045e-07
BERT.;1.661735668827435e-07
â€˜**;8.604330478729702e-07
from;5.9603160448213154e-08
transformers;2.1924163032235181e-07
import;8.430696828596885e-08
AutoTokenizer,;3.369779094733418e-06
BertModel;5.233556153772154e-07
import;8.048969651373486e-08
torch;9.59463629424224e-08
tokenizer;2.405570586304479e-07
=;6.885461135813826e-08
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.9477669066399523
model;5.78463528381058e-08
=;6.069098398053871e-08
"BertModel.from_pretrained(""bert-base-uncased"")";0.052108829786153325
inputs;8.380078931527741e-08
=;6.150074684681085e-08
"tokenizer(""Hello,";3.1069045919494935e-06
my;5.3033531607883494e-08
dog;6.626160086136561e-08
is;5.351688574791258e-08
"cute"",";2.9431131182823986e-07
"return_tensors=""pt"")";4.6019298317356875e-05
outputs;1.0362703782141483e-07
=;7.171101405846671e-08
model(**inputs);8.838549594940105e-06
last_hidden_states;5.414387265221135e-06
=;7.127290324543354e-08
outputs.last_hidden_state;4.741818163010611e-05
***;3.858044895491037e-07
