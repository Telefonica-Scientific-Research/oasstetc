text;attention
The;2.2182835464410228e-11
easiest;1.8278298145923446e-11
way;1.7241206893378408e-11
to;2.5237730924628585e-09
import;1.9064065516123173e-11
the;2.3465110019094313e-11
BERT;2.1721130777672018e-11
language;2.030399242632411e-11
model;1.8834323178069916e-11
into;1.731784434465608e-11
python;2.0454775616873804e-11
for;1.888824129599249e-11
use;1.7618138434060297e-11
with;1.8155057217232704e-11
PyTorch;6.6690780175309905e-09
is;1.8532237357044787e-11
using;1.6799335537767755e-11
the;2.1928800640793954e-11
Hugging;1.8255343789111745e-11
Face;2.0692533147447197e-11
Transformer's;5.965961898233868e-11
library,;3.449019423595597e-09
which;1.6967698690583903e-11
has;1.787067352179934e-11
built;1.7962926509916323e-11
in;1.6995755320382304e-11
methods;1.8715060443179913e-11
for;1.8134006934950018e-11
pre-training,;9.918919420820205e-07
inference,;3.430246986461231e-09
and;1.7223716330738613e-11
deploying;2.274502206549482e-11
BERT.;3.94334853791167e-09
â€˜**;3.5260022435363835e-11
from;1.8003474191143886e-11
transformers;2.5984521819472943e-11
import;1.8330498304731638e-11
AutoTokenizer,;0.00018751791805679736
BertModel;5.196136094389487e-09
import;1.8279375120196563e-11
torch;3.92745923417048e-11
tokenizer;2.5218873365191707e-11
=;1.770113999056686e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.9941463424740016
model;2.2335378835575166e-11
=;1.7600495233510487e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.005664454914190071
inputs;2.0949855772202896e-11
=;1.7430302789409066e-11
"tokenizer(""Hello,";1.1530377548067309e-08
my;1.7745380467039385e-11
dog;1.8720119940000004e-11
is;1.787921560442541e-11
"cute"",";6.224730349236207e-07
"return_tensors=""pt"")";3.243959308463023e-08
outputs;1.788434761070401e-11
=;1.771328590313082e-11
model(**inputs);5.959731147382251e-11
last_hidden_states;4.37429525423257e-11
=;1.8446730395523513e-11
outputs.last_hidden_state;6.74531070423163e-11
***;3.564315592013546e-11
