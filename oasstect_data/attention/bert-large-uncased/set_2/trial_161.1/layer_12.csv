text;attention
The;2.530789719955183e-05
easiest;2.0401584880617225e-05
way;2.3593640382522014e-05
to;2.152660368509732e-05
import;2.7430802584185885e-05
the;2.748124133485975e-05
BERT;2.348746389464343e-05
language;2.3352692713281873e-05
model;2.41459402395236e-05
into;2.338431994559348e-05
python;2.2696136079087164e-05
for;2.5616109163870106e-05
use;2.378343467671853e-05
with;2.6145273632850714e-05
PyTorch;6.271382124092479e-05
is;4.267956522520791e-05
using;2.8315061678883974e-05
the;4.4001102527772115e-05
Hugging;2.8376410452154924e-05
Face;2.661910419365974e-05
Transformer's;8.382687997337113e-05
library,;4.71712799818898e-05
which;2.434073015518713e-05
has;3.390775898826071e-05
built;2.0410841570349865e-05
in;2.0525329682408707e-05
methods;2.7415612706310344e-05
for;3.7035818193489035e-05
pre-training,;0.00010330416092768384
inference,;2.9912385319970793e-05
and;2.9601933808873e-05
deploying;4.9289338781365396e-05
BERT.;8.030001438374093e-05
â€˜**;0.0001112550249807478
from;2.9521830417437194e-05
transformers;2.634304696855949e-05
import;3.422009443922392e-05
AutoTokenizer,;0.00012532633804724237
BertModel;5.136263725984551e-05
import;4.0100657687912376e-05
torch;2.6331692391498244e-05
tokenizer;3.4273873973354234e-05
=;3.574765796078615e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5096408176234212
model;2.2788607573848657e-05
=;3.3218500587869475e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.4854772758407254
inputs;2.384993509482255e-05
=;3.348231130266104e-05
"tokenizer(""Hello,";0.0003641739257173345
my;2.2935026698774752e-05
dog;2.17802953562717e-05
is;2.1763879878920027e-05
"cute"",";6.871906953100924e-05
"return_tensors=""pt"")";0.0017980453596657489
outputs;2.589244603110466e-05
=;3.2109379209014294e-05
model(**inputs);0.00029513440839072825
last_hidden_states;9.158416281579929e-05
=;2.906039174412752e-05
outputs.last_hidden_state;0.0002704591248599584
***;5.432656706490188e-05
