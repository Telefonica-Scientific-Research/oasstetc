text;attention
The;2.936942644560568e-11
easiest;2.2750331015495516e-11
way;2.0126515378871355e-11
to;2.1129186534648975e-11
import;2.60775201562729e-11
the;3.953512300899442e-11
BERT;2.3153974792548766e-11
language;4.146167103913154e-11
model;2.4376794790226983e-11
into;2.3011229414388353e-11
python;4.4439102680989554e-11
for;2.3118317433685856e-11
use;2.2266581031362193e-11
with;2.6200666705882606e-11
PyTorch;9.180523068742795e-05
is;2.4460338111542853e-11
using;2.051622155398986e-11
the;2.9919341639812784e-11
Hugging;2.0932238020852375e-11
Face;2.3580730227032342e-11
Transformer's;9.859740669368014e-11
library,;4.161440543336595e-11
which;2.0628763080128137e-11
has;2.1678108216702873e-11
built;2.1508057552203083e-11
in;2.259445670541674e-11
methods;2.1927007806971743e-11
for;2.0656958852754958e-11
pre-training,;4.090155538024043e-05
inference,;3.8218525141678965e-11
and;2.44931778568974e-11
deploying;3.0261708544026257e-11
BERT.;5.815683301706346e-07
â€˜**;4.712410816077737e-11
from;2.4887995940220243e-11
transformers;3.458904310527117e-11
import;2.2602677995188616e-11
AutoTokenizer,;0.9836550767150161
BertModel;7.869123296131777e-06
import;2.7025389314023888e-11
torch;3.09557262342384e-11
tokenizer;3.6175024502500165e-11
=;2.2627307892677394e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.007315891954853667
model;2.813237123692917e-11
=;2.0296338415664552e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.008887871564303668
inputs;3.340007520682784e-11
=;2.2150729641329443e-11
"tokenizer(""Hello,";1.1060911845254992e-10
my;2.4107622483987217e-11
dog;2.5982663774768794e-11
is;2.9624164396317e-11
"cute"",";5.871795685383938e-11
"return_tensors=""pt"")";4.6268527890074934e-10
outputs;2.3610760017543954e-11
=;2.2277462679397056e-11
model(**inputs);8.746652505553432e-11
last_hidden_states;6.602216592256078e-11
=;2.506479131374216e-11
outputs.last_hidden_state;9.844636516800419e-11
***;3.4947144415098876e-11
