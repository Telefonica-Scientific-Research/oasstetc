text;attention
The;2.4030730449850607e-06
easiest;1.7240698006044612e-06
way;1.685087775671823e-06
to;1.8413608596916393e-06
import;2.113433244540816e-06
the;2.581825250899161e-06
BERT;1.5256225120229054e-06
language;2.1249634164596863e-06
model;2.847833160055025e-06
into;2.2359567927286036e-06
python;2.120549300649546e-06
for;1.919484737045397e-06
use;2.015931128139026e-06
with;1.942674379918558e-06
PyTorch;7.772445284944425e-06
is;2.5359528410194605e-06
using;2.292923080894057e-06
the;2.850040008795593e-06
Hugging;1.753887239072079e-06
Face;2.165601978812813e-06
Transformer's;1.656152750408232e-05
library,;7.372681103781925e-06
which;2.0704171178968116e-06
has;2.2730969130680866e-06
built;1.9386475934702127e-06
in;2.2979610283942506e-06
methods;2.2386889662583826e-06
for;2.106951444058068e-06
pre-training,;2.2402768250138452e-05
inference,;3.808678242812129e-06
and;2.95944284354527e-06
deploying;3.1715186359262966e-06
BERT.;6.90682373349142e-06
â€˜**;1.2546642865541502e-05
from;1.7387753139730553e-06
transformers;2.537928595861727e-06
import;1.7854812437931084e-06
AutoTokenizer,;2.5537861222514523e-05
BertModel;3.8985006788652375e-06
import;1.7771398809598598e-06
torch;1.821500488506044e-06
tokenizer;3.6169287336511273e-06
=;2.879407873785632e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7567179069496397
model;2.2999602774837606e-06
=;2.5183768625135154e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.24264563819012547
inputs;1.606304263144895e-06
=;2.712895478858851e-06
"tokenizer(""Hello,";8.023027663701002e-05
my;2.3353608911600115e-06
dog;1.8408947948903784e-06
is;2.2034312123196767e-06
"cute"",";7.934011229118749e-06
"return_tensors=""pt"")";0.00021417076528046174
outputs;1.3109781010890072e-06
=;1.8028648025460478e-06
model(**inputs);1.8308850464525767e-05
last_hidden_states;1.3573357697823308e-05
=;2.561857621051489e-06
outputs.last_hidden_state;9.345979804024905e-05
***;4.882790469062006e-06
