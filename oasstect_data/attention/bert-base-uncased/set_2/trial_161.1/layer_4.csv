text;attention
The;1.4656018695625892e-05
easiest;2.5152745008379392e-05
way;1.496351489778633e-05
to;1.4184714898385927e-05
import;1.9181218616637984e-05
the;1.4031281188665859e-05
BERT;1.3179759439955054e-05
language;1.6103365220402242e-05
model;1.2886090103228874e-05
into;1.8187577551337715e-05
python;3.3236064248050556e-05
for;1.6294792954478843e-05
use;1.4206588963851253e-05
with;1.8067730852539517e-05
PyTorch;4.6857004176348715e-05
is;1.8155029499488947e-05
using;1.8975139819717338e-05
the;1.5449594798282402e-05
Hugging;2.4433757515722934e-05
Face;1.3912152012365122e-05
Transformer's;5.921451795869937e-05
library,;5.028378908122265e-05
which;1.6054772152332615e-05
has;1.7674429313270776e-05
built;2.1427615839788324e-05
in;1.3203077051031129e-05
methods;2.123690202468716e-05
for;1.8689478660043862e-05
pre-training,;8.575733004190641e-05
inference,;2.8797370002691187e-05
and;1.3498989802324475e-05
deploying;2.894637704676424e-05
BERT.;2.202517060797251e-05
â€˜**;6.18224113778285e-05
from;1.421290479603338e-05
transformers;3.486841122084714e-05
import;2.0695239199711353e-05
AutoTokenizer,;0.00010045944550275915
BertModel;3.559212993535784e-05
import;2.0342694804004444e-05
torch;1.5115590683903325e-05
tokenizer;1.8310787118963594e-05
=;1.988123667117297e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6663018062991295
model;1.3735074882344447e-05
=;2.6510991823594884e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.3306376740700525
inputs;1.9757828782939647e-05
=;2.7837004127848272e-05
"tokenizer(""Hello,";0.00018354336929865935
my;1.4767938946584508e-05
dog;1.3197081741483324e-05
is;1.3057426739789465e-05
"cute"",";4.655245408803037e-05
"return_tensors=""pt"")";0.0011214655665773969
outputs;1.303515960593869e-05
=;1.6711729246153725e-05
model(**inputs);0.0001197203465872547
last_hidden_states;0.0001068621129951475
=;1.545557422761122e-05
outputs.last_hidden_state;0.0001583040704716115
***;2.9781089321019126e-05
