text;attention
The;1.769786831251037e-06
easiest;2.035318023879186e-06
way;1.6302494240824707e-06
to;1.433264209266584e-06
import;1.7127555249697667e-06
the;1.7317844399339674e-06
BERT;1.5012841483085407e-06
language;1.7958905833355695e-06
model;1.4736543843475674e-06
into;1.7387413209522827e-06
python;1.935396598168944e-06
for;1.7531076500920217e-06
use;1.535132098440367e-06
with;1.7921028912165708e-06
PyTorch;6.517538788233714e-06
is;2.4188710689225768e-06
using;1.7375485567954663e-06
the;1.6775879575278685e-06
Hugging;2.408459527057452e-06
Face;1.6261292684018153e-06
Transformer's;9.371367850741154e-06
library,;5.690285950309268e-06
which;1.94002118933988e-06
has;1.5244493669112104e-06
built;1.774154965708349e-06
in;1.5342507393749072e-06
methods;1.6971187261488616e-06
for;1.6170815904527125e-06
pre-training,;1.22260758318514e-05
inference,;4.137895251886601e-06
and;1.8021268172387717e-06
deploying;2.9762810458389183e-06
BERT.;5.087094938502243e-06
â€˜**;2.063619907834221e-05
from;1.9306058103569794e-06
transformers;2.234866083047519e-06
import;1.6198663478606972e-06
AutoTokenizer,;2.7389441644287095e-05
BertModel;4.6111216833393135e-06
import;1.585720184959981e-06
torch;1.8332163261324607e-06
tokenizer;3.447279592877239e-06
=;2.179064609598775e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5805594406693758
model;1.4557691495334375e-06
=;2.2473849069023785e-06
"BertModel.from_pretrained(""bert-base-uncased"")";0.41878163169256105
inputs;1.343361168343013e-06
=;1.8954784053278956e-06
"tokenizer(""Hello,";4.7338694570989916e-05
my;1.6607484921285948e-06
dog;1.5426234794566067e-06
is;1.4561298996210982e-06
"cute"",";6.990328821570829e-06
"return_tensors=""pt"")";0.0002987017298560335
outputs;1.356879022201324e-06
=;1.5299620501409704e-06
model(**inputs);2.0013313644381486e-05
last_hidden_states;1.7004920447825572e-05
=;1.8512679898153272e-06
outputs.last_hidden_state;8.656429242764627e-05
***;6.904564810911507e-06
