text;attention
The;9.853453184705035e-11
easiest;1.0307731638883783e-10
way;1.0220459111417865e-10
to;1.1166020549257502e-10
import;1.3205739084416572e-10
the;1.0725153596678308e-10
BERT;1.0454282860091454e-10
language;1.1070679708659047e-10
model;1.0786996696303683e-10
into;1.383883748231751e-10
python;1.4336039075746352e-10
for;1.0832186352404151e-10
use;9.772026564603748e-11
with;1.1910705385521112e-10
PyTorch;3.7318708099529866e-10
is;1.2112592827045588e-10
using;1.2592688815213933e-10
the;1.112832917465169e-10
Hugging;1.1418665273327494e-10
Face;1.0822163423619238e-10
Transformer's;0.04480233505298418
library,;0.014415226188732054
which;1.0437320873208554e-10
has;1.0137515875128868e-10
built;1.085433081545063e-10
in;1.0616713683968156e-10
methods;1.1491059075049171e-10
for;1.0277693179582871e-10
pre-training,;1.9560662607942055e-10
inference,;0.00048638392506627674
and;1.0624090010740815e-10
deploying;1.7169255256479864e-10
BERT.;0.9402956963762795
â€˜**;1.5781900173845556e-10
from;1.0694772300411949e-10
transformers;1.105124301258016e-10
import;1.3848684025994062e-10
AutoTokenizer,;7.322239996085815e-10
BertModel;1.5728317861892767e-10
import;1.3807430375467524e-10
torch;1.2238435422552502e-10
tokenizer;1.4590054585143128e-10
=;1.3628669644426434e-10
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.4935594944391772e-07
model;1.0606100200816226e-10
=;1.2224481373898014e-10
"BertModel.from_pretrained(""bert-base-uncased"")";1.830097218385053e-07
inputs;1.3234337863215777e-10
=;1.3077603989452455e-10
"tokenizer(""Hello,";1.0617121557985277e-09
my;1.1709011177131516e-10
dog;1.1055272718759455e-10
is;1.0678671722233502e-10
"cute"",";2.571892711511451e-10
"return_tensors=""pt"")";1.621052070165412e-08
outputs;1.126727727888257e-10
=;1.3700304286885429e-10
model(**inputs);7.265460322839721e-10
last_hidden_states;3.229128967980445e-10
=;1.2184513413717965e-10
outputs.last_hidden_state;5.691904908552784e-10
***;1.494796972811133e-10
