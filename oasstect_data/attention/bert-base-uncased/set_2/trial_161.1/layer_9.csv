text;attention
The;0.0005706247795074649
easiest;0.0009686429101562564
way;0.0005794753414368829
to;0.0006561611076186059
import;0.0013776847607772284
the;0.0008059358891727662
BERT;0.0009058467397549198
language;0.0008645884692750286
model;0.0009042870478752793
into;0.0006447873177299542
python;0.0027706525613148815
for;0.0006257156804670292
use;0.0007454297187153686
with;0.001025448301795199
PyTorch;0.002946899745899932
is;0.0006776684487835491
using;0.0006335509171676851
the;0.0006843693444167851
Hugging;0.0009877032894277345
Face;0.0008634197589016615
Transformer's;0.00593015359423262
library,;0.002513003069972787
which;0.0006793412520610617
has;0.0007290719548771578
built;0.0007721537238212262
in;0.0006023612346750085
methods;0.0009241598110818621
for;0.0006740103031336901
pre-training,;0.002400984736295459
inference,;0.002170752186042115
and;0.0005824766533760535
deploying;0.0013093320527689841
BERT.;0.017634998427876886
â€˜**;0.0043766632544074615
from;0.0007796634451159831
transformers;0.0017712259483766292
import;0.0006683390498864453
AutoTokenizer,;0.003210489954468955
BertModel;0.0009815078191928658
import;0.0006016971613438914
torch;0.00107898097281836
tokenizer;0.0009308795828412637
=;0.0006361306566148878
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6879601975288377
model;0.0006804172034947936
=;0.0006023258527879197
"BertModel.from_pretrained(""bert-base-uncased"")";0.18931210128176207
inputs;0.0006539675879584046
=;0.0005758006497982894
"tokenizer(""Hello,";0.005619607264839642
my;0.0008340074682294714
dog;0.0010972410782922604
is;0.0006688585353077717
"cute"",";0.0020427890800071373
"return_tensors=""pt"")";0.01873312368548616
outputs;0.0005645712941646488
=;0.0006171831538367777
model(**inputs);0.006399513418343839
last_hidden_states;0.0024738636638228905
=;0.0006985193737141318
outputs.last_hidden_state;0.004951431618859865
***;0.0033172112849801263
