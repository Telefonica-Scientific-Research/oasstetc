text;attention
The;0.0023232920188493073
easiest;0.0014970044246150611
way;0.0019041933795283278
to;0.002417213396799191
import;0.0018377123708911195
the;0.002931290213782519
BERT;0.002407768980475293
language;0.0018614927509723678
model;0.0018955325226503947
into;0.0020694233170391978
python;0.0019051751699066027
for;0.002006407673295846
use;0.0020837950246775966
with;0.002535658786519463
PyTorch;0.0032476440278630114
is;0.002187271511279273
using;0.0018077252706665083
the;0.003081358472995262
Hugging;0.0022082400197393835
Face;0.00172933969266316
Transformer's;0.0037693038148675618
library,;0.003408955227060508
which;0.001861304915981008
has;0.0020777823748089727
built;0.0018585324793539784
in;0.0018472239953844234
methods;0.002103707531965662
for;0.0022349375498561526
pre-training,;0.0052135723557616255
inference,;0.002568418610735142
and;0.0025157989696763944
deploying;0.0015787538967331654
BERT.;0.00558969473076502
â€˜**;0.00433969321308739
from;0.001891353223695747
transformers;0.0022802209922806
import;0.0017324221099403452
AutoTokenizer,;0.005582212374013277
BertModel;0.0022067522455043276
import;0.0017880839033062109
torch;0.0016539719538667958
tokenizer;0.0024653839722208145
=;0.002027665919449944
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.41246302608846636
model;0.0017652775612530128
=;0.0022133302131984666
"BertModel.from_pretrained(""bert-base-uncased"")";0.40299219420907945
inputs;0.0018067677450655613
=;0.0018465872686432429
"tokenizer(""Hello,";0.006507153986997652
my;0.0021169173690377082
dog;0.002038446029555226
is;0.0019630035033672793
"cute"",";0.0028575770607954533
"return_tensors=""pt"")";0.018099037319284406
outputs;0.0016268524881503368
=;0.0018783751012854777
model(**inputs);0.011528580303815802
last_hidden_states;0.004685504754940201
=;0.0018048524401257952
outputs.last_hidden_state;0.013540615953755998
***;0.0017326152176584525
