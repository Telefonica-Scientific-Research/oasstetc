text;attention
The;9.062266552190698e-16
easiest;8.492416275009856e-16
way;8.008525625486817e-16
to;8.797181958634977e-16
import;9.102261064143025e-16
the;1.0113107326965105e-15
BERT;1.051693962312523e-15
language;7.486589658067721e-16
model;8.583816908570663e-16
into;7.495642105440689e-16
python;1.0109940171170786e-15
for;7.937582650498397e-16
use;8.229793809367714e-16
with;1.012374851593183e-15
PyTorch;1.3991474415266412e-15
is;1.5058954952368542e-15
using;9.110955572275992e-16
the;1.046278886890095e-15
Hugging;9.32677772078318e-16
Face;8.17738937397825e-16
Transformer's;2.2905536403308497e-15
library,;2.215116330284927e-15
which;8.743610576789641e-16
has;8.11460049230783e-16
built;7.283019269310854e-16
in;7.996054198047815e-16
methods;9.23397096722516e-16
for;9.475686141916528e-16
pre-training,;3.916270661928976e-15
inference,;1.3895896904417379e-15
and;8.4304894192885125e-16
deploying;8.947136413973025e-16
BERT.;0.9999999999787459
â€˜**;1.8619539809447843e-15
from;8.289614461543043e-16
transformers;1.4136356759597706e-15
import;1.0894143721739704e-15
AutoTokenizer,;2.2128474802226892e-15
BertModel;9.905528647397954e-16
import;1.0291388378922357e-15
torch;7.658048216316907e-16
tokenizer;8.129404422583677e-16
=;8.5116006496876515e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.7094907864547512e-13
model;7.321974102300784e-16
=;8.314631831540106e-16
"BertModel.from_pretrained(""bert-base-uncased"")";1.405387109948065e-13
inputs;7.2300287899005835e-16
=;8.247630283671423e-16
"tokenizer(""Hello,";3.2439647970471456e-15
my;7.689894948492488e-16
dog;7.719899167057943e-16
is;7.617077916398274e-16
"cute"",";1.3446802929338513e-15
"return_tensors=""pt"")";1.1111794506105843e-14
outputs;7.75605547287047e-16
=;8.105051981156703e-16
model(**inputs);3.931328852648103e-15
last_hidden_states;1.7847864045416866e-15
=;7.721591984417306e-16
outputs.last_hidden_state;2.0865061503648936e-11
***;8.623924295490156e-16
