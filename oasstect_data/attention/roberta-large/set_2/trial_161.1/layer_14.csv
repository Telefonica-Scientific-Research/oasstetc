text;attention
The;1.0220621253530454e-10
easiest;9.070895887839332e-11
way;8.343800372770042e-11
to;9.951636147796995e-11
import;9.263197359400754e-11
the;1.182244404305487e-10
BERT;1.203231142121513e-10
language;1.1256918631969724e-10
model;1.261677374442686e-10
into;8.885408659712088e-11
python;1.1420909218288272e-10
for;7.63663445426988e-11
use;6.683405962929634e-11
with;8.236805356006214e-11
PyTorch;9.556953355821301e-11
is;8.92676665632481e-11
using;8.729970549889624e-11
the;9.95356352339762e-11
Hugging;8.927437098212194e-11
Face;7.531427543401941e-11
Transformer's;1.202513833709507e-10
library,;1.7255426675326202e-09
which;7.683486226422339e-11
has;7.458245519755174e-11
built;6.931129730120444e-11
in;7.146843980465052e-11
methods;8.381196301477847e-11
for;8.436573385874024e-11
pre-training,;1.8517124098202394e-09
inference,;1.1561295839235309e-09
and;8.067160870124301e-11
deploying;8.973922364224804e-11
BERT.;0.07313580465646816
â€˜**;3.314955869130995e-10
from;1.0568747604506695e-10
transformers;1.1220877882005431e-10
import;1.0071810838209185e-10
AutoTokenizer,;1.4352886276931162e-10
BertModel;1.1357555259411309e-10
import;1.6663037379944512e-10
torch;2.357055234061967e-10
tokenizer;1.6930984474480265e-10
=;9.556786524311018e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1216600410728979
model;1.4666784097187677e-10
=;8.809121766106562e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.6725120914971479
inputs;1.197000819585308e-10
=;1.0318469774921221e-10
"tokenizer(""Hello,";4.071387196972038e-10
my;7.229867023365916e-11
dog;9.149178434886984e-11
is;7.002974309187448e-11
"cute"",";9.985416489210521e-11
"return_tensors=""pt"")";9.49078660318497e-10
outputs;9.806351066429555e-11
=;1.0630000248553597e-10
model(**inputs);6.638834049630246e-10
last_hidden_states;1.8518900282996711e-10
=;9.534393916263601e-11
outputs.last_hidden_state;0.13269205036731108
***;1.4033125397737845e-10
