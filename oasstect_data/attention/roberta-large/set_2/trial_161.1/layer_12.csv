text;attention
The;7.256450465941787e-11
easiest;6.999328509386101e-11
way;7.065557758392045e-11
to;7.474005250056532e-11
import;8.914964959102556e-11
the;8.616557340881928e-11
BERT;9.986545237465565e-11
language;8.670431351988737e-11
model;1.3309779480853631e-10
into;6.28091017465099e-11
python;8.757919771780903e-11
for;5.7379957364264364e-11
use;5.197640867393308e-11
with;6.976622794948009e-11
PyTorch;1.1149931939978003e-10
is;6.604425873012821e-11
using;7.828595984200999e-11
the;8.493737528013186e-11
Hugging;8.648352701849774e-11
Face;7.306913576058209e-11
Transformer's;1.646063497525086e-10
library,;5.863414411732909e-10
which;7.094856618400158e-11
has;6.481672925135435e-11
built;6.869730131035672e-11
in;5.920294007867646e-11
methods;7.842705294223051e-11
for;6.57831358356341e-11
pre-training,;1.0855624234228911e-09
inference,;4.537488446372044e-10
and;6.639433707792116e-11
deploying;7.302463203000842e-11
BERT.;0.006083548185439739
â€˜**;1.4455467929531333e-10
from;1.0156066195257875e-10
transformers;1.2055996952517524e-10
import;1.0338608457621872e-10
AutoTokenizer,;2.515999457606496e-10
BertModel;1.4783908363867204e-10
import;1.1228602951342269e-10
torch;1.1329836284016599e-10
tokenizer;1.384283453700243e-10
=;8.961475285619045e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.024539700684515386
model;1.3898694635761707e-10
=;8.524550474607958e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.9471905236740581
inputs;9.688120875750633e-11
=;8.497628568541114e-11
"tokenizer(""Hello,";7.420902061578173e-10
my;6.772614764815241e-11
dog;7.340911187584621e-11
is;5.593415719139464e-11
"cute"",";1.0182003037752515e-10
"return_tensors=""pt"")";2.3133120067090613e-09
outputs;8.094666704192444e-11
=;8.17570457078838e-11
model(**inputs);9.534949110563085e-10
last_hidden_states;3.2797997683693093e-10
=;9.311137275688512e-11
outputs.last_hidden_state;0.022186216308492036
***;7.6374676518191e-11
