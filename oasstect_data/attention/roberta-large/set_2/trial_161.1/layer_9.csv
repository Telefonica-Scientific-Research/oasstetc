text;attention
The;9.319340860449547e-10
easiest;6.422979558349067e-10
way;6.077483977530808e-10
to;8.205482662863915e-10
import;8.296317690194491e-10
the;7.92158713199442e-10
BERT;1.7051000448337207e-09
language;7.044109318236721e-10
model;1.5517391372587865e-09
into;6.661278687926587e-10
python;1.0451821882223917e-09
for;6.129386388122095e-10
use;5.719500425548961e-10
with;6.380063710501645e-10
PyTorch;1.7317889317397985e-09
is;6.315488762884541e-10
using;6.210944936041192e-10
the;8.575169039684761e-10
Hugging;9.523032792886404e-10
Face;7.329959146580628e-10
Transformer's;1.8467688323425397e-09
library,;6.6271901391030764e-09
which;6.869097152181665e-10
has;6.543204096011639e-10
built;6.447912772115093e-10
in;6.224857661992288e-10
methods;9.899754634420246e-10
for;7.073336312588282e-10
pre-training,;1.3989584471256362e-08
inference,;1.9259518916756374e-09
and;7.300075131414054e-10
deploying;7.528897678034332e-10
BERT.;0.03095775183014231
â€˜**;2.432535301930368e-09
from;1.0933906512760827e-09
transformers;1.5282413901551291e-09
import;1.2727773118656385e-09
AutoTokenizer,;5.291783054583596e-09
BertModel;2.1666163997561833e-09
import;1.374984304877121e-09
torch;1.1720034370821852e-09
tokenizer;1.6173685572834272e-09
=;9.805201095975852e-10
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.0002891090705648383
model;1.1501788644893986e-09
=;8.164328479438748e-10
"BertModel.from_pretrained(""bert-base-uncased"")";0.6796684386234384
inputs;1.083274298989991e-09
=;8.340930756972673e-10
"tokenizer(""Hello,";5.987906997164764e-09
my;6.421415849716399e-10
dog;7.283152137500561e-10
is;5.965690504914285e-10
"cute"",";1.2737554914303342e-09
"return_tensors=""pt"")";2.481807458472484e-08
outputs;1.0150963220534224e-09
=;7.424108485429274e-10
model(**inputs);9.946109688536639e-09
last_hidden_states;4.8399216456440475e-09
=;7.642518293387009e-10
outputs.last_hidden_state;0.28908457785311603
***;6.287538947892491e-10
