text;attention
The;2.352436589588664e-13
easiest;2.1342423435559416e-13
way;2.0324813535320663e-13
to;3.1137317693237563e-13
import;2.316906251986179e-13
the;2.261359369683467e-13
BERT;3.1473452571957135e-13
language;2.2737302538398028e-13
model;2.8080083664524076e-13
into;1.8749382005269014e-13
python;2.2858197260072517e-13
for;2.0046479009135766e-13
use;2.2150751962604858e-13
with;2.0835898371601842e-13
PyTorch;3.748746651290509e-13
is;1.8579407351045039e-13
using;2.0993984341202487e-13
the;2.3398616628063005e-13
Hugging;2.825264584619623e-13
Face;2.2094421020433919e-13
Transformer's;4.1748229783212134e-13
library,;9.48079410333073e-13
which;2.179548266227198e-13
has;1.979216442957776e-13
built;2.1391226947941413e-13
in;1.9984044463478827e-13
methods;2.1894433266738499e-13
for;2.2739069173890386e-13
pre-training,;1.2529250787806546e-12
inference,;8.118885704797738e-13
and;2.1686118756059015e-13
deploying;2.752212056298981e-13
BERT.;0.03380232993517293
â€˜**;4.262099742711447e-13
from;2.1529543888502555e-13
transformers;2.897285896703603e-13
import;2.354622943721572e-13
AutoTokenizer,;4.329615514098039e-13
BertModel;2.669868469047698e-13
import;2.3685162481870323e-13
torch;4.1104832303808384e-13
tokenizer;2.68622329099903e-13
=;1.861411717389312e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5073638999694645
model;2.077949281455681e-13
=;1.843020783068273e-13
"BertModel.from_pretrained(""bert-base-uncased"")";0.40679959469461335
inputs;2.0942748946801748e-13
=;1.8318261578765368e-13
"tokenizer(""Hello,";4.620977987571486e-13
my;2.0108932720796825e-13
dog;2.2873924714517914e-13
is;2.1851753462547891e-13
"cute"",";2.574831975704121e-13
"return_tensors=""pt"")";1.4972744258725379e-12
outputs;2.092400248240594e-13
=;1.8927175812102377e-13
model(**inputs);6.332301745891971e-13
last_hidden_states;5.376406805120391e-13
=;1.8535138774564213e-13
outputs.last_hidden_state;0.05203417538212449
***;2.538795442719736e-13
