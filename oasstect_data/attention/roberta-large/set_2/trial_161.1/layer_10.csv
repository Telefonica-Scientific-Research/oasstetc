text;attention
The;8.756949693575092e-11
easiest;6.474897034620314e-11
way;5.942824584481972e-11
to;7.740641444801348e-11
import;1.0421933440946636e-10
the;1.1007544892293592e-10
BERT;1.5288811129463548e-10
language;9.398462720159618e-11
model;2.0059843057514e-10
into;6.617122614269442e-11
python;1.2348664573660291e-10
for;6.309775215751462e-11
use;5.897529523604543e-11
with;7.617186679144477e-11
PyTorch;1.416707778153137e-10
is;6.563873801752592e-11
using;6.53030257821219e-11
the;9.239370502559008e-11
Hugging;9.396069604432823e-11
Face;7.587559729371268e-11
Transformer's;2.0301158067722627e-10
library,;3.5234117008348707e-10
which;6.611117471542984e-11
has;6.969767545399805e-11
built;6.034006582183867e-11
in;5.499502466614048e-11
methods;7.00531571781636e-11
for;8.005318772934811e-11
pre-training,;1.125892028082209e-09
inference,;2.4331750087990063e-10
and;6.695689252881678e-11
deploying;6.487971379224617e-11
BERT.;0.004850718849012418
â€˜**;1.505743726589396e-10
from;7.402695980239092e-11
transformers;1.7594872115686578e-10
import;1.3567149340663251e-10
AutoTokenizer,;4.0888411713829514e-10
BertModel;2.146597580873205e-10
import;1.2227268431027447e-10
torch;9.812428833143849e-11
tokenizer;1.3302031395061262e-10
=;9.515337588914796e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.00011612339822373674
model;9.539131221546918e-11
=;1.0186139898437285e-10
"BertModel.from_pretrained(""bert-base-uncased"")";0.9409195885565699
inputs;9.66154042313997e-11
=;8.10151046103332e-11
"tokenizer(""Hello,";7.113775343578057e-10
my;6.660597191671491e-11
dog;8.815193309308491e-11
is;6.528809054475238e-11
"cute"",";1.225620654339325e-10
"return_tensors=""pt"")";1.4433610779281048e-09
outputs;8.118619827644405e-11
=;8.128385141411523e-11
model(**inputs);1.2882113226062796e-09
last_hidden_states;4.718382631880446e-10
=;8.008552622671908e-11
outputs.last_hidden_state;0.05411355831462315
***;6.708611805063133e-11
