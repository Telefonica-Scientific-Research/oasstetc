text;attention
The;1.1177323251487127e-07
easiest;2.0838114693222277e-07
way;1.633040786799446e-07
to;9.069103122106516e-08
import;1.5720841115320512e-07
the;8.217536141523819e-08
BERT;2.756152191540601e-07
language;1.2384852825260343e-07
model;1.0541023133197849e-07
into;1.1711364766937899e-07
python;2.4538432074978985e-07
for;1.0443744916149731e-07
use;1.1758561351880473e-07
with;1.0871189009260863e-07
PyTorch;9.877519445735426e-07
is;1.04364306640792e-07
using;1.353746696954236e-07
the;8.146051549576602e-08
Hugging;4.1179188338892557e-07
Face;1.1461441617690637e-07
Transformer's;1.3165391923242242e-06
library,;2.475540804058252e-07
which;1.4272922753280232e-07
has;1.2236053208955612e-07
built;1.0710792449941658e-07
in;9.845597791594972e-08
methods;1.326049201801244e-07
for;1.0482750947947834e-07
pre-training,;1.6082298003721325e-06
inference,;3.799588736192109e-07
and;9.387159467287783e-08
deploying;1.567242612485951e-07
BERT.;5.95234067449767e-07
â€˜**;6.44150287962451e-07
from;1.240362900164213e-07
transformers;3.2501846045596615e-07
import;1.3903307211943876e-07
AutoTokenizer,;2.0098825837317323e-06
BertModel;3.5096665171252163e-07
import;1.4119833454716588e-07
torch;1.6121753979089298e-07
tokenizer;3.831670627159982e-07
=;1.4455861218802735e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7339374973204151
model;9.774031296570865e-08
=;1.2882526987153765e-07
"BertModel.from_pretrained(""bert-base-uncased"")";0.2659346059496945
inputs;2.0912230041743701e-07
=;1.33896305297252e-07
"tokenizer(""Hello,";8.097711735326305e-06
my;1.0162916179169735e-07
dog;1.1358769684107457e-07
is;9.436989351611443e-08
"cute"",";4.65244218502716e-07
"return_tensors=""pt"")";6.126994605126519e-05
outputs;2.1065046503866027e-07
=;1.2828607479357457e-07
model(**inputs);7.870841218297514e-06
last_hidden_states;4.167241043246999e-06
=;1.2251439246051688e-07
outputs.last_hidden_state;3.1189432263438994e-05
***;1.192667324749022e-07
