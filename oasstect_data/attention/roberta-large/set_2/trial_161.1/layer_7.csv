text;attention
The;2.617440685918631e-12
easiest;1.710078920802449e-12
way;1.4920260967556943e-12
to;1.7527924681725099e-12
import;1.7692787120058129e-12
the;2.1332324083758543e-12
BERT;2.336554675195125e-12
language;1.9238101840059416e-12
model;2.0554811227086773e-12
into;1.8634684897986908e-12
python;2.4215978823265177e-12
for;1.8442538157064902e-12
use;1.884560291735891e-12
with;2.071465035474196e-12
PyTorch;3.718718338562174e-12
is;2.2859298218662683e-12
using;2.0499813893343705e-12
the;2.5301274614413162e-12
Hugging;2.6669827037606155e-12
Face;2.0209481587360232e-12
Transformer's;3.912829929388506e-12
library,;6.623729727161996e-12
which;1.7864656556552193e-12
has;2.052926410647544e-12
built;1.8948952328039216e-12
in;1.684183257938373e-12
methods;1.8273242330580974e-12
for;2.117242295913768e-12
pre-training,;7.474016418733905e-11
inference,;3.539543804210468e-12
and;1.865693299219291e-12
deploying;1.7761300537816392e-12
BERT.;0.403523517437401
â€˜**;8.62441188151572e-12
from;2.823558638182176e-12
transformers;2.7530818845532298e-12
import;2.440157391638621e-12
AutoTokenizer,;6.323118746683892e-12
BertModel;2.69091826052015e-12
import;2.2530644372979783e-12
torch;2.1208225131709393e-12
tokenizer;2.651141667272381e-12
=;2.4343922393062266e-12
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.5357650843945122e-09
model;1.907655903533992e-12
=;2.4264883828792923e-12
"BertModel.from_pretrained(""bert-base-uncased"")";4.650635458085058e-07
inputs;1.992013229834509e-12
=;2.6022364410818625e-12
"tokenizer(""Hello,";1.2179156349098691e-11
my;2.0349240864573493e-12
dog;1.931494775193371e-12
is;2.024329587567906e-12
"cute"",";4.570243716443692e-12
"return_tensors=""pt"")";6.709346002666583e-11
outputs;2.0906108978492127e-12
=;2.4905537801324823e-12
model(**inputs);1.2144612121929133e-11
last_hidden_states;7.487167348408527e-12
=;2.5183032278386602e-12
outputs.last_hidden_state;0.5964760146533502
***;2.3600625271593948e-12
