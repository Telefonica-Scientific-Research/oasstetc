text;attention
The;0.000569965426228669
easiest;0.0005187574769919027
way;0.0005180043405699283
to;0.0006404121560462647
import;0.0007142839780448478
the;0.0007006337459412965
BERT;0.0007811235914017654
language;0.0006590237834805531
model;0.0005721654113492052
into;0.0006241655797183737
python;0.0006962394157399801
for;0.0005304634999735531
use;0.0006060188481209855
with;0.0007891344067179046
PyTorch;0.0009602529046354225
is;0.0007493298856955414
using;0.0007662370640163869
the;0.0007261725804972437
Hugging;0.0006859336238588719
Face;0.0005608303724360424
Transformer's;0.0021377265517340547
library,;0.0021979072903131015
which;0.0006321468888595322
has;0.000639370005785445
built;0.0006770829878718405
in;0.000580567247325725
methods;0.0005907794334174583
for;0.0006865213496266198
pre-training,;0.00277170965418302
inference,;0.0013161659456433516
and;0.0008478723502213424
deploying;0.0004222990598540912
BERT.;0.014530923415287321
â€˜**;0.00139670328102147
from;0.0007635987106435889
transformers;0.0010616973716908707
import;0.0008750947609290304
AutoTokenizer,;0.0024960468406483903
BertModel;0.0008734256270412965
import;0.0007966948017889937
torch;0.00043263622480226266
tokenizer;0.0006329258425349207
=;0.0007943438860966489
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.49685440412710197
model;0.0006173884713189032
=;0.0006268701028050062
"BertModel.from_pretrained(""bert-base-uncased"")";0.39384136027733524
inputs;0.00041646532452050127
=;0.0006827164827955324
"tokenizer(""Hello,";0.004661928169179732
my;0.0005129579569885202
dog;0.000669590449483878
is;0.0005951779741729765
"cute"",";0.0014239723664461882
"return_tensors=""pt"")";0.023130707044639664
outputs;0.00045865942975765675
=;0.0008328118549402172
model(**inputs);0.004996672138376215
last_hidden_states;0.0028850942736628584
=;0.000757043000017911
outputs.last_hidden_state;0.014085008000919564
***;0.00042778493675220126
