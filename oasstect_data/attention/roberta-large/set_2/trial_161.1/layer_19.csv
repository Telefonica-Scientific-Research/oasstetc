text;attention
The;3.584228412171112e-11
easiest;3.814822675313335e-11
way;3.59333061745068e-11
to;4.44595822037109e-11
import;4.1209929782379616e-11
the;4.6258685929491837e-11
BERT;4.638278970925377e-11
language;4.3123043629813083e-11
model;5.4211055654436783e-11
into;3.2186766476333586e-11
python;4.3444891725126125e-11
for;3.827911878427344e-11
use;3.569950746219097e-11
with;3.864218482673723e-11
PyTorch;4.712471040063901e-11
is;3.6217809179839005e-11
using;3.8957421363386205e-11
the;4.094223817616898e-11
Hugging;4.7575950360672984e-11
Face;3.61292556077211e-11
Transformer's;6.77620219798622e-11
library,;8.827355577358385e-10
which;4.522093782192431e-11
has;3.8785530469023906e-11
built;3.9256387959692055e-11
in;3.504211257197031e-11
methods;4.296566082952727e-11
for;4.880563931897873e-11
pre-training,;1.444893667547063e-09
inference,;7.950930799093619e-10
and;3.7983271505008933e-11
deploying;4.2897420865012453e-11
BERT.;0.011111178897743988
â€˜**;8.601032766508878e-11
from;4.0073336677829176e-11
transformers;5.594684671180779e-11
import;3.973454015695374e-11
AutoTokenizer,;9.703893360991753e-11
BertModel;5.4536664901495107e-11
import;4.717113548237258e-11
torch;1.0594407363489897e-10
tokenizer;4.3952418907343126e-11
=;3.6072412116459005e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.4360659423294173
model;4.5641367280298045e-11
=;3.3944220018807025e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.5051232134589708
inputs;5.2431835493672395e-11
=;3.5062358701005775e-11
"tokenizer(""Hello,";2.1364549264590332e-10
my;3.760962574470394e-11
dog;4.244601285498008e-11
is;3.7396217616004364e-11
"cute"",";5.7627205101228755e-11
"return_tensors=""pt"")";8.217225157800135e-10
outputs;5.0503121681472895e-11
=;3.693193453450151e-11
model(**inputs);3.3421214621255953e-10
last_hidden_states;1.9532298654589018e-10
=;4.2594079503680674e-11
outputs.last_hidden_state;0.04769965828126985
***;4.681623674872491e-11
