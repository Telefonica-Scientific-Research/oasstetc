text;attention
The;3.755905071164146e-13
easiest;3.640091231784332e-13
way;3.635939075901754e-13
to;4.674296458520822e-13
import;3.9044688193533386e-13
the;3.716432433994105e-13
BERT;5.524168883641371e-13
language;4.3288192979442286e-13
model;4.77001986360286e-13
into;3.426683847913642e-13
python;4.668399226574529e-13
for;3.758855036612977e-13
use;3.618795116432642e-13
with;3.5997966904183785e-13
PyTorch;6.831102610354092e-13
is;3.711557228958276e-13
using;3.54676440557942e-13
the;3.843508935408274e-13
Hugging;5.60928007627287e-13
Face;4.1278404192945736e-13
Transformer's;9.335154109325237e-13
library,;2.1634762781871918e-12
which;4.884207815675971e-13
has;3.5934631719592854e-13
built;3.7250436644405287e-13
in;4.2386952102482586e-13
methods;3.894219262839783e-13
for;5.02997055405032e-13
pre-training,;3.3768670274881765e-12
inference,;2.0319435301428197e-12
and;3.9366940141689863e-13
deploying;4.0245445768367794e-13
BERT.;0.01450416000943387
â€˜**;8.333319780835289e-13
from;3.7139138129848174e-13
transformers;9.378546514558732e-13
import;3.5543913698679037e-13
AutoTokenizer,;9.949385496983026e-13
BertModel;4.666008904481968e-13
import;3.9831395898224825e-13
torch;3.648101153825049e-13
tokenizer;4.940296471771271e-13
=;3.395690124869797e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5372773213147141
model;4.788884002143871e-13
=;3.5186521217163197e-13
"BertModel.from_pretrained(""bert-base-uncased"")";0.41181603783376947
inputs;4.727838237516218e-13
=;3.5445183720891136e-13
"tokenizer(""Hello,";1.1550214606646903e-12
my;3.450764397971279e-13
dog;3.8528369085214363e-13
is;3.596218352418182e-13
"cute"",";4.267496862021253e-13
"return_tensors=""pt"")";3.90289397899197e-12
outputs;4.569638966678714e-13
=;3.7173590428155783e-13
model(**inputs);1.5243090167139475e-12
last_hidden_states;1.2441996078588104e-12
=;3.4171003035794446e-13
outputs.last_hidden_state;0.036402480803612865
***;4.340284119474325e-13
