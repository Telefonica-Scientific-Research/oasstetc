text;attention
The;5.2234190630413446e-11
easiest;2.8126909049353544e-11
way;3.2410377046986475e-11
to;7.340773488165816e-11
import;3.037731524089586e-11
the;3.709765215796021e-11
BERT;4.4339388186920524e-11
language;2.7320016583729893e-11
model;3.752131840918529e-11
into;3.48137363878664e-11
python;3.4198956485384065e-11
for;5.26177181023106e-11
use;3.17848676381172e-11
with;3.695165214443295e-11
PyTorch;6.499782376470657e-11
is;3.18142899705569e-11
using;3.024107489040915e-11
the;3.7006578980350725e-11
Hugging;4.502517379017706e-11
Face;3.0939652561163436e-11
Transformer's;1.1960715085612662e-10
library,;5.258343270168492e-11
which;1.1138234220791575e-10
has;3.154561685852603e-11
built;3.4312380243817786e-11
in;3.23177278356315e-11
methods;3.3978158615608134e-11
for;4.038270304945205e-11
pre-training,;1.443728236364759e-10
inference,;4.4595120598993e-11
and;3.321479212545632e-11
deploying;3.0931403869327377e-11
BERT.;0.0007286715737338562
â€˜**;8.975387367823341e-11
from;3.484769054770159e-11
transformers;5.3174064841214245e-11
import;4.20289696421897e-11
AutoTokenizer,;2.2623182957929166e-10
BertModel;5.6319232786589757e-11
import;4.612900186509592e-11
torch;2.7757949020251684e-11
tokenizer;5.594717620179533e-11
=;2.91345590688725e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.8748870744068469
model;3.33090412069737e-11
=;2.964824244597257e-11
"BertModel.from_pretrained(""bert-base-uncased"")";0.11960465267503123
inputs;2.8781160258396157e-11
=;3.1247161689758103e-11
"tokenizer(""Hello,";3.7361113812955596e-10
my;3.0340590740489306e-11
dog;3.251160221144328e-11
is;3.624781701282832e-11
"cute"",";4.745566367440454e-11
"return_tensors=""pt"")";4.992737462420489e-10
outputs;2.8088859106398275e-11
=;2.916535229562203e-11
model(**inputs);2.770966805812485e-10
last_hidden_states;1.963527955763772e-10
=;2.8217523367760677e-11
outputs.last_hidden_state;0.0047795974488845485
***;3.0381399389321973e-11
