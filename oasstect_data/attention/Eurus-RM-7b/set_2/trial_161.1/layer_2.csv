text;attention
The;0.014933641282384655
easiest;0.014935078663756386
way;0.014726644958177526
to;0.014351536790960608
import;0.016395127675618352
the;0.013666390119249373
BERT;0.020368977935998716
language;0.016233695026148326
model;0.015465894899396712
into;0.014366879641741095
python;0.016114867172187
for;0.014178311429976569
use;0.013944939304602182
with;0.013321217880257833
PyTorch;0.019945150685040575
is;0.014448180189960478
using;0.014068836537303165
the;0.013057313676988053
Hugging;0.015987751322339417
Face;0.013824173621238606
Transformer's;0.017747261686287933
library,;0.015700531981251802
which;0.013292037397159742
has;0.012973474577830066
built;0.01346402327432431
in;0.01363248725335386
methods;0.013692928202026645
for;0.013492006233462216
pre-training,;0.019081705448079526
inference,;0.014936311697371394
and;0.012563053138341142
deploying;0.01356746229639717
BERT.;0.015649668197903786
â€˜**;0.017983074352715324
from;0.014854274262713024
transformers;0.01469936044533742
import;0.014917131672327503
AutoTokenizer,;0.019564493025114805
BertModel;0.015039750832841616
import;0.014757731257986892
torch;0.013862286339732154
tokenizer;0.014726555973859612
=;0.014117342233965427
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.07030422423892196
model;0.013437442853658462
=;0.013889392956343553
"BertModel.from_pretrained(""bert-base-uncased"")";0.034993608440787506
inputs;0.013420118415269239
=;0.013891400070854655
"tokenizer(""Hello,";0.021942727341403342
my;0.012745074843091864
dog;0.013153065750447513
is;0.012605130450259124
"cute"",";0.013754756270079275
"return_tensors=""pt"")";0.01866467073706233
outputs;0.012884860079101869
=;0.013103192844266488
model(**inputs);0.016791626955393844
last_hidden_states;0.016752804889987236
=;0.012545719152079022
outputs.last_hidden_state;0.014532256417038019
***;0.0119383667002458
