text;attention
The;0.00918488552974637
easiest;0.007929226705152118
way;0.007672606166290148
to;0.006604609991698214
import;0.009673505912058605
the;0.007124341394871263
BERT;0.012554755134395454
language;0.006588236026895323
model;0.008321805483108518
into;0.008127706959524129
python;0.009245462615402949
for;0.007337194556628938
use;0.006665074961235276
with;0.0069420851612615065
PyTorch;0.01103051391831319
is;0.009317102915628522
using;0.008091348025425101
the;0.0075210712209182705
Hugging;0.00949330332191707
Face;0.008443616648341761
Transformer's;0.015293578941468563
library,;0.009548274277022597
which;0.007957548035143048
has;0.0072510591673012855
built;0.0064676248793413245
in;0.0068849451561014754
methods;0.0074148145504444075
for;0.007390848450124002
pre-training,;0.012734892769646351
inference,;0.008252526488294838
and;0.0063961566676458284
deploying;0.007056250575057282
BERT.;0.01622225397227989
â€˜**;0.017601273879085787
from;0.011859532001061428
transformers;0.009157006414788466
import;0.008328572930350617
AutoTokenizer,;0.011660383907354849
BertModel;0.00879665314464998
import;0.009252389159346865
torch;0.007628924247731196
tokenizer;0.010025437087357994
=;0.010092027949735786
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2312334385206758
model;0.006550824608800063
=;0.006798784744670334
"BertModel.from_pretrained(""bert-base-uncased"")";0.22187603243294954
inputs;0.0077507188027020595
=;0.007965162877872072
"tokenizer(""Hello,";0.018269684861458666
my;0.0062849255934139025
dog;0.006473876669061103
is;0.006217255771364605
"cute"",";0.008278587614352247
"return_tensors=""pt"")";0.018960172765266986
outputs;0.006474674028493235
=;0.006707246968749566
model(**inputs);0.013144494187475299
last_hidden_states;0.010776872618908789
=;0.006412530669703782
outputs.last_hidden_state;0.012537472764239097
***;0.006145816199696089
