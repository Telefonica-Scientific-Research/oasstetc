text;attention
The;0.0008520149590154183
easiest;0.000477083818968564
way;0.00042937581147011453
to;0.0004196747151372711
import;0.0006854564180613422
the;0.0006675696328105101
BERT;0.0022444981394829826
language;0.0004987498019223162
model;0.0006586154276206265
into;0.0005765968343576819
python;0.0008071038211609871
for;0.0005231513027878044
use;0.00042556650796306755
with;0.00045783488484298166
PyTorch;0.0013494561770836168
is;0.0005105186720323982
using;0.0006717874576626447
the;0.0005258758120396317
Hugging;0.001257322842290465
Face;0.0006065707468785617
Transformer's;0.021304789482677708
library,;0.0007517961240677202
which;0.0004177047661046263
has;0.00042749717388686795
built;0.00039378575542127253
in;0.0004626733168456162
methods;0.0004367834246962372
for;0.0004932947464835641
pre-training,;0.0016467804043933713
inference,;0.0006106871222305716
and;0.0003663766395668637
deploying;0.0005520329983107611
BERT.;0.01042450261239248
â€˜**;0.0027472060209686863
from;0.000737125689494634
transformers;0.0008425045303462957
import;0.0009133539512327498
AutoTokenizer,;0.004216194877556292
BertModel;0.001722459062231543
import;0.0007370648844196697
torch;0.000900500797283964
tokenizer;0.0016015040218042018
=;0.0006728267009031395
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6734046958491947
model;0.000599388540703279
=;0.00043568968870876944
"BertModel.from_pretrained(""bert-base-uncased"")";0.2429535831000759
inputs;0.0006086364691542648
=;0.00043130452921823545
"tokenizer(""Hello,";0.0019526314479424362
my;0.0003641059570088093
dog;0.00037599154772926055
is;0.00033959593234827214
"cute"",";0.0006078829794076245
"return_tensors=""pt"")";0.003267852890583744
outputs;0.000443593171567787
=;0.0003656145369774029
model(**inputs);0.0019047888084979888
last_hidden_states;0.0013905854797338054
=;0.0003609661860773077
outputs.last_hidden_state;0.001822746094648143
***;0.00034607790351442633
