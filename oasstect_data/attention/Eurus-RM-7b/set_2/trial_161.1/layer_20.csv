text;attention
The;0.005371779340367399
easiest;0.005035035619325686
way;0.00454304053192568
to;0.004405861396114319
import;0.0075950720294272155
the;0.00477726232526557
BERT;0.03443924417953788
language;0.005032853081374762
model;0.007288433290419953
into;0.0059185745619216865
python;0.006816973037551819
for;0.0052564290352646245
use;0.004837135074907915
with;0.0047501426942670255
PyTorch;0.01140910077557597
is;0.00510684988618169
using;0.005621204156958794
the;0.0048937874589141325
Hugging;0.010210261958779632
Face;0.006644393658346941
Transformer's;0.01991221186708774
library,;0.006168920846520305
which;0.004378850848215539
has;0.004686997049992168
built;0.004345269342748818
in;0.0048654212952559785
methods;0.004928561520444151
for;0.005027491971019691
pre-training,;0.012083184657595312
inference,;0.006489038442848366
and;0.0042047268152778546
deploying;0.005171270985583727
BERT.;0.008602186925092842
â€˜**;0.0061984851222005605
from;0.006844775769875574
transformers;0.007479816033206317
import;0.005649514552971115
AutoTokenizer,;0.02242013657199449
BertModel;0.008139790794472516
import;0.005358374502524493
torch;0.006615913297474569
tokenizer;0.01069447855507086
=;0.006168679550519242
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5204438825350566
model;0.005170795906662862
=;0.004811536105579621
"BertModel.from_pretrained(""bert-base-uncased"")";0.04027440072800714
inputs;0.0069215043626378436
=;0.005152083219274332
"tokenizer(""Hello,";0.0134355177682455
my;0.0042913679744758645
dog;0.004349246396078468
is;0.004142593573895921
"cute"",";0.0050528120882065205
"return_tensors=""pt"")";0.01531565440951321
outputs;0.005366310039004516
=;0.004386373249580268
model(**inputs);0.009353991525177833
last_hidden_states;0.008894630982935773
=;0.004515727178047146
outputs.last_hidden_state;0.0075692193640932
***;0.004164821183106516
