text;attention
The;0.012288059884604761
easiest;0.01341906928269725
way;0.011769637627662642
to;0.010369049601550126
import;0.018019138988229527
the;0.010860878027902896
BERT;0.02494893515260661
language;0.011517531411171377
model;0.013386454759985556
into;0.013019569289485174
python;0.013106002830302879
for;0.011348444367191327
use;0.009980150929434104
with;0.009784801680716
PyTorch;0.014853474115475728
is;0.012598531791108558
using;0.011049918126163055
the;0.009724224393774522
Hugging;0.014714684733947724
Face;0.020783825778324366
Transformer's;0.020981853662810152
library,;0.014385954149907986
which;0.010237504516792064
has;0.010113128870096794
built;0.009678974948792657
in;0.011394612711703618
methods;0.011135481038090358
for;0.010920953710237337
pre-training,;0.017330544176473346
inference,;0.012441054831097805
and;0.009364691405341503
deploying;0.011161561964372254
BERT.;0.013415713328612867
â€˜**;0.013447599226396295
from;0.014083836128840092
transformers;0.016193756319521288
import;0.01244944684757222
AutoTokenizer,;0.02078144182030041
BertModel;0.01516984299430361
import;0.01212207652528975
torch;0.011146915277456496
tokenizer;0.014173095731673893
=;0.014614271194272277
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.14044285191608388
model;0.011931028061880846
=;0.01176639036539629
"BertModel.from_pretrained(""bert-base-uncased"")";0.07047193148197957
inputs;0.01220803204244931
=;0.010624403491018218
"tokenizer(""Hello,";0.02453801949441173
my;0.00954251701734385
dog;0.010147526190927248
is;0.009169989088315705
"cute"",";0.010882279809684158
"return_tensors=""pt"")";0.02043249909132874
outputs;0.01109470997159233
=;0.00963438116995772
model(**inputs);0.015992656160363366
last_hidden_states;0.01646902800395474
=;0.009611620842117431
outputs.last_hidden_state;0.011849120503584106
***;0.008904321145321605
