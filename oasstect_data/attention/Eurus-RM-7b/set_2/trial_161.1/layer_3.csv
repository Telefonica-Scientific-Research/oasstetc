text;attention
The;0.013421663947755801
easiest;0.014420615037346618
way;0.012408437845355433
to;0.01190204021838154
import;0.019231043495744015
the;0.012048671077351145
BERT;0.016503895583206845
language;0.014140134952375998
model;0.013165315351519028
into;0.013208993415388946
python;0.014847112830066005
for;0.01187858892532659
use;0.012152889287595973
with;0.012122232716897356
PyTorch;0.01918393929015532
is;0.012825127065789761
using;0.012078817450567658
the;0.011311200155134395
Hugging;0.016992565252241493
Face;0.015352117386505031
Transformer's;0.017355449324382977
library,;0.01388945953194129
which;0.011426244595367034
has;0.01145624037194294
built;0.012200657788353327
in;0.012864035042675479
methods;0.012806092768733778
for;0.011911232488404607
pre-training,;0.019147557177267696
inference,;0.01441127034772268
and;0.011232021909444478
deploying;0.013067892825958445
BERT.;0.013736263562716459
â€˜**;0.015764973072758117
from;0.013310539434879035
transformers;0.01489067711653727
import;0.012922255656190797
AutoTokenizer,;0.019880031127575633
BertModel;0.014171078257266717
import;0.012199663906541886
torch;0.013765098363806304
tokenizer;0.014098980959655352
=;0.012224179687165023
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09825024814884122
model;0.011980806795925225
=;0.01189227998592271
"BertModel.from_pretrained(""bert-base-uncased"")";0.05416287425069883
inputs;0.012649976508172938
=;0.011865817998179649
"tokenizer(""Hello,";0.0260716762814478
my;0.011294589623637946
dog;0.011841392207642649
is;0.010919484343076706
"cute"",";0.013098137196320574
"return_tensors=""pt"")";0.02680173394264573
outputs;0.012517721689182505
=;0.011516901597834108
model(**inputs);0.019305263083169457
last_hidden_states;0.01859474598994745
=;0.011281651295749678
outputs.last_hidden_state;0.017506015234191173
***;0.010521387225421185
