text;attention
The;0.013123667040447741
easiest;0.01284132317652572
way;0.012143959734178436
to;0.010989048049353616
import;0.013970847341954001
the;0.010592700009252866
BERT;0.020491687859768022
language;0.010802401838101269
model;0.013592156643916133
into;0.011554234212591349
python;0.012198791792215268
for;0.0116112191088279
use;0.01049056212543063
with;0.010534167262704616
PyTorch;0.013975349479224112
is;0.01167476359079684
using;0.011529555372303234
the;0.010808933653398078
Hugging;0.014958155237498369
Face;0.011821564045968416
Transformer's;0.02126625747297002
library,;0.0129908525339286
which;0.01121368282248711
has;0.010963707426332805
built;0.010575541019534408
in;0.010875782371395385
methods;0.011416923947771293
for;0.011427060775246375
pre-training,;0.017463943085222793
inference,;0.012863715200788186
and;0.010146044951161167
deploying;0.010741961666278483
BERT.;0.021862193194392053
â€˜**;0.01800974536715932
from;0.015867943208476676
transformers;0.013550722537504445
import;0.013278400685782935
AutoTokenizer,;0.020778329311274234
BertModel;0.013824666497449085
import;0.013673379091481938
torch;0.012782968320820367
tokenizer;0.01602110304155586
=;0.012906089756538678
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.1369770353188181
model;0.01083135474704313
=;0.010188533431404448
"BertModel.from_pretrained(""bert-base-uncased"")";0.07897384631040766
inputs;0.010912708462940722
=;0.010722844764258794
"tokenizer(""Hello,";0.019346081123069674
my;0.010006309597014896
dog;0.010067631308540879
is;0.009981048822150689
"cute"",";0.012707399110744023
"return_tensors=""pt"")";0.01758489922704367
outputs;0.010498220767935596
=;0.010144784576335322
model(**inputs);0.016867736306407373
last_hidden_states;0.014140746284233107
=;0.00997850193621169
outputs.last_hidden_state;0.015374599622792882
***;0.010489616422638447
