text;attention
The;0.013251262475315245
easiest;0.011656995640411507
way;0.011723696536642987
to;0.010310608566395573
import;0.014995766189125678
the;0.010137912074221225
BERT;0.0178703645023542
language;0.009579371196524682
model;0.01374965010025332
into;0.011935245825254015
python;0.012457379905451234
for;0.011046149549652442
use;0.010273667930739605
with;0.009838878198774277
PyTorch;0.014019809140190203
is;0.011867741704844503
using;0.01258069135149094
the;0.010953622294386603
Hugging;0.013348763625332622
Face;0.011997825786417766
Transformer's;0.021604850716233536
library,;0.014421102600360397
which;0.011100690131669469
has;0.011024796516509148
built;0.009865989790662399
in;0.011085652359311917
methods;0.011433107783410288
for;0.01040296021257927
pre-training,;0.014581103510823685
inference,;0.011567297354820118
and;0.009637850662060682
deploying;0.012038703940650408
BERT.;0.0173270049271186
â€˜**;0.014720456857667406
from;0.019995890831446976
transformers;0.01300248824335505
import;0.01222008497986569
AutoTokenizer,;0.015664756826021732
BertModel;0.010645031890959831
import;0.012362163212094397
torch;0.010621190892331017
tokenizer;0.014166747475458123
=;0.012695699518122135
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.09065109784958544
model;0.009959202912189656
=;0.010203492238207983
"BertModel.from_pretrained(""bert-base-uncased"")";0.15380535332177347
inputs;0.01058500091051652
=;0.010618660012551006
"tokenizer(""Hello,";0.026858198712349287
my;0.00941920895508207
dog;0.01005371437143451
is;0.009474508626576384
"cute"",";0.011394781887264345
"return_tensors=""pt"")";0.018936303602527835
outputs;0.009828015068599772
=;0.009805078258550443
model(**inputs);0.016716020732850976
last_hidden_states;0.013482019041893546
=;0.009677463500967843
outputs.last_hidden_state;0.013682033222404223
***;0.009068822947363967
