text;attention
The;0.014285464689762785
easiest;0.008185068469558215
way;0.008082150730171333
to;0.007150197429911928
import;0.013547985748107052
the;0.007700013906186761
BERT;0.03127806470112469
language;0.0086876632480754
model;0.016364023661174384
into;0.011580929187566107
python;0.019532452338020416
for;0.010439537718647387
use;0.00855205618322169
with;0.007762611298276978
PyTorch;0.01773585968066803
is;0.011644247519886628
using;0.008456137827043558
the;0.00744446672197628
Hugging;0.012293368738870849
Face;0.012340415932348884
Transformer's;0.03923704917482857
library,;0.01529397117817999
which;0.008193983011700986
has;0.007556208613488543
built;0.0065978782646990124
in;0.007844538071298061
methods;0.008396989647895895
for;0.007658169289362828
pre-training,;0.014178635739397075
inference,;0.009899601981675757
and;0.0067951384822002425
deploying;0.008703857119497392
BERT.;0.013101674647640317
â€˜**;0.010687303445225178
from;0.012014494325391128
transformers;0.011422574854142574
import;0.01180244431448653
AutoTokenizer,;0.029753564798140904
BertModel;0.015738617383282062
import;0.009481032865510929
torch;0.008585642614668832
tokenizer;0.01246952755072531
=;0.01186536966449399
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2085295059275182
model;0.009268077469152897
=;0.008376470253064426
"BertModel.from_pretrained(""bert-base-uncased"")";0.0715158891496299
inputs;0.010568024687655164
=;0.008041708818958052
"tokenizer(""Hello,";0.027480386959508326
my;0.0067127264466447945
dog;0.007375940383467734
is;0.006709705698308735
"cute"",";0.010029523997904405
"return_tensors=""pt"")";0.02708379028060333
outputs;0.008390361646611489
=;0.007378618388882236
model(**inputs);0.014339044037376484
last_hidden_states;0.015536028928235739
=;0.007192808251059536
outputs.last_hidden_state;0.00870642208863259
***;0.006423983818254278
