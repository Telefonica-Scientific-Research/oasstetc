text;attention
The;0.009884064414499166
easiest;0.003067423406615523
way;0.002987166933502302
to;0.0024612551012373543
import;0.004265244752229211
the;0.002658351938919256
BERT;0.02058463595929097
language;0.0032337010528484937
model;0.005826881194604418
into;0.004403426483362054
python;0.0064439256780711935
for;0.0035269203289382887
use;0.0027063455285268783
with;0.00267990698801859
PyTorch;0.014441860938294486
is;0.004209011978488277
using;0.0031903231635566683
the;0.0032877699527917286
Hugging;0.009649947031282342
Face;0.005610170401678915
Transformer's;0.027044749762825934
library,;0.004379358087033841
which;0.0025176585922110054
has;0.002574883905981804
built;0.002334088444474532
in;0.0027604682507269703
methods;0.002902381429951698
for;0.0028095913185245827
pre-training,;0.010894457099972613
inference,;0.004956679534205639
and;0.0023483305633762463
deploying;0.002803292506739658
BERT.;0.00592790378247431
â€˜**;0.0036526598228642667
from;0.004833041703265518
transformers;0.00448571024200972
import;0.0038768850074998263
AutoTokenizer,;0.041828131539731965
BertModel;0.0067431650179365235
import;0.004852136711276109
torch;0.00447383607910849
tokenizer;0.005607692266877587
=;0.004568932143131055
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5805412573574981
model;0.0033252962565644708
=;0.002871851243764167
"BertModel.from_pretrained(""bert-base-uncased"")";0.05091365700527123
inputs;0.004466830508047081
=;0.0029642453411802406
"tokenizer(""Hello,";0.01261961442551667
my;0.0024349094803711308
dog;0.002550892809774895
is;0.0023147667489783383
"cute"",";0.003362383555238015
"return_tensors=""pt"")";0.024894281044979746
outputs;0.0030987494979818667
=;0.0026093153778105364
model(**inputs);0.010766254771429172
last_hidden_states;0.00745407877982609
=;0.0025933965341140954
outputs.last_hidden_state;0.0036919392309349135
***;0.002231912991763357
