text;attention
The;0.009227090344198233
easiest;0.007077680523445855
way;0.00849120602933525
to;0.006543274970962613
import;0.007914642224054821
the;0.006880883117308852
BERT;0.01149378998361948
language;0.006529891555079901
model;0.007624885006033002
into;0.007772264533358588
python;0.00954165482325738
for;0.0069435635344970645
use;0.006476474150285594
with;0.006750304783409961
PyTorch;0.010543399591639293
is;0.007098684105789273
using;0.008122820199158158
the;0.007028632863485828
Hugging;0.009707454879693938
Face;0.007848801222880836
Transformer's;0.014726198227429472
library,;0.009173268195170704
which;0.006977421531881334
has;0.006795959280667105
built;0.006683515337097077
in;0.00722982664821323
methods;0.007915213061459763
for;0.00714355839188701
pre-training,;0.013564486662631211
inference,;0.008055205798111175
and;0.006345921450788722
deploying;0.007040541965685491
BERT.;0.01853454010496564
â€˜**;0.01552841728703466
from;0.012611148253092517
transformers;0.01091601887212111
import;0.008939360907317449
AutoTokenizer,;0.012496079053283623
BertModel;0.009727896655416662
import;0.00881269781787945
torch;0.007826882005114734
tokenizer;0.013917791733324672
=;0.009866469531764894
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.19550572109885447
model;0.007179475463311347
=;0.006854648576410109
"BertModel.from_pretrained(""bert-base-uncased"")";0.2661570307535345
inputs;0.007524046090020115
=;0.007356467506188494
"tokenizer(""Hello,";0.015172205212149343
my;0.006225398082797578
dog;0.006462134345326919
is;0.006191041092640024
"cute"",";0.0089593862447696
"return_tensors=""pt"")";0.01602221500958602
outputs;0.006808907229754361
=;0.00650542925682875
model(**inputs);0.012148025140826942
last_hidden_states;0.010213637638528646
=;0.006322193985490272
outputs.last_hidden_state;0.011775522788533407
***;0.006170697270617508
