text;attention
The;0.016106378886322915
easiest;0.013331402921854276
way;0.011696167869914346
to;0.011185642387000774
import;0.016777526587798238
the;0.01203130956190686
BERT;0.022918791774069098
language;0.01378066085915007
model;0.015019145633500538
into;0.015825646948541678
python;0.014341393194521282
for;0.012218451331837299
use;0.011096998379443175
with;0.011212237595460991
PyTorch;0.014727188753096232
is;0.012390669813026656
using;0.01208053031301017
the;0.010485474062375594
Hugging;0.0157025867212016
Face;0.018180450662925146
Transformer's;0.01766329950995153
library,;0.014037596462541137
which;0.011076918771719483
has;0.010997529193900152
built;0.01104911916302891
in;0.011642970623283025
methods;0.011097739704842416
for;0.011367566030344504
pre-training,;0.016172116620275418
inference,;0.012126686709309628
and;0.009968947769675315
deploying;0.01097634783842157
BERT.;0.013155859530550347
â€˜**;0.017188449163973364
from;0.014066147508995723
transformers;0.013341236015222808
import;0.012123800884971822
AutoTokenizer,;0.019898476888770122
BertModel;0.015716987250892638
import;0.01301020398380578
torch;0.012109303387975294
tokenizer;0.01421446913717061
=;0.013852548198111507
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.13202847026389608
model;0.013473768571571133
=;0.013325485159453898
"BertModel.from_pretrained(""bert-base-uncased"")";0.05372612785154139
inputs;0.012087516152500484
=;0.01243661525873928
"tokenizer(""Hello,";0.02385994648300427
my;0.01024526703000437
dog;0.010551193780964887
is;0.009958764747920852
"cute"",";0.011066711115487161
"return_tensors=""pt"")";0.018456091314793015
outputs;0.01096339202605635
=;0.01066068141043501
model(**inputs);0.017337125262545005
last_hidden_states;0.017751366766905165
=;0.011309537167535244
outputs.last_hidden_state;0.01322256802228315
***;0.009576397009673197
