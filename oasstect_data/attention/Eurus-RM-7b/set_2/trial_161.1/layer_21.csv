text;attention
The;0.01232339001328932
easiest;0.009006546313067893
way;0.008264502880180116
to;0.0075097147277362036
import;0.01083979911585281
the;0.008296016111037826
BERT;0.03806706479421728
language;0.009077343787582808
model;0.012230284685786725
into;0.009573228221716261
python;0.010941929822855193
for;0.008585583354942953
use;0.007553323524408035
with;0.007992053741543378
PyTorch;0.01622197608846913
is;0.008199997024140858
using;0.008572520120680842
the;0.008333582080540683
Hugging;0.012518369545087319
Face;0.010739517822865773
Transformer's;0.02556824645461649
library,;0.01068925014295253
which;0.007405239315353806
has;0.007926576221373152
built;0.00730666759177813
in;0.008485959118200853
methods;0.008368823431126222
for;0.008526254919505167
pre-training,;0.017305785784314392
inference,;0.010239065659807008
and;0.007062792911641062
deploying;0.008401636107208506
BERT.;0.015541360948846129
â€˜**;0.010283834311317288
from;0.010854675484132041
transformers;0.010428459986836558
import;0.009064857061794808
AutoTokenizer,;0.029555304235282843
BertModel;0.013586659574003358
import;0.00925694029860692
torch;0.009350708837529993
tokenizer;0.014303499086585134
=;0.00933947365165383
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.2757836657410004
model;0.008437509296635956
=;0.007921545402737653
"BertModel.from_pretrained(""bert-base-uncased"")";0.05854618102627034
inputs;0.009052571236175647
=;0.00848310206759534
"tokenizer(""Hello,";0.020368946766189392
my;0.007087072779458437
dog;0.0074669334838668
is;0.00721397275867086
"cute"",";0.009389490390448518
"return_tensors=""pt"")";0.01940277655550563
outputs;0.007616004565956153
=;0.007393031695560633
model(**inputs);0.014241856483160321
last_hidden_states;0.01297225175333398
=;0.007225118596143468
outputs.last_hidden_state;0.010964813625069507
***;0.006734340865753337
