text;attention
The;0.012985914658832677
easiest;0.011814186475386932
way;0.01108186593299548
to;0.009874712471167234
import;0.013042910164907218
the;0.010099243424373586
BERT;0.0220669404854006
language;0.0108145772245982
model;0.011791165335057537
into;0.010949187694251763
python;0.014132011099059209
for;0.011291951325058003
use;0.010235031117677373
with;0.010261278261372946
PyTorch;0.01679567583206904
is;0.010994725043251408
using;0.011125539857056655
the;0.010603716551814142
Hugging;0.013841178639101222
Face;0.012541073177254832
Transformer's;0.018113647090876126
library,;0.012398139899113246
which;0.010665990350397642
has;0.011241818042849587
built;0.009925997017926761
in;0.011140423398580174
methods;0.010884261960988866
for;0.010961763766701686
pre-training,;0.015275325780437499
inference,;0.011601361862866979
and;0.010145574851408353
deploying;0.010542256871073508
BERT.;0.017523078091821655
â€˜**;0.015136249894696537
from;0.0157313190344047
transformers;0.011634863982384542
import;0.012283812902930303
AutoTokenizer,;0.019029145975784485
BertModel;0.012690621943810099
import;0.013407411883026113
torch;0.013679089296609753
tokenizer;0.01707637356235855
=;0.01360912374332985
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.12412968888854833
model;0.010416962522614894
=;0.010562313150080046
"BertModel.from_pretrained(""bert-base-uncased"")";0.10503883106995304
inputs;0.011256099314273378
=;0.01091873568340381
"tokenizer(""Hello,";0.029360378439559253
my;0.009466719767357468
dog;0.009897365662485383
is;0.009386988615319124
"cute"",";0.013421398449874715
"return_tensors=""pt"")";0.020683660632022448
outputs;0.010423262519475965
=;0.009993308293947003
model(**inputs);0.016853771945210837
last_hidden_states;0.013666891992783354
=;0.009739174558853229
outputs.last_hidden_state;0.014236995396105087
***;0.009506917123069641
