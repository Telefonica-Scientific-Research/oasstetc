text;attention
The;0.0031647939826411923
easiest;0.0009334679913137754
way;0.0008713888974327702
to;0.0007215015949617395
import;0.0014142485944613717
the;0.0008332565816944402
BERT;0.007090749954869932
language;0.0008797496639819319
model;0.0026156162391207166
into;0.002098460911645368
python;0.0036089453765158317
for;0.001523585316829541
use;0.0009885338514966995
with;0.0008081882470377832
PyTorch;0.005055618260809569
is;0.002019879091303269
using;0.0010091569839449607
the;0.0009518313403463744
Hugging;0.0028248160345967516
Face;0.001641160807441225
Transformer's;0.009581445355015181
library,;0.0018670788170320406
which;0.0009056957657596957
has;0.0008850318013898422
built;0.0006907944220601256
in;0.0009199066919329031
methods;0.0009370760433331632
for;0.000849013981060521
pre-training,;0.003919859537343761
inference,;0.0016941207885265135
and;0.000751191951720366
deploying;0.0008679337571786998
BERT.;0.0034009370722220887
â€˜**;0.0015221710653857195
from;0.001748319112826458
transformers;0.0017044182361695436
import;0.0017678606300888526
AutoTokenizer,;0.008648445054153464
BertModel;0.0017801258996675057
import;0.0014544479783726511
torch;0.0010321855758227707
tokenizer;0.002510615268655195
=;0.0019650718292232045
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7882481503929709
model;0.0011940747238100963
=;0.000943264972181434
"BertModel.from_pretrained(""bert-base-uncased"")";0.08103640462794426
inputs;0.0015542238611009089
=;0.0011475778505637536
"tokenizer(""Hello,";0.010670273374135036
my;0.0007315287054284038
dog;0.0007843930662948793
is;0.0006760543361618992
"cute"",";0.0013030811984884084
"return_tensors=""pt"")";0.007543172007510741
outputs;0.001043796177054379
=;0.0008523945660534375
model(**inputs);0.005194371735617771
last_hidden_states;0.0018306754244579138
=;0.0007929226898827256
outputs.last_hidden_state;0.0013727377802453475
***;0.0006222061527120782
