text;attention
The;0.015916050935554628
easiest;0.012775533699781585
way;0.011468708105249135
to;0.010338454856331288
import;0.017560043166604228
the;0.010900009497552255
BERT;0.03109833752544616
language;0.010330509444227767
model;0.013582167428013533
into;0.012542881914338312
python;0.012766352395628653
for;0.011485654290580944
use;0.010814990594894706
with;0.010388672099759815
PyTorch;0.017634387633028957
is;0.011225674569629526
using;0.011781100514196449
the;0.01089532639278335
Hugging;0.022483779215344446
Face;0.0144494557571405
Transformer's;0.022792572842524027
library,;0.012961569219924193
which;0.010826321436237796
has;0.010787362081944061
built;0.010308253517797093
in;0.012133355437716445
methods;0.011808426642550702
for;0.010656465314822656
pre-training,;0.016207296094540694
inference,;0.013921879861455955
and;0.010614635615972553
deploying;0.011003600585204547
BERT.;0.017227423944420613
â€˜**;0.015133265985808594
from;0.013550750783343607
transformers;0.01508751393057777
import;0.01189018597346176
AutoTokenizer,;0.017436789222982682
BertModel;0.013018239830164462
import;0.012577073459984009
torch;0.011733713623960512
tokenizer;0.015180921821884312
=;0.011666331975041603
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.14429030887996064
model;0.011103273920975881
=;0.01095885012816168
"BertModel.from_pretrained(""bert-base-uncased"")";0.05042066792986912
inputs;0.012632253845858591
=;0.010429858829965844
"tokenizer(""Hello,";0.02316564264952873
my;0.009954200871576757
dog;0.010133294247545607
is;0.009973288715893132
"cute"",";0.01215058525658429
"return_tensors=""pt"")";0.021208144572090238
outputs;0.010691005606527745
=;0.00988274189983738
model(**inputs);0.01663165111652486
last_hidden_states;0.014158238866716527
=;0.010148237205454875
outputs.last_hidden_state;0.013553405609916774
***;0.009552310608604547
