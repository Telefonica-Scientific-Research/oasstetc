text;attention
The;9.658683009050873e-16
easiest;7.578356695513356e-16
way;7.51709782611255e-16
to;7.242421234304983e-16
import;9.079840405374088e-16
the;7.197958518623664e-16
BERT;1.2149290905322206e-15
language;9.135729399646583e-16
model;8.002529520276105e-16
into;9.106483218783512e-16
python;8.23439799150897e-16
for;7.798637348788745e-16
use;6.965265268519372e-16
with;7.334599227821736e-16
PyTorch;1.0423398948232623e-15
is;9.498165868818322e-16
using;7.799501989833485e-16
the;6.521872884736268e-16
Hugging;8.877980160910511e-16
Face;1.1701796284097336e-15
Transformer's;1.639825003340265e-15
library,;8.518700884051914e-16
which;6.875004948397365e-16
has;6.58660385862782e-16
built;6.402411775734327e-16
in;6.864262405950616e-16
methods;7.082130573980183e-16
for;6.960247351190376e-16
pre-training,;1.2522902307349145e-15
inference,;7.859191534540011e-16
and;6.504828416386941e-16
deploying;7.492769361894694e-16
BERT.;0.9999999999999247
â€˜**;9.139135264626056e-16
from;1.0179790804693156e-15
transformers;9.235654171569944e-16
import;9.224048377329071e-16
AutoTokenizer,;1.7173040064862707e-15
BertModel;9.381379456791308e-16
import;8.676061592924541e-16
torch;7.316537273874807e-16
tokenizer;9.138378659155112e-16
=;8.828934652970772e-16
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.083371227878884e-14
model;8.189636893731395e-16
=;7.770757922002686e-16
"BertModel.from_pretrained(""bert-base-uncased"")";3.041775269837734e-15
inputs;7.993982886291025e-16
=;8.062139970115815e-16
"tokenizer(""Hello,";1.5330049975513913e-15
my;6.589528730727571e-16
dog;6.770220637542807e-16
is;6.548936755410739e-16
"cute"",";7.967953939417624e-16
"return_tensors=""pt"")";1.2005663211513452e-15
outputs;7.203844021508582e-16
=;6.675432952896668e-16
model(**inputs);1.0505727893482304e-15
last_hidden_states;1.2077567560616692e-15
=;6.765576818501222e-16
outputs.last_hidden_state;7.656538967579047e-16
***;6.0794588989854795e-16
