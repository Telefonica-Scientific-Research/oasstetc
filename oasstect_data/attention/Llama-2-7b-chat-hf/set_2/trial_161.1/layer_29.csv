text;attention
The;3.440116716589386e-11
easiest;2.158123471471846e-11
way;2.330677838775689e-11
to;2.0639591941843403e-11
import;2.414445094351313e-11
the;2.3261104188649815e-11
BERT;4.8227089574697415e-11
language;2.267946956586269e-11
model;2.4165749255021916e-11
into;2.6156711005713128e-11
python;2.555330062435875e-11
for;2.5218521264657164e-11
use;2.1913205477926938e-11
with;2.385766848841222e-11
PyTorch;5.6578706010062235e-11
is;2.2477742066587388e-11
using;2.8435629654342003e-11
the;2.8584862224359162e-11
Hugging;1.0783694399964959e-10
Face;2.5894818465340092e-11
Transformer's;3.5769260975279496e-09
library,;2.926270042959661e-11
which;2.0767408428410526e-11
has;2.1902887857260143e-11
built;2.156163269791545e-11
in;2.1294616954289167e-11
methods;2.168621287471649e-11
for;2.0928679027958222e-11
pre-training,;4.285745315748863e-11
inference,;2.44758911334495e-11
and;1.9747247704300012e-11
deploying;2.2404086801918693e-11
BERT.;0.9999999920645942
â€˜**;4.1879094919139886e-11
from;3.620409629186713e-11
transformers;4.3235672270184e-11
import;2.8422264307243836e-11
AutoTokenizer,;6.039599374636343e-11
BertModel;3.959863382160156e-11
import;2.5160496981375214e-11
torch;3.305399238805604e-11
tokenizer;3.506003495916404e-11
=;2.60598019775376e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.21917142688553e-09
model;2.2947755495308754e-11
=;2.075615678133934e-11
"BertModel.from_pretrained(""bert-base-uncased"")";3.2770657745988485e-10
inputs;2.4637143906559766e-11
=;2.2196128558877316e-11
"tokenizer(""Hello,";6.267201186902674e-11
my;2.0491584950140363e-11
dog;2.20962319573876e-11
is;2.2010323384743458e-11
"cute"",";3.175169690179216e-11
"return_tensors=""pt"")";1.1081494443601621e-10
outputs;2.0304370328845387e-11
=;1.9076827041136785e-11
model(**inputs);3.4435543789077124e-11
last_hidden_states;3.774046692762823e-11
=;1.8768813800383486e-11
outputs.last_hidden_state;3.088782524228077e-11
***;1.914057690902145e-11
