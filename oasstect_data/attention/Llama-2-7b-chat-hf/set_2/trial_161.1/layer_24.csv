text;attention
The;2.2488754754531185e-14
easiest;1.798480596296155e-14
way;1.7785661857580476e-14
to;1.67881688249104e-14
import;2.2899810176392758e-14
the;1.736176329924533e-14
BERT;6.048521614205425e-14
language;1.832755297320351e-14
model;2.0064280318910174e-14
into;1.9679947678810623e-14
python;2.025931346079165e-14
for;1.8138232046331948e-14
use;1.7348106175557685e-14
with;1.772491166826448e-14
PyTorch;3.3431521230346854e-14
is;1.829609300391611e-14
using;1.9849363620256812e-14
the;1.8749258194948826e-14
Hugging;3.1841262278198835e-14
Face;1.9529873217704734e-14
Transformer's;3.988397492263868e-13
library,;2.1236206654343518e-14
which;1.7329986295881414e-14
has;1.697338525321163e-14
built;1.745520055234317e-14
in;1.7115277274227254e-14
methods;1.7982515559888715e-14
for;1.722107431994775e-14
pre-training,;2.7601939151512836e-14
inference,;1.94846044720719e-14
and;1.610601856439869e-14
deploying;1.8310345673075427e-14
BERT.;0.9999999999981284
â€˜**;2.3602145671068056e-14
from;1.8695425078721867e-14
transformers;2.4073591005952885e-14
import;1.9782023438700428e-14
AutoTokenizer,;3.133063863703128e-14
BertModel;2.5445364359066897e-14
import;1.9398442745629006e-14
torch;1.9809378010204886e-14
tokenizer;2.349299011171111e-14
=;1.7683520950512812e-14
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.7668112992559123e-13
model;1.9145397279434265e-14
=;1.6772495915345168e-14
"BertModel.from_pretrained(""bert-base-uncased"")";6.242152506711036e-14
inputs;2.0492114154748068e-14
=;1.7001520194913555e-14
"tokenizer(""Hello,";3.491241078292304e-14
my;1.6454577889826422e-14
dog;1.71929659740197e-14
is;1.675372571134729e-14
"cute"",";1.92921629743241e-14
"return_tensors=""pt"")";4.116158178845985e-14
outputs;1.826446195780388e-14
=;1.6117225646036256e-14
model(**inputs);2.328873476571609e-14
last_hidden_states;2.4548734661144635e-14
=;1.58645458506841e-14
outputs.last_hidden_state;2.0876498545058073e-14
***;1.6205482036661238e-14
