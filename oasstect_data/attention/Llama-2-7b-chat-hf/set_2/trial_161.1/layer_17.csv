text;attention
The;3.3429337330964285e-14
easiest;2.5547694380555056e-14
way;2.6048289278000388e-14
to;2.4457200066841235e-14
import;3.300286813663869e-14
the;2.5151262167649394e-14
BERT;1.52515036551459e-13
language;2.740204560210486e-14
model;3.951261032342865e-14
into;3.024099202572024e-14
python;3.5827246902416335e-14
for;2.7373998424010273e-14
use;2.4391461113580533e-14
with;2.4923468359444178e-14
PyTorch;5.2245466335771204e-14
is;3.024209788189013e-14
using;2.963449089497521e-14
the;2.7160981182272662e-14
Hugging;4.0166844954837165e-14
Face;4.268974061691785e-14
Transformer's;1.1974334728614031e-12
library,;3.361616829060185e-14
which;2.4567956529227254e-14
has;2.4146879617147362e-14
built;2.3297994998018577e-14
in;2.642439150223491e-14
methods;2.6755483711006522e-14
for;2.461932110858055e-14
pre-training,;4.399523027841821e-14
inference,;3.0777375770129676e-14
and;2.280416813381363e-14
deploying;2.578562099895867e-14
BERT.;0.9999999999960156
â€˜**;2.9193218575074413e-14
from;3.118581248826453e-14
transformers;3.576116546305449e-14
import;2.7990366723496616e-14
AutoTokenizer,;8.101010454205962e-14
BertModel;4.3478829812116844e-14
import;2.840705075300058e-14
torch;3.206870933869679e-14
tokenizer;5.10505571280053e-14
=;3.3101684763877875e-14
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";5.560233848493506e-13
model;2.6393721300494982e-14
=;2.4642197160319086e-14
"BertModel.from_pretrained(""bert-base-uncased"")";2.017172400358709e-13
inputs;2.8830545290387517e-14
=;2.5618571983420022e-14
"tokenizer(""Hello,";6.515259312765026e-14
my;2.3207170602267935e-14
dog;2.6087112973638065e-14
is;2.3135799308823275e-14
"cute"",";2.8379951236051566e-14
"return_tensors=""pt"")";7.233631246445348e-14
outputs;2.7193767098434467e-14
=;2.5175550832301648e-14
model(**inputs);4.7770661537526447e-14
last_hidden_states;4.6770849934104704e-14
=;2.4344120873227897e-14
outputs.last_hidden_state;3.982828281641496e-14
***;2.2730252729008637e-14
