text;attention
The;1.7753256122582272e-13
easiest;1.3124019678460852e-13
way;1.2522582383375591e-13
to;1.2296621318468524e-13
import;1.7549052837737694e-13
the;1.2141032027178588e-13
BERT;2.924081474146821e-13
language;1.4334610102638542e-13
model;1.8884436742680249e-13
into;1.729246155190236e-13
python;2.236717449195947e-13
for;1.4471689196125968e-13
use;1.3504922983449674e-13
with;1.1932767530445172e-13
PyTorch;2.6209851829966995e-13
is;1.657391880155295e-13
using;1.4370361231931039e-13
the;1.1481169734744267e-13
Hugging;1.5901052844365389e-13
Face;2.1856879428359842e-13
Transformer's;8.83347056563523e-13
library,;1.7549072188841983e-13
which;1.2148045793369937e-13
has;1.151040953475376e-13
built;1.0521951890989204e-13
in;1.4602585851061854e-13
methods;1.2348482049531015e-13
for;1.1975639286128448e-13
pre-training,;2.3792275397180155e-13
inference,;1.5074980919794064e-13
and;1.0940161229390596e-13
deploying;1.365577777988458e-13
BERT.;0.9999999999826588
â€˜**;1.8162321160357764e-13
from;1.657586998441835e-13
transformers;1.7599728975362758e-13
import;1.8202752029861096e-13
AutoTokenizer,;5.445670949440632e-13
BertModel;2.5496264379141085e-13
import;1.8576336868830435e-13
torch;1.4007666354082738e-13
tokenizer;1.9795567431830987e-13
=;1.7039267528640583e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";5.28507993936613e-12
model;1.6315225150489034e-13
=;1.5328227976919374e-13
"BertModel.from_pretrained(""bert-base-uncased"")";1.0528969829708844e-12
inputs;1.5896278544237213e-13
=;1.5276915314692473e-13
"tokenizer(""Hello,";3.9453202965821834e-13
my;1.1460075812523144e-13
dog;1.2541114554435096e-13
is;1.1323763707555018e-13
"cute"",";1.40216488784011e-13
"return_tensors=""pt"")";3.061913112470514e-13
outputs;1.4254035065537059e-13
=;1.2042377117455587e-13
model(**inputs);2.717406810458736e-13
last_hidden_states;2.856041615006378e-13
=;1.1928110118387762e-13
outputs.last_hidden_state;1.7316237349351066e-13
***;1.0643671977690508e-13
