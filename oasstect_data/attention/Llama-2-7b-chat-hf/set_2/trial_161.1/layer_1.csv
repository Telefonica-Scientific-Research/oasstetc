text;attention
The;0.003183787352481266
easiest;9.071836291104852e-05
way;6.0054257518961347e-05
to;0.003201642071909408
import;5.850708219370732e-05
the;0.08846958535082437
BERT;0.00028864418466590546
language;5.842974332968952e-05
model;4.365954844707639e-05
into;0.00010874585835579077
python;7.943496146238617e-05
for;0.00021023785357976167
use;3.529598457721226e-05
with;0.0001378757726809335
PyTorch;0.00046064775909070095
is;7.754358735332079e-05
using;4.282478985024666e-05
the;0.004756028245662282
Hugging;0.00019580279376148484
Face;3.369518102390721e-05
Transformer's;0.001139221763483563
library,;0.007365705934977426
which;3.109182387627996e-05
has;2.9708318314225626e-05
built;2.6261885416415934e-05
in;5.867800651584808e-05
methods;2.672488366359798e-05
for;4.6822903300243436e-05
pre-training,;0.00695461330316187
inference,;0.0004096708457781051
and;4.599172608685363e-05
deploying;4.214490488269273e-05
BERT.;0.029441400820034963
â€˜**;6.239352598711252e-05
from;3.3241100765390216e-05
transformers;4.083427186430283e-05
import;2.5252818910462724e-05
AutoTokenizer,;0.0006121365425942064
BertModel;3.879731751944522e-05
import;2.3864066076384765e-05
torch;4.08046679730216e-05
tokenizer;3.00449895163168e-05
=;4.0332308109044686e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.7914982167225872
model;2.027047977613052e-05
=;3.11373889228296e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.05950923286084854
inputs;1.9385981693146907e-05
=;2.5815802367884095e-05
"tokenizer(""Hello,";0.00018738575382272647
my;1.8881808937188845e-05
dog;1.8854747363613917e-05
is;2.0227918640040297e-05
"cute"",";4.609942964078357e-05
"return_tensors=""pt"")";0.0001789796507204733
outputs;1.8878578065308204e-05
=;2.2780561682421635e-05
model(**inputs);9.652097595020848e-05
last_hidden_states;4.373282722018538e-05
=;1.8474231029690417e-05
outputs.last_hidden_state;5.158190855865974e-05
***;1.4642901685651385e-05
