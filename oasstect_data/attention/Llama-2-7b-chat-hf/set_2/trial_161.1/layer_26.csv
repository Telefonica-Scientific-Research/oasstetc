text;attention
The;4.5361524097369365e-13
easiest;3.351109714497945e-13
way;3.5271791742386443e-13
to;2.956989482653487e-13
import;3.959618467521839e-13
the;3.150302766088956e-13
BERT;2.3182604479809714e-12
language;3.5144053844414556e-13
model;3.703350364332915e-13
into;3.6808527156950716e-13
python;3.914012389850114e-13
for;3.2977912381695157e-13
use;3.154954791800738e-13
with;3.31318521480918e-13
PyTorch;6.790129142363456e-13
is;3.3713277439645874e-13
using;4.041446758587436e-13
the;3.854515531041641e-13
Hugging;7.842139969748742e-13
Face;4.090412030335433e-13
Transformer's;1.1819798841294945e-11
library,;3.834819840572972e-13
which;3.0296630392923095e-13
has;3.0236705364529173e-13
built;3.045710756546136e-13
in;3.1045114363780364e-13
methods;3.3077113654705957e-13
for;3.0638940722394876e-13
pre-training,;5.325067794890581e-13
inference,;3.393662779072549e-13
and;2.9123186747180575e-13
deploying;3.341067148387016e-13
BERT.;0.9999999999535398
â€˜**;5.338188574526113e-13
from;3.7389839620810687e-13
transformers;4.49395331341672e-13
import;4.0565536107342077e-13
AutoTokenizer,;6.208555899111454e-13
BertModel;6.340019178563962e-13
import;3.387299180967048e-13
torch;3.988000923708829e-13
tokenizer;4.215921017671034e-13
=;3.4201392610124065e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";7.113391537311249e-12
model;3.2716625488608117e-13
=;3.0156734494989765e-13
"BertModel.from_pretrained(""bert-base-uncased"")";2.459590279580365e-12
inputs;3.336689020146266e-13
=;3.1117795034086174e-13
"tokenizer(""Hello,";7.781375795577159e-13
my;2.940743897768317e-13
dog;3.067385470287182e-13
is;3.0989906342660494e-13
"cute"",";3.599492286821149e-13
"return_tensors=""pt"")";8.808036716596828e-13
outputs;3.170038964684764e-13
=;2.882685890128944e-13
model(**inputs);5.879906581345442e-13
last_hidden_states;4.832551449965866e-13
=;2.9494961058214424e-13
outputs.last_hidden_state;4.2731136354743265e-13
***;2.855608691141603e-13
