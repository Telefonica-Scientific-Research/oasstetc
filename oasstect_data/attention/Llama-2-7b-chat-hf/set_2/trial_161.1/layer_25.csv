text;attention
The;1.660367342153467e-14
easiest;1.2645785543909502e-14
way;1.2942253099638029e-14
to;1.1604904633672797e-14
import;1.2128170210286761e-14
the;1.1544601545112589e-14
BERT;2.509925322053023e-14
language;1.2621046340672196e-14
model;1.2600065227495907e-14
into;1.2387888808504783e-14
python;1.39350310038421e-14
for;1.2022056720760823e-14
use;1.1469951737158291e-14
with;1.1661142156406475e-14
PyTorch;1.6658012865615702e-14
is;1.1827428128239368e-14
using;1.2738231462167043e-14
the;1.227286060459939e-14
Hugging;2.3965975474035956e-14
Face;1.2918832261747537e-14
Transformer's;2.652482306902186e-13
library,;1.369554143033155e-14
which;1.1210436068921076e-14
has;1.1066040322033791e-14
built;1.1172804706948075e-14
in;1.1437606019473934e-14
methods;1.1772434272258238e-14
for;1.1068284713292863e-14
pre-training,;1.5889471882057746e-14
inference,;1.1751145314779442e-14
and;1.0994751259926811e-14
deploying;1.2329516079361083e-14
BERT.;0.999999999998787
â€˜**;2.8905648953910594e-14
from;1.3608888460172093e-14
transformers;1.3554082134564692e-14
import;1.2870289269128403e-14
AutoTokenizer,;1.6588558113939528e-14
BertModel;1.4759321092460844e-14
import;1.1688582448915718e-14
torch;1.2783998402462778e-14
tokenizer;1.3514067633993462e-14
=;1.2696461009093566e-14
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";9.791876188537822e-14
model;1.1753825131654717e-14
=;1.171842547787551e-14
"BertModel.from_pretrained(""bert-base-uncased"")";4.650122583752183e-14
inputs;1.2884762961605925e-14
=;1.172251417644655e-14
"tokenizer(""Hello,";2.443071393265836e-14
my;1.131640946659692e-14
dog;1.1404285792640744e-14
is;1.1038659861464281e-14
"cute"",";1.2822411452661306e-14
"return_tensors=""pt"")";2.691849809491748e-14
outputs;1.184427593732013e-14
=;1.108444701267323e-14
model(**inputs);1.7224282614597584e-14
last_hidden_states;1.6719849552145415e-14
=;1.0978087630986316e-14
outputs.last_hidden_state;1.531404737665826e-14
***;1.1177358575620552e-14
