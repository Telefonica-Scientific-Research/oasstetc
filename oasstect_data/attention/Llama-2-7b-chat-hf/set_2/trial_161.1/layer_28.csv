text;attention
The;1.0058912598810543e-12
easiest;7.318769264327411e-13
way;7.765514969616345e-13
to;7.086586392753188e-13
import;7.738746646803911e-13
the;7.214799202244252e-13
BERT;1.3774785559060923e-12
language;7.642036886176794e-13
model;7.968622982735585e-13
into;8.305534537038127e-13
python;8.133869316688311e-13
for;8.084520479108109e-13
use;7.428573509884218e-13
with;7.737231884690527e-13
PyTorch;1.6299270371325697e-12
is;7.674365698773442e-13
using;8.721821751388121e-13
the;8.965991277890612e-13
Hugging;1.9144077232150323e-12
Face;9.391012557937457e-13
Transformer's;7.107374960205472e-11
library,;8.059013177135126e-13
which;7.198525096417634e-13
has;7.456616598222507e-13
built;7.234430914827953e-13
in;7.259407537625377e-13
methods;8.114733856247565e-13
for;7.313428999588808e-13
pre-training,;1.1212947762955105e-12
inference,;7.48565300462495e-13
and;6.974552579014662e-13
deploying;7.506530255719809e-13
BERT.;0.9999999998533646
â€˜**;1.2634825018708356e-12
from;9.27566889365392e-13
transformers;1.4497756009311214e-12
import;9.072116723864401e-13
AutoTokenizer,;1.4832533312490574e-12
BertModel;1.0423219038294696e-12
import;7.682472854718298e-13
torch;9.015348100024231e-13
tokenizer;9.084015940523967e-13
=;7.932195788835449e-13
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.6557175809761992e-11
model;7.230500518956987e-13
=;7.108550818742371e-13
"BertModel.from_pretrained(""bert-base-uncased"")";5.37255149675272e-12
inputs;7.611707123727441e-13
=;7.143510035920525e-13
"tokenizer(""Hello,";1.803288273670967e-12
my;7.286249912204679e-13
dog;7.375034005532525e-13
is;7.103755487391242e-13
"cute"",";9.322720105165311e-13
"return_tensors=""pt"")";2.405590406170116e-12
outputs;6.943456625508531e-13
=;6.612440620849758e-13
model(**inputs);9.86777833022139e-13
last_hidden_states;1.0871162421059777e-12
=;6.63838365934764e-13
outputs.last_hidden_state;9.700646742936527e-13
***;6.691444481501091e-13
