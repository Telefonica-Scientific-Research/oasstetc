text;attention
The;9.878987400329104e-15
easiest;8.397764791181868e-15
way;8.061914226626651e-15
to;7.244403777850913e-15
import;9.511778261564285e-15
the;7.305577138662802e-15
BERT;3.6856364782348996e-14
language;8.458467584375023e-15
model;1.0720376574833935e-14
into;8.548595054612307e-15
python;9.673185883296832e-15
for;7.795229678694864e-15
use;7.556072223526942e-15
with;7.466884401288757e-15
PyTorch;1.3307434052457302e-14
is;7.963875169775993e-15
using;8.615714411427801e-15
the;7.947371867557633e-15
Hugging;1.1163909104519474e-14
Face;1.0662470946864963e-14
Transformer's;1.2931609811214611e-13
library,;9.159420097150995e-15
which;7.070177293903232e-15
has;7.119353714465917e-15
built;6.916969414190222e-15
in;7.817749399410257e-15
methods;7.932149539870692e-15
for;7.496278189050657e-15
pre-training,;1.1693038393677779e-14
inference,;8.416357719558134e-15
and;6.9278415685394255e-15
deploying;7.834394975942359e-15
BERT.;0.9999999999991536
â€˜**;9.87724387089907e-15
from;8.224016201621573e-15
transformers;9.802922990457078e-15
import;9.281978589039942e-15
AutoTokenizer,;1.828919082888403e-14
BertModel;1.246012382618147e-14
import;9.28111279467947e-15
torch;8.957892237932884e-15
tokenizer;1.1519878220211592e-14
=;9.182121509731446e-15
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.0435381260425963e-13
model;8.363565002874558e-15
=;7.566060524904369e-15
"BertModel.from_pretrained(""bert-base-uncased"")";4.8535388865120604e-14
inputs;9.313467769063605e-15
=;7.589731584251291e-15
"tokenizer(""Hello,";1.6817630725858944e-14
my;7.165680013088783e-15
dog;7.618134892467322e-15
is;6.986608736778346e-15
"cute"",";8.355870345365929e-15
"return_tensors=""pt"")";1.6705376312268878e-14
outputs;8.361333234164783e-15
=;7.414952972743456e-15
model(**inputs);1.3611034838766035e-14
last_hidden_states;1.206389618733154e-14
=;7.36059661544326e-15
outputs.last_hidden_state;1.021835425457988e-14
***;6.788969680096427e-15
