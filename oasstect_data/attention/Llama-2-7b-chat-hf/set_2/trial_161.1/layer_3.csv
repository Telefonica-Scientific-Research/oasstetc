text;attention
The;4.101189645070326e-19
easiest;3.9196573215381278e-19
way;3.6396823856475094e-19
to;3.628652879591081e-19
import;4.296009406842964e-19
the;3.706197646762111e-19
BERT;5.1306274379283735e-19
language;4.107188184903707e-19
model;3.9804223135599675e-19
into;4.13722877855082e-19
python;4.216899028240137e-19
for;3.7852246417365236e-19
use;3.708238557025698e-19
with;3.7772551143005713e-19
PyTorch;5.642179706669375e-19
is;3.9886902118289903e-19
using;3.8728086505174484e-19
the;3.6369674478949807e-19
Hugging;4.936362745374725e-19
Face;3.8899373289622646e-19
Transformer's;5.384286376546272e-19
library,;4.0866040444278723e-19
which;3.507981673096183e-19
has;3.5370910576099154e-19
built;3.665166251200205e-19
in;3.62160361391213e-19
methods;3.5801340883746304e-19
for;3.5272352093806565e-19
pre-training,;5.1644108703528215e-19
inference,;3.8178179549433136e-19
and;3.3546332929479905e-19
deploying;3.594656503203078e-19
BERT.;1.0
â€˜**;4.814844215479614e-19
from;3.8319596944220547e-19
transformers;3.911718542481976e-19
import;4.018383566425391e-19
AutoTokenizer,;5.281235078839407e-19
BertModel;3.960829754108291e-19
import;3.5815565793662865e-19
torch;3.914733748403664e-19
tokenizer;3.750908419003548e-19
=;3.664269087357767e-19
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";1.8644063157131602e-18
model;3.415091770772602e-19
=;3.4948833125713967e-19
"BertModel.from_pretrained(""bert-base-uncased"")";7.960120864861061e-19
inputs;3.556146681758982e-19
=;3.4600452578097346e-19
"tokenizer(""Hello,";4.9122536739140765e-19
my;3.372946212574646e-19
dog;3.411575204965774e-19
is;3.3709176178263834e-19
"cute"",";3.7861855676558393e-19
"return_tensors=""pt"")";4.975120114756008e-19
outputs;3.441466824178204e-19
=;3.4453145226677264e-19
model(**inputs);4.589604955787145e-19
last_hidden_states;4.540444237594733e-19
=;3.2926105702408937e-19
outputs.last_hidden_state;4.030671591045991e-19
***;3.1688414194628933e-19
