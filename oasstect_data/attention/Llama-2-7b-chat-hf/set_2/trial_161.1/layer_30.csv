text;attention
The;3.479889093632261e-11
easiest;2.3985609939308183e-11
way;2.937821197679008e-11
to;2.2897510335147926e-11
import;2.4544168427900672e-11
the;2.188248157194931e-11
BERT;3.8187565894971117e-11
language;2.4415866599241388e-11
model;2.8762405331599558e-11
into;2.2970874274082053e-11
python;2.455446753664934e-11
for;2.1609617029959405e-11
use;2.1984471492683045e-11
with;2.1026698289977516e-11
PyTorch;4.09176250435523e-11
is;2.537105876068122e-11
using;2.5598689520385527e-11
the;2.3644425713433847e-11
Hugging;6.788552699363678e-11
Face;2.5041614696026625e-11
Transformer's;4.0327723355853114e-09
library,;2.93383285067054e-11
which;2.210327099196628e-11
has;2.190200191243037e-11
built;2.2771249471910475e-11
in;2.178023876846804e-11
methods;2.4952408478029236e-11
for;2.1354028283257013e-11
pre-training,;3.826255341807846e-11
inference,;2.4807510237598878e-11
and;2.1146071836952798e-11
deploying;2.2633707283437484e-11
BERT.;0.9999999916805676
â€˜**;4.107850908280211e-11
from;2.493616969825797e-11
transformers;3.694849005002269e-11
import;2.5821403499492522e-11
AutoTokenizer,;5.2769293100089495e-11
BertModel;3.4177225713014856e-11
import;2.3006270622346487e-11
torch;2.8727293706752804e-11
tokenizer;2.976310485371689e-11
=;2.493598836890976e-11
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.3153139906904267e-09
model;2.4062537613163688e-11
=;2.194113244750816e-11
"BertModel.from_pretrained(""bert-base-uncased"")";2.60303850496022e-10
inputs;2.484609804787068e-11
=;2.2137530993428745e-11
"tokenizer(""Hello,";5.829384464988145e-11
my;2.1181640544248213e-11
dog;2.2076632254819247e-11
is;2.0688186127678838e-11
"cute"",";2.8584061868971698e-11
"return_tensors=""pt"")";9.667096376134074e-11
outputs;2.2570374398098194e-11
=;2.0155789524545765e-11
model(**inputs);4.315573023322685e-11
last_hidden_states;4.232575616922492e-11
=;2.0048510223904616e-11
outputs.last_hidden_state;3.9445305183504255e-11
***;2.0185336919002264e-11
