text;attention
The;2.16551244332161e-09
easiest;1.6520060565988042e-08
way;2.6126998749393667e-09
to;2.384355744952626e-09
import;2.5308488602999805e-09
the;2.126651717150881e-09
BERT;2.042715341768092e-08
language;2.691037818335974e-09
model;2.965591939975418e-09
into;2.6422574956702504e-09
python;1.9745878333414953e-08
for;2.4369402260275102e-09
use;3.025163968453836e-09
with;2.6124972793032886e-09
PyTorch;8.005154592070086e-08
is;2.549876945313158e-09
using;3.3743102235247773e-09
the;2.1065003116570405e-09
Hugging;7.993185060708203e-09
Face;3.234774973188659e-09
Transformer's;5.7476156631991864e-08
library,;8.657272782297101e-09
which;2.791112477558619e-09
has;2.9384469927961624e-09
built;3.4243179227051232e-09
in;2.323114473978357e-09
methods;4.53479723867821e-09
for;2.3944000459065147e-09
pre-training,;5.995881094653543e-08
inference,;2.1364288780029595e-08
and;2.281153856336732e-09
deploying;9.150534951901564e-09
BERT.;6.246683260228769e-08
â€˜**;4.721587261526069e-08
from;2.2164179326446773e-09
transformers;8.203886376658371e-09
import;2.5112778727172472e-09
AutoTokenizer,;2.4677705267474437e-07
BertModel;3.8813862602854824e-08
import;2.38848402505882e-09
torch;3.812674288658655e-09
tokenizer;1.6096126131220175e-08
=;3.058876541572466e-09
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.6330576723277779
model;2.74344129923797e-09
=;3.7814956307029195e-09
"BertModel.from_pretrained(""bert-base-uncased"")";0.3669312051806337
inputs;4.265628992818274e-09
=;3.7860581157344454e-09
"tokenizer(""Hello,";1.669063862977345e-06
my;2.9523389587882413e-09
dog;3.1371255842406474e-09
is;2.1793782122827696e-09
"cute"",";1.7458293600654843e-08
"return_tensors=""pt"")";7.36030920346234e-06
outputs;6.0307821950083625e-09
=;3.049492202686023e-09
model(**inputs);2.1258538057341327e-07
last_hidden_states;9.584899589031201e-08
=;2.8297784895855327e-09
outputs.last_hidden_state;9.290660825488077e-07
***;8.351635898887387e-09
