text;attention
The;3.705650902497555e-05
easiest;3.761929988794317e-05
way;4.472643356068862e-05
to;2.6897802319836016e-05
import;4.2140843979306926e-05
the;4.2418185167016336e-05
BERT;4.086018260601154e-05
language;3.517884566062068e-05
model;2.939494156926955e-05
into;3.4509471167031156e-05
python;3.4948693954089286e-05
for;3.2184247547646006e-05
use;2.9340213952186334e-05
with;4.3003831409368775e-05
PyTorch;0.00011178157559568183
is;0.00011469950229409024
using;5.8077584032516006e-05
the;4.620379091925296e-05
Hugging;3.511070943058225e-05
Face;2.8015252771024806e-05
Transformer's;5.8388990436124626e-05
library,;7.66573225070565e-05
which;3.891438568657293e-05
has;4.164780300572227e-05
built;4.4599519696371886e-05
in;2.98944221635605e-05
methods;4.759525357334755e-05
for;5.5507500232145856e-05
pre-training,;0.0001687074892877185
inference,;5.425211580007915e-05
and;3.6190448123856454e-05
deploying;6.215379912033685e-05
BERT.;0.00018971817853648918
â€˜**;0.00022933908984938955
from;3.729818774241313e-05
transformers;4.670785437293995e-05
import;5.859623939863784e-05
AutoTokenizer,;0.00022371445610393123
BertModel;5.813413668060741e-05
import;5.320288716176171e-05
torch;2.8823162781571203e-05
tokenizer;4.3033830355052654e-05
=;5.571987476440934e-05
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5465780666837462
model;3.0461392806416356e-05
=;5.1849879478398444e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.44600715687840287
inputs;2.983339838299384e-05
=;6.657669802385787e-05
"tokenizer(""Hello,";0.0004773064820349051
my;2.9263925152241034e-05
dog;2.5976578342414486e-05
is;2.7490734951902308e-05
"cute"",";9.80124548942252e-05
"return_tensors=""pt"")";0.002620325543760252
outputs;3.469559486650524e-05
=;4.0852742963256626e-05
model(**inputs);0.0006668014024656148
last_hidden_states;0.00010537330670628683
=;3.7395407872759e-05
outputs.last_hidden_state;0.0003514056339929422
***;7.819039692656701e-05
