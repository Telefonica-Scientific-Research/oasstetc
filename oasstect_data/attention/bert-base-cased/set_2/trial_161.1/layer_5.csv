text;attention
The;4.854690919964023e-06
easiest;1.3971056931175366e-05
way;4.6220476737442264e-06
to;3.8221174270224345e-06
import;5.711916447843271e-06
the;4.347696169833764e-06
BERT;9.518488902004619e-06
language;4.484682420163127e-06
model;4.070792763478339e-06
into;5.024853035897454e-06
python;8.857205269724194e-06
for;4.489083664583394e-06
use;5.135613384575829e-06
with;5.836076630296065e-06
PyTorch;2.8309146219750005e-05
is;7.608313223901828e-06
using;6.52680112956176e-06
the;6.9324311374381215e-06
Hugging;7.225742587479607e-06
Face;4.651392825495305e-06
Transformer's;1.3334994531514943e-05
library,;1.098135398231836e-05
which;5.1920343442144565e-06
has;5.302271310044819e-06
built;6.278364760888254e-06
in;4.1658915102236465e-06
methods;6.245949991899831e-06
for;6.931987369369831e-06
pre-training,;3.6065878615119287e-05
inference,;8.255471765747866e-06
and;6.231878874071992e-06
deploying;8.973061418003847e-06
BERT.;2.976985390006887e-05
â€˜**;1.3740586743561292e-05
from;4.018737779642781e-06
transformers;7.445200551504768e-06
import;4.983717658834167e-06
AutoTokenizer,;3.721534275506997e-05
BertModel;1.5783125213275195e-05
import;4.604108149334843e-06
torch;3.7742027066003855e-06
tokenizer;5.102872442878605e-06
=;8.216595285290925e-06
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.3918998832499685
model;4.6776727796802765e-06
=;1.0232449147550622e-05
"BertModel.from_pretrained(""bert-base-uncased"")";0.6069315227851009
inputs;3.6168604724025624e-06
=;6.537154745616163e-06
"tokenizer(""Hello,";5.1275547394225264e-05
my;4.02692408561125e-06
dog;4.5305496579150015e-06
is;4.652031363673332e-06
"cute"",";2.013458683021647e-05
"return_tensors=""pt"")";0.0004587169845979217
outputs;4.5560246552491965e-06
=;5.465334921825965e-06
model(**inputs);4.993404008632521e-05
last_hidden_states;1.8579508965364174e-05
=;6.84025676242653e-06
outputs.last_hidden_state;0.00010813548670044626
***;8.068921340914094e-06
