text;attention
The;3.576333906160978e-07
easiest;9.1056556881342e-07
way;3.035820098248587e-07
to;3.77991928832656e-07
import;2.8091004680507686e-07
the;4.48058107250336e-07
BERT;8.533294973083228e-07
language;3.730491018647651e-07
model;2.776655787649264e-07
into;3.7053517565011547e-07
python;1.2757965883972653e-06
for;3.559097185314454e-07
use;3.59075053260729e-07
with;3.987830534159283e-07
PyTorch;3.2782729020320845e-06
is;3.573048820747349e-07
using;3.5724276403684187e-07
the;6.217405788759746e-07
Hugging;4.880386276711639e-07
Face;3.63887770379765e-07
Transformer's;2.2456274031341176e-06
library,;1.0196911552639208e-06
which;4.059261811045445e-07
has;3.7947661750287427e-07
built;3.5354437513102145e-07
in;3.477375320277088e-07
methods;4.0037995152469903e-07
for;3.4711154655092176e-07
pre-training,;5.873532656411969e-06
inference,;1.5948086094776283e-06
and;4.374042873043237e-07
deploying;6.338548174926061e-07
BERT.;5.57334935563594e-06
â€˜**;5.633831159687405e-06
from;3.111949064875018e-07
transformers;6.004032914753991e-07
import;3.0787195704823124e-07
AutoTokenizer,;4.532277136009915e-06
BertModel;1.4847607854453438e-06
import;2.986449246338384e-07
torch;2.739238387612666e-07
tokenizer;4.355219972378575e-07
=;3.2698895289849497e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.5939509812095689
model;2.7784643156513026e-07
=;3.3937924380131855e-07
"BertModel.from_pretrained(""bert-base-uncased"")";0.4056627148656921
inputs;2.789709530223181e-07
=;3.5673693748537604e-07
"tokenizer(""Hello,";7.381124495341557e-06
my;4.6231781206832123e-07
dog;2.679553466161664e-07
is;3.2778785031453783e-07
"cute"",";1.8181561055788614e-06
"return_tensors=""pt"")";0.00026268236763777296
outputs;6.645131370295521e-07
=;3.672047698607546e-07
model(**inputs);1.3121150096418766e-05
last_hidden_states;2.6297098331777515e-06
=;3.1146687444932203e-07
outputs.last_hidden_state;4.811981067988604e-05
***;6.701907519924345e-07
