text;attention
The;2.1518280286584935e-09
easiest;3.7175361356329832e-09
way;2.626601938019924e-09
to;1.9267225544860264e-09
import;2.86784544957279e-09
the;2.4751797105327422e-09
BERT;3.944428408577658e-09
language;4.838098423668769e-09
model;2.554388667577743e-09
into;2.2270843756366092e-09
python;4.552682405983932e-09
for;2.1224687778990733e-09
use;2.802436473485488e-09
with;3.319189486994572e-09
PyTorch;1.3103830842538426e-08
is;2.9523277276805515e-09
using;2.5019819689997417e-09
the;3.729702261909284e-09
Hugging;2.9157973440532366e-09
Face;2.4422940475165952e-09
Transformer's;5.768715074895839e-09
library,;6.505885439925268e-09
which;2.1502391665584667e-09
has;3.577698287204217e-09
built;2.8944782763162664e-09
in;2.1465391119939394e-09
methods;3.4294098808168507e-09
for;2.2400571751039714e-09
pre-training,;1.112493336536615e-08
inference,;5.822292619602433e-09
and;2.4454179823195626e-09
deploying;4.291219612627716e-09
BERT.;0.9999744063808432
â€˜**;3.7361917238423366e-09
from;2.1844093862481526e-09
transformers;4.24309194592356e-09
import;2.7625228460176487e-09
AutoTokenizer,;1.5301812982633396e-08
BertModel;5.414907126849843e-09
import;2.666790931704511e-09
torch;3.02960875257874e-09
tokenizer;3.93356936666821e-09
=;2.2844420353597604e-09
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";2.0985550472316614e-05
model;2.3857042890931135e-09
=;2.554487329705173e-09
"BertModel.from_pretrained(""bert-base-uncased"")";4.245891133706839e-06
inputs;2.1713037710697838e-09
=;2.4669713812399857e-09
"tokenizer(""Hello,";3.3364670825708266e-08
my;2.231686732860164e-09
dog;2.597156907218274e-09
is;2.18902010386945e-09
"cute"",";6.067444847930444e-09
"return_tensors=""pt"")";6.155159785882154e-08
outputs;2.3496065754881522e-09
=;2.304827465544983e-09
model(**inputs);1.253492916579625e-08
last_hidden_states;2.2869404372338856e-08
=;2.5292910825308496e-09
outputs.last_hidden_state;2.889383168183416e-08
***;5.388958142361663e-09
