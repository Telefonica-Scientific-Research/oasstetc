text;attention
The;2.1453664839301644e-07
easiest;2.987102002555195e-07
way;1.2709456799655761e-07
to;1.8096192828140857e-07
import;1.605346143030928e-07
the;1.9118440914220855e-07
BERT;2.392198713266267e-07
language;1.9125505136763144e-07
model;1.5426527028743977e-07
into;1.7268345633667574e-07
python;2.7988882918701533e-07
for;1.5575334568789312e-07
use;1.4379624537627e-07
with;2.3811582997725897e-07
PyTorch;4.862024538230738e-07
is;2.519634746636372e-07
using;1.6566506705940602e-07
the;2.8630052393093046e-07
Hugging;1.793893335831748e-07
Face;1.6528493431290108e-07
Transformer's;9.391153019841404e-07
library,;8.281915727856657e-07
which;1.5893223942197662e-07
has;1.860684111309374e-07
built;2.1025009081190978e-07
in;1.7857854081175942e-07
methods;1.831926412673523e-07
for;2.1690731132250924e-07
pre-training,;1.7265645068438234e-06
inference,;1.0632925209808157e-06
and;2.227872660496448e-07
deploying;4.791356704076187e-07
BERT.;3.7817560880400603e-06
â€˜**;1.5019267201567976e-06
from;1.8551836962425452e-07
transformers;4.0172205998150957e-07
import;2.0858809949403118e-07
AutoTokenizer,;2.7026461952777202e-06
BertModel;1.0585231214398865e-06
import;1.565139006645235e-07
torch;1.8514522543430416e-07
tokenizer;2.058728819685507e-07
=;3.1104027950270184e-07
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";0.31969275312015977
model;1.625852554433211e-07
=;2.248624438855775e-07
"BertModel.from_pretrained(""bert-base-uncased"")";0.680207382268921
inputs;1.422360605500996e-07
=;2.0397756007504666e-07
"tokenizer(""Hello,";5.339431643557326e-06
my;1.8548652601871154e-07
dog;1.3257112698123268e-07
is;1.5894657286808202e-07
"cute"",";9.441804054459399e-07
"return_tensors=""pt"")";4.556101516713487e-05
outputs;1.913891358954127e-07
=;1.9178301438969163e-07
model(**inputs);2.277195548368004e-06
last_hidden_states;1.7841486725622564e-06
=;1.8285087496989735e-07
outputs.last_hidden_state;2.0585126253180985e-05
***;2.2174958702375297e-07
