text;attention
The;2.3406666448088524e-20
easiest;4.7491093686502556e-20
way;2.3391230064012322e-20
to;2.2274402583025968e-20
import;3.295788714268694e-20
the;3.0798543193943765e-20
BERT;7.062580652902232e-20
language;3.04654694610438e-20
model;2.8768511395667735e-20
into;2.9895714294073786e-20
python;8.826140950654399e-20
for;2.678952782174633e-20
use;2.6950148513844273e-20
with;2.813358825735603e-20
PyTorch;2.726468689256486e-19
is;2.795743780036628e-20
using;2.491169668200273e-20
the;2.683167922180142e-20
Hugging;5.471194365858387e-20
Face;3.2682376914380034e-20
Transformer's;8.616052424107758e-20
library,;3.9478416863539264e-20
which;2.670059776932024e-20
has;2.6328527659089528e-20
built;2.763271940098999e-20
in;2.432123382065545e-20
methods;2.781217125370202e-20
for;2.5976501155641234e-20
pre-training,;8.43869809997688e-20
inference,;5.345107201795324e-20
and;2.8620736397296376e-20
deploying;3.9821532540692015e-20
BERT.;0.9999999999999989
â€˜**;1.0377398937752541e-19
from;3.1538572402812333e-20
transformers;5.81418881937858e-20
import;4.3318426393172166e-20
AutoTokenizer,;1.965267761850939e-19
BertModel;9.251654964359037e-20
import;5.512452859991569e-20
torch;4.034437793312046e-20
tokenizer;4.293911033008377e-20
=;3.22108756789904e-20
"AutoTokenizer.from_pretrained(""bert-base-uncased"")";4.649517586495404e-16
model;3.83314853468377e-20
=;2.5107704840092797e-20
"BertModel.from_pretrained(""bert-base-uncased"")";6.387337856621166e-16
inputs;3.075661417368194e-20
=;3.2222159091817746e-20
"tokenizer(""Hello,";2.602232075602539e-19
my;2.7804357790604504e-20
dog;3.5660512113850616e-20
is;2.727230962698209e-20
"cute"",";7.662338204544353e-20
"return_tensors=""pt"")";4.297007726295355e-18
outputs;3.990618241662383e-20
=;2.7820770423245685e-20
model(**inputs);6.322489698410234e-19
last_hidden_states;2.1594401339956824e-19
=;3.343996631499644e-20
outputs.last_hidden_state;1.7078264463051211e-18
***;4.2245500351645335e-20
